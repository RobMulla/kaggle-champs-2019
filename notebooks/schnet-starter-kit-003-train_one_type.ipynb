{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Code Try to train single type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 0\n",
    "DEVICE_cupy = '@cupy:0'\n",
    "FILTER_TYPES = ['2JHC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "Next, I import main packages. Other sub-modules are imported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chainer\n",
    "import chainer_chemistry\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "In this example, 90% of training data is used actual training data, and the other 10% is used for validation.\n",
    "Each dataset is grouped by molecule_name name for following procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filter_type=None):\n",
    "\n",
    "    train = pd.merge(pd.read_csv('../input/train.csv'),\n",
    "                     pd.read_csv('../input/scalar_coupling_contributions.csv'))\n",
    "\n",
    "    test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "    counts = train['molecule_name'].value_counts()\n",
    "    moles = list(counts.index)\n",
    "\n",
    "    random.shuffle(moles)\n",
    "\n",
    "    num_train = int(len(moles) * 0.9)\n",
    "    train_moles = sorted(moles[:num_train])\n",
    "    valid_moles = sorted(moles[num_train:])\n",
    "    test_moles = sorted(list(set(test['molecule_name'])))\n",
    "\n",
    "    valid = train.query('molecule_name not in @train_moles').copy()\n",
    "    train = train.query('molecule_name in @train_moles').copy()\n",
    "\n",
    "    train.sort_values('molecule_name', inplace=True)\n",
    "    valid.sort_values('molecule_name', inplace=True)\n",
    "    test.sort_values('molecule_name', inplace=True)\n",
    "    \n",
    "    if filter_type is not None:\n",
    "        train = train.loc[train['type'].isin(filter_type)]\n",
    "        valid = valid.loc[valid['type'].isin(filter_type)]\n",
    "        test = test.loc[test['type'].isin(filter_type)]\n",
    "\n",
    "    return train, valid, test, train_moles, valid_moles, test_moles\n",
    "\n",
    "train, valid, test, train_moles, valid_moles, test_moles = load_dataset(filter_type=FILTER_TYPES)\n",
    "\n",
    "train_gp = train.groupby('molecule_name')\n",
    "valid_gp = valid.groupby('molecule_name')\n",
    "test_gp = test.groupby('molecule_name')\n",
    "\n",
    "structures = pd.read_csv('../input/structures.csv')\n",
    "structures_groups = structures.groupby('molecule_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "I implemented a class named `Graph` whose instances contain molecules.\n",
    "The distances between atoms are calculated in the initializer of this class.\n",
    "## Define Graph class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "class Graph:\n",
    "\n",
    "    def __init__(self, points_df, list_atoms):\n",
    "\n",
    "        self.points = points_df[['x', 'y', 'z']].values\n",
    "\n",
    "        self._dists = distance.cdist(self.points, self.points)\n",
    "\n",
    "        self.adj = self._dists < 1.5\n",
    "        self.num_nodes = len(points_df)\n",
    "\n",
    "        self.atoms = points_df['atom']\n",
    "        dict_atoms = {at: i for i, at in enumerate(list_atoms)}\n",
    "\n",
    "        atom_index = [dict_atoms[atom] for atom in self.atoms]\n",
    "        one_hot = np.identity(len(dict_atoms))[atom_index]\n",
    "\n",
    "        bond = np.sum(self.adj, 1) - 1\n",
    "        bonds = np.identity(len(dict_atoms))[bond - 1]\n",
    "\n",
    "        self._array = np.concatenate([one_hot, bonds], axis=1).astype(np.float32)\n",
    "\n",
    "    @property\n",
    "    def input_array(self):\n",
    "        return self._array\n",
    "\n",
    "    @property\n",
    "    def dists(self):\n",
    "        return self._dists.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into graph object\n",
    "Each dataset is represented as a list of Graphs and prediction targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set moles to only include ones from the given types for evaluation\n",
    "train_moles = list(set(train['molecule_name']))\n",
    "test_moles = list(set(test['molecule_name']))\n",
    "valid_moles = list(set(valid['molecule_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of atoms\n",
      "['H', 'F', 'O', 'N', 'C']\n",
      "preprocess training molecules ...\n",
      "preprocess validation molecules ...\n",
      "preprocess test molecules ...\n"
     ]
    }
   ],
   "source": [
    "list_atoms = list(set(structures['atom']))\n",
    "print('list of atoms')\n",
    "print(list_atoms)\n",
    "    \n",
    "train_graphs = list()\n",
    "train_targets = list()\n",
    "print('preprocess training molecules ...')\n",
    "for mole in train_moles:\n",
    "    train_graphs.append(Graph(structures_groups.get_group(mole), list_atoms))\n",
    "    train_targets.append(train_gp.get_group(mole))\n",
    "\n",
    "valid_graphs = list()\n",
    "valid_targets = list()\n",
    "print('preprocess validation molecules ...')\n",
    "for mole in valid_moles:\n",
    "    valid_graphs.append(Graph(structures_groups.get_group(mole), list_atoms))\n",
    "    valid_targets.append(valid_gp.get_group(mole))\n",
    "\n",
    "test_graphs = list()\n",
    "test_targets = list()\n",
    "print('preprocess test molecules ...')\n",
    "for mole in test_moles:\n",
    "    test_graphs.append(Graph(structures_groups.get_group(mole), list_atoms))\n",
    "    test_targets.append(test_gp.get_group(mole))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into chainer's dataset\n",
    "This type of dataset can be handled by `DictDataset`.\n",
    "Graph objects and prediction targets are merged as a `DictDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.datasets.dict_dataset import DictDataset\n",
    "\n",
    "train_dataset = DictDataset(graphs=train_graphs, targets=train_targets)\n",
    "valid_dataset = DictDataset(graphs=valid_graphs, targets=valid_targets)\n",
    "test_dataset = DictDataset(graphs=test_graphs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "## Build SchNet model\n",
    "The prediction model is implemented as follows.\n",
    "First, fully connected layer is applied to input arrays to align dimensions.\n",
    "Next, SchNet layer is applied for feature extraction.\n",
    "Finally, features vectors are concatenated and thrown into three layers MLP.\n",
    "I add batch-normalization layers like ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SchNet at 0x7fc60187a358>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chainer import reporter\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer_chemistry.links import SchNetUpdate\n",
    "from chainer_chemistry.links import GraphLinear, GraphBatchNormalization\n",
    "\n",
    "class SchNetUpdateBN(SchNetUpdate):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SchNetUpdateBN, self).__init__(*args, **kwargs)\n",
    "        with self.init_scope():\n",
    "            self.bn = GraphBatchNormalization(args[0])\n",
    "\n",
    "    def __call__(self, h, adj, **kwargs):\n",
    "        v = self.linear[0](h)\n",
    "        v = self.cfconv(v, adj)\n",
    "        v = self.linear[1](v)\n",
    "        v = F.softplus(v)\n",
    "        v = self.linear[2](v)\n",
    "        return h + self.bn(v)\n",
    "\n",
    "class SchNet(chainer.Chain):\n",
    "\n",
    "    def __init__(self, num_layer=3):\n",
    "        super(SchNet, self).__init__()\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.gn = GraphLinear(512)\n",
    "            for l in range(self.num_layer):\n",
    "                self.add_link('sch{}'.format(l), SchNetUpdateBN(512))\n",
    "\n",
    "            self.interaction1 = L.Linear(128)\n",
    "            self.interaction2 = L.Linear(128)\n",
    "            self.interaction3 = L.Linear(4)\n",
    "\n",
    "    def __call__(self, input_array, dists, pairs_index, targets):\n",
    "\n",
    "        out = self.predict(input_array, dists, pairs_index)\n",
    "        loss = F.mean_absolute_error(out, targets)\n",
    "        reporter.report({'loss': loss}, self)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, input_array, dists, pairs_index, **kwargs):\n",
    "\n",
    "        h = self.gn(input_array)\n",
    "\n",
    "        for l in range(self.num_layer):\n",
    "            h = self['sch{}'.format(l)](h, dists)\n",
    "\n",
    "        h = F.concat((h, input_array), axis=2)\n",
    "\n",
    "        concat = F.concat([\n",
    "            h[pairs_index[:, 0], pairs_index[:, 1], :],\n",
    "            h[pairs_index[:, 0], pairs_index[:, 2], :],\n",
    "            F.expand_dims(dists[pairs_index[:, 0],\n",
    "                                pairs_index[:, 1],\n",
    "                                pairs_index[:, 2]], 1)\n",
    "        ], axis=1)\n",
    "\n",
    "        h1 = F.leaky_relu(self.interaction1(concat))\n",
    "        h2 = F.leaky_relu(self.interaction2(h1))\n",
    "        out = self.interaction3(h2)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = SchNet(num_layer=3)\n",
    "model.to_gpu(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training preparation\n",
    "## Make samplers\n",
    "For mini-batch training, I implement a sampler named `SameSizeSampler`.\n",
    "The molecules which have same number of atoms are selected simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.iterators import OrderSampler\n",
    "\n",
    "class SameSizeSampler(OrderSampler):\n",
    "\n",
    "    def __init__(self, structures_groups, moles, batch_size,\n",
    "                 random_state=None, use_remainder=False):\n",
    "\n",
    "        self.structures_groups = structures_groups\n",
    "        self.moles = moles\n",
    "        self.batch_size = batch_size\n",
    "        if random_state is None:\n",
    "            random_state = np.random.random.__self__\n",
    "        self._random = random_state\n",
    "        self.use_remainder = use_remainder\n",
    "\n",
    "    def __call__(self, current_order, current_position):\n",
    "\n",
    "        batches = list()\n",
    "\n",
    "        atom_counts = pd.DataFrame()\n",
    "        atom_counts['mol_index'] = np.arange(len(self.moles))\n",
    "        atom_counts['molecular_name'] = self.moles\n",
    "        atom_counts['num_atom'] = [len(self.structures_groups.get_group(mol))\n",
    "                                   for mol in self.moles]\n",
    "\n",
    "        num_atom_counts = atom_counts['num_atom'].value_counts()\n",
    "\n",
    "        for count, num_mol in num_atom_counts.to_dict().items():\n",
    "            if self.use_remainder:\n",
    "                num_batch_for_this = -(-num_mol // self.batch_size)\n",
    "            else:\n",
    "                num_batch_for_this = num_mol // self.batch_size\n",
    "\n",
    "            target_mols = atom_counts.query('num_atom==@count')['mol_index'].values\n",
    "            random.shuffle(target_mols)\n",
    "\n",
    "            devider = np.arange(0, len(target_mols), self.batch_size)\n",
    "            devider = np.append(devider, 99999)\n",
    "\n",
    "            if self.use_remainder:\n",
    "                target_mols = np.append(\n",
    "                    target_mols,\n",
    "                    np.repeat(target_mols[-1], -len(target_mols) % self.batch_size))\n",
    "\n",
    "            for b in range(num_batch_for_this):\n",
    "                batches.append(target_mols[devider[b]:devider[b + 1]])\n",
    "\n",
    "        random.shuffle(batches)\n",
    "        batches = np.concatenate(batches).astype(np.int32)\n",
    "\n",
    "        return batches\n",
    "\n",
    "batch_size = 8\n",
    "train_sampler = SameSizeSampler(structures_groups, train_moles, batch_size)\n",
    "valid_sampler = SameSizeSampler(structures_groups, valid_moles, batch_size,\n",
    "                                use_remainder=True)\n",
    "test_sampler = SameSizeSampler(structures_groups, test_moles, batch_size,\n",
    "                               use_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make iterators, oprimizer\n",
    "Iterators for data feeding is made as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = chainer.iterators.SerialIterator(\n",
    "    train_dataset, batch_size, order_sampler=train_sampler)\n",
    "\n",
    "valid_iter = chainer.iterators.SerialIterator(\n",
    "    valid_dataset, batch_size, repeat=False, order_sampler=valid_sampler)\n",
    "\n",
    "test_iter = chainer.iterators.SerialIterator(\n",
    "    test_dataset, batch_size, repeat=False, order_sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make optimizer\n",
    "Adam is used as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x7fc6ef9f24a8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chainer import optimizers\n",
    "optimizer = optimizers.Adam(alpha=1e-3)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make updator\n",
    "Since the model receives input arrays separately, I implement an original converter.\n",
    "`input_array` and `dists` are exstracted from `Graph` object and `pair_index` and `targets` are exstracted from `targets` object.\n",
    "`targets` is added only for training.\n",
    "When this converter is used for evaluation, `targets` is not added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import training\n",
    "from chainer.dataset import to_device\n",
    "\n",
    "def coupling_converter(batch, device):\n",
    "\n",
    "    list_array = list()\n",
    "    list_dists = list()\n",
    "    list_targets = list()\n",
    "    list_pairs_index = list()\n",
    "\n",
    "    with_target = 'fc' in batch[0]['targets'].columns\n",
    "\n",
    "    for i, d in enumerate(batch):\n",
    "        list_array.append(d['graphs'].input_array)\n",
    "        list_dists.append(d['graphs'].dists)\n",
    "        if with_target:\n",
    "            list_targets.append(\n",
    "                d['targets'][['fc', 'sd', 'pso', 'dso']].values.astype(np.float32))\n",
    "\n",
    "        sample_index = np.full((len(d['targets']), 1), i)\n",
    "        atom_index = d['targets'][['atom_index_0', 'atom_index_1']].values\n",
    "\n",
    "        list_pairs_index.append(np.concatenate([sample_index, atom_index], axis=1))\n",
    "\n",
    "    input_array = to_device(device, np.stack(list_array))\n",
    "    dists = to_device(device, np.stack(list_dists))\n",
    "    pairs_index = np.concatenate(list_pairs_index)\n",
    "\n",
    "    array = {'input_array': input_array, 'dists': dists, 'pairs_index': pairs_index}\n",
    "\n",
    "    if with_target:\n",
    "        array['targets'] = to_device(device, np.concatenate(list_targets))\n",
    "\n",
    "    return array\n",
    "\n",
    "updater = training.StandardUpdater(train_iter, optimizer,\n",
    "                                   converter=coupling_converter, device=DEVICE)\n",
    "trainer = training.Trainer(updater, (200, 'epoch'), out=\"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training extensions\n",
    "## Evaluator\n",
    "I implemented an Evaluator which measure validation score during training.\n",
    "The prediction for test data is also calculated in this evaluator and the submision file is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.training.extensions import Evaluator\n",
    "from chainer import cuda\n",
    "\n",
    "class TypeWiseEvaluator(Evaluator):\n",
    "\n",
    "    def __init__(self, iterator, target, converter, device, name,\n",
    "                 is_validate=False, is_submit=False):\n",
    "\n",
    "        super(TypeWiseEvaluator, self).__init__(\n",
    "            iterator, target, converter=converter, device=device)\n",
    "\n",
    "        self.is_validate = is_validate\n",
    "        self.is_submit = is_submit\n",
    "        self.name = name\n",
    "\n",
    "    def calc_score(self, df_truth, pred):\n",
    "\n",
    "        target_types = list(set(df_truth['type']))\n",
    "\n",
    "        diff = df_truth['scalar_coupling_constant'] - pred\n",
    "\n",
    "        scores = 0\n",
    "        metrics = {}\n",
    "\n",
    "        for target_type in target_types:\n",
    "\n",
    "            target_pair = df_truth['type'] == target_type\n",
    "            score_exp = np.mean(np.abs(diff[target_pair]))\n",
    "            scores += np.log(score_exp)\n",
    "            metrics[f'LogMAE_{target_type}'] = score_exp\n",
    "            metrics[target_type] = scores\n",
    "\n",
    "        metrics['ALL_LogMAE'] = scores / len(target_types)\n",
    "\n",
    "        observation = {}\n",
    "        with reporter.report_scope(observation):\n",
    "            reporter.report(metrics, self._targets['main'])\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def evaluate(self):\n",
    "        iterator = self._iterators['main']\n",
    "        eval_func = self._targets['main']\n",
    "\n",
    "        iterator.reset()\n",
    "        it = iterator\n",
    "\n",
    "        y_total = []\n",
    "        t_total = []\n",
    "\n",
    "        for batch in it:\n",
    "            in_arrays = self.converter(batch, self.device)\n",
    "            with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "                y = eval_func.predict(**in_arrays)\n",
    "\n",
    "            y_data = cuda.to_cpu(y.data)\n",
    "            y_total.append(y_data)\n",
    "            t_total.extend([d['targets'] for d in batch])\n",
    "\n",
    "        df_truth = pd.concat(t_total, axis=0)\n",
    "        y_pred = np.sum(np.concatenate(y_total), axis=1)\n",
    "\n",
    "        if self.is_submit:\n",
    "            submit = pd.DataFrame()\n",
    "            submit['id'] = df_truth['id']\n",
    "            submit['scalar_coupling_constant'] = y_pred\n",
    "            submit.drop_duplicates(subset='id', inplace=True)\n",
    "            submit.sort_values('id', inplace=True)\n",
    "            submit.to_csv('kernel_schnet.csv', index=False)\n",
    "\n",
    "        if self.is_validate:\n",
    "            return self.calc_score(df_truth, y_pred)\n",
    "\n",
    "        return {}\n",
    "\n",
    "trainer.extend(\n",
    "    TypeWiseEvaluator(iterator=valid_iter, target=model, converter=coupling_converter, \n",
    "                      name='valid', device=DEVICE, is_validate=True))\n",
    "trainer.extend(\n",
    "    TypeWiseEvaluator(iterator=test_iter, target=model, converter=coupling_converter,\n",
    "                      name='test', device=DEVICE, is_submit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other extensions\n",
    "ExponentialShift is set as a learning rate scheduler.\n",
    "An extension which turn off training mode is also set to deactivate normalizatoin from second epoch.\n",
    "\n",
    "Log options are set to report the metrics.\n",
    "This helps us to analyze the result of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.extend(training.extensions.ExponentialShift('alpha', 0.99999))\n",
    "\n",
    "from chainer.training import make_extension\n",
    "\n",
    "def stop_train_mode(trigger):\n",
    "    @make_extension(trigger=trigger)\n",
    "    def _stop_train_mode(_):\n",
    "        chainer.config.train = False\n",
    "    return _stop_train_mode\n",
    "\n",
    "trainer.extend(stop_train_mode(trigger=(1, 'epoch')))\n",
    "\n",
    "trainer.extend(\n",
    "    training.extensions.observe_value(\n",
    "        'alpha', lambda tr: tr.updater.get_optimizer('main').alpha))\n",
    "\n",
    "trainer.extend(training.extensions.LogReport())\n",
    "trainer.extend(training.extensions.PrintReport(\n",
    "    ['epoch', 'elapsed_time', 'main/loss', 'valid/main/ALL_LogMAE', 'alpha']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Run\n",
    "I tuned number of epochs to prevent timeout.\n",
    "SchNet tends to be underfitting, longer training makes the model better basically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       elapsed_time  main/loss   valid/main/ALL_LogMAE  alpha     \n",
      "\u001b[J1           507.229       0.327115    -0.30334               0.000909118  \n",
      "\u001b[J2           1034.15       0.183978    -0.68444               0.000826487  \n",
      "\u001b[J3           1559.81       0.145881    -0.877421              0.000751367  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d00edd6d96cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE_cupy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/training/updaters/standard_updater.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/training/updaters/standard_updater.py\u001b[0m in \u001b[0;36mupdate_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0min_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/optimizer.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, lossfun, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzerograds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, retain_grad, enable_double_backprop, loss_scale)\u001b[0m\n\u001b[1;32m   1436\u001b[0m             \u001b[0;31m# to _backprop_to_all, but it is working because grad_var is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m             \u001b[0;31m# immediately popped away as None = _backprop_utils._reduce([None])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             \u001b[0m_backprop_to_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36m_backprop_to_all\u001b[0;34m(outputs, retain_grad, loss_scale)\u001b[0m\n\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m             _backprop_utils.backprop_step(\n\u001b[0;32m-> 1654\u001b[0;31m                 func, target_input_indexes, out_grad, in_grad, is_debug)\n\u001b[0m\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/_backprop_utils.py\u001b[0m in \u001b[0;36mbackprop_step\u001b[0;34m(func, target_input_indexes, grad_outputs, grad_inputs, is_debug)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             gxs = func.backward(\n\u001b[0;32m--> 138\u001b[0;31m                 target_input_indexes, grad_outputs)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0m_reraise_with_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/functions/connection/linear.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, indexes, grad_outputs)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'use_ideep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config_use_ideep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mgx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearGradData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;31m# In normal case, simply run the forward method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# Check for output array types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/functions/connection/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mgx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chainer.config.train = True\n",
    "with chainer.using_device(DEVICE_cupy):\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('kernel_schnet.csv')\n",
    "display(submit.head())\n",
    "print('shape: {}'.format(submit.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
