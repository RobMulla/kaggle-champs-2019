{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_struct=pd.read_csv('../input/structures.csv')\n",
    "df_train_sub_charge=pd.read_csv('../input/mulliken_charges.csv')\n",
    "df_train_sub_tensor=pd.read_csv('../input/magnetic_shielding_tensors.csv')\n",
    "train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(bond_type, filename_start, filename_end, fold):\n",
    "    logger.info('Creating Datasets')\n",
    "    # Read input for first fold\n",
    "    X_train = pd.read_parquet(f'../type_results/{bond_type}/meta/{filename_start}_X_train_meta_fc_f{fold}{filename_end}') \n",
    "    X_test = pd.read_parquet(f'../type_results/{bond_type}/meta/{filename_start}_X_test_meta_fc_f{fold}{filename_end}') \n",
    "    X_valid = pd.read_parquet(f'../type_results/{bond_type}/meta/{filename_start}_X_valid_meta_fc_f{fold}{filename_end}') \n",
    "    X_train['split'] = 'TRAIN'\n",
    "    X_test['split'] = 'TEST'\n",
    "    X_valid['split'] = 'VALID'\n",
    "    logger.info('Adding target to dataset')\n",
    "    # Add target to train and val\n",
    "    X_tr_val = pd.concat([X_train, X_valid])\n",
    "    X_tr_val = X_tr_val.sort_index()\n",
    "    X_tr_val['scalar_coupling_constant'] = train.loc[train['type'] == '3JHH']['scalar_coupling_constant'].tolist()\n",
    "    X_tr_val['molecule_name'] = train.loc[train['type'] == '3JHH']['molecule_name'].tolist()\n",
    "    X_tr_val['atom_index_0'] = train.loc[train['type'] == '3JHH']['atom_index_0'].tolist()\n",
    "    X_tr_val['atom_index_1'] = train.loc[train['type'] == '3JHH']['atom_index_1'].tolist()\n",
    "\n",
    "    # Combine all\n",
    "    X_all = pd.concat([X_tr_val, X_test])\n",
    "    logger.info('Adding custom target features')\n",
    "    for atom_idx in [0,1]:\n",
    "        X_all = map_atom_info(X_all,df_struct, atom_idx)\n",
    "        X_all = map_atom_info(X_all,df_train_sub_charge, atom_idx)\n",
    "        X_all = map_atom_info(X_all,df_train_sub_tensor, atom_idx)\n",
    "        X_all = X_all.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                                            'x': f'x_{atom_idx}',\n",
    "                                            'y': f'y_{atom_idx}',\n",
    "                                            'z': f'z_{atom_idx}',\n",
    "                                            'mulliken_charge': f'charge_{atom_idx}',\n",
    "                                            'XX': f'XX_{atom_idx}',\n",
    "                                            'YX': f'YX_{atom_idx}',\n",
    "                                            'ZX': f'ZX_{atom_idx}',\n",
    "                                            'XY': f'XY_{atom_idx}',\n",
    "                                            'YY': f'YY_{atom_idx}',\n",
    "                                            'ZY': f'ZY_{atom_idx}',\n",
    "                                            'XZ': f'XZ_{atom_idx}',\n",
    "                                            'YZ': f'YZ_{atom_idx}',\n",
    "                                            'ZZ': f'ZZ_{atom_idx}',})\n",
    "    \n",
    "    ys_all = X_all[['scalar_coupling_constant',\"charge_0\",\"charge_1\",\n",
    "                \"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\",\"YX_0\",\"ZX_0\",\n",
    "                \"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\n",
    "                \"XZ_1\",\"YZ_1\"]]\n",
    "    split_all = X_all['split']\n",
    "    \n",
    "    X_all = X_all.drop(['scalar_coupling_constant',\"charge_0\",\"charge_1\",\n",
    "                \"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\",\"YX_0\",\"ZX_0\",\n",
    "                \"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\n",
    "                \"XZ_1\",\"YZ_1\"], axis=1)\n",
    "    \n",
    "    #Impute NA with mean\n",
    "    # THIS PART TAKES A LONG TIME\n",
    "    logger.info('Filling in NA vaules with the mean value (this can take some time.....)')\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    X_all = SimpleImputer().fit_transform(X_all) \n",
    "#     MEAN = X_all.mean()\n",
    "#     X_all.fillna( value=MEAN, inplace=True )\n",
    "    \n",
    "    splits = X_all['split']\n",
    "    target_all = ys_all['scalar_coupling_constant']\n",
    "    X_all = X_all.drop('split', axis=1)\n",
    "    \n",
    "    X_all = X_all.drop(['atom_0','atom_1','molecule_name'], axis=1)\n",
    "    \n",
    "    # STANDARD SCALAR STUFF\n",
    "    logger.info('Applying Standard scalar to data')\n",
    "    X_all[X_all.columns] = StandardScaler().fit_transform(X_all[X_all.columns])\n",
    "    ys_all[ys_all.columns] = StandardScaler().fit_transform(ys_all[ys_all.columns])\n",
    "    \n",
    "    X_train = X_all.loc[splits == 'TRAIN']\n",
    "    X_valid = X_all.loc[splits == 'VALID']\n",
    "    X_test = X_all.loc[splits == 'TEST']\n",
    "    \n",
    "    y_train = ys_all.loc[splits == 'TRAIN']\n",
    "    y_valid = ys_all.loc[splits == 'VALID']\n",
    "    y_test = ys_all.loc[splits == 'TEST']\n",
    "    \n",
    "    target_train = target_all[splits == 'TRAIN']\n",
    "    target_valid = target_all[splits == 'VALID']\n",
    "    target_test = target_all[splits == 'TEST']\n",
    "    \n",
    "    m1=2\n",
    "    m2=4\n",
    "    m3=1\n",
    "\n",
    "    train_input=X_train.values\n",
    "    cv_input=X_valid.values\n",
    "    train_target=target_train.values\n",
    "    cv_target=target_valid.values\n",
    "    train_target_1=m1 * y_train[[\"charge_0\",\"charge_1\"]].values\n",
    "    cv_target_1=m1 * y_valid[[\"charge_0\",\"charge_1\"]].values\n",
    "    train_target_2=m2 * y_train[[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]].values\n",
    "    cv_target_2=m2 * y_valid[[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]].values\n",
    "    train_target_3=m3 * y_train[[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]].values\n",
    "    cv_target_3=m3 * y_valid[[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]].values\n",
    "    test_input=X_test.values\n",
    "    logger.info('Done creating data for model')\n",
    "    return train_input, cv_input, train_target, cv_target, train_target_1, cv_target_1, train_target_2, cv_target_2, train_target_3, cv_target_3, test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger():\n",
    "    \"\"\"\n",
    "        credits to: https://www.kaggle.com/ogrellier/user-level-lightgbm-lb-1-4480\n",
    "    \"\"\"\n",
    "#     os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "#     time.tzset()\n",
    "    FORMAT = \"[%(asctime)s] %(levelname)s : %(message)s\"\n",
    "    logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "#     logging.basicConfig(format=FORMAT)\n",
    "    logger = logging.getLogger(\"main\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    formatter = logging.Formatter(FORMAT)\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-07-31 18:12:49,619] INFO : Creating Datasets\n",
      "2019-07-31 18:12:49,619 | INFO : Creating Datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/pyarrow/pandas_compat.py:708: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels = getattr(columns, 'labels', None) or [\n",
      "/home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/pyarrow/pandas_compat.py:735: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "  return pd.MultiIndex(levels=new_levels, labels=labels, names=columns.names)\n",
      "/home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-07-31 18:12:51,358] INFO : Adding target to dataset\n",
      "2019-07-31 18:12:51,358 | INFO : Adding target to dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-07-31 18:12:57,606] INFO : Adding custom target features\n",
      "2019-07-31 18:12:57,606 | INFO : Adding custom target features\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'map_atom_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6d40f03bcd1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_target_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_target_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_target_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbond_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c3e13b67090c>\u001b[0m in \u001b[0;36mcreate_datasets\u001b[0;34m(bond_type, filename_start, filename_end, fold)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adding custom target features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0matom_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mX_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_atom_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mX_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_atom_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train_sub_charge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mX_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_atom_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train_sub_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'map_atom_info' is not defined"
     ]
    }
   ],
   "source": [
    "filename_start = 'M053_0725_0821_3JHH'\n",
    "filename_end = '_0.1753MAE_-1.7413LMAE.parquet'\n",
    "bond_type = '3JHH'\n",
    "fold = 1\n",
    "\n",
    "train_input, cv_input, train_target, cv_target, train_target_1, cv_target_1, train_target_2, cv_target_2, train_target_3, cv_target_3, test_input = create_datasets(bond_type, filename_start, filename_end, fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
