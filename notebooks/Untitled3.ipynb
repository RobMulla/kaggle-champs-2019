{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/structure_with_acsf_soap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soap_cols = [c for c in df.columns if 'soap' in c]\n",
    "acsf_cols = [c for c in df.columns if 'acsf' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:21<00:00, 20.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for c in tqdm(soap_cols):\n",
    "    df[c] = ss.fit_transform(df[[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:04<00:00, 19.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for c in tqdm(acsf_cols):\n",
    "    df[c] = ss.fit_transform(df[[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01366052],\n",
       "       [-0.01366052],\n",
       "       [-0.01366052],\n",
       "       ...,\n",
       "       [-0.01366052],\n",
       "       [-0.01366052],\n",
       "       [-0.01366052]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.fit_transform(df[[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soap_f0</th>\n",
       "      <th>soap_f1</th>\n",
       "      <th>soap_f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.091164</td>\n",
       "      <td>3.141734</td>\n",
       "      <td>4.616939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003828</td>\n",
       "      <td>2.331910</td>\n",
       "      <td>3.497835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003827</td>\n",
       "      <td>2.331909</td>\n",
       "      <td>3.497834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003825</td>\n",
       "      <td>2.331893</td>\n",
       "      <td>3.497803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003825</td>\n",
       "      <td>2.331893</td>\n",
       "      <td>3.497803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    soap_f0   soap_f1   soap_f2\n",
       "0 -0.091164  3.141734  4.616939\n",
       "1  0.003828  2.331910  3.497835\n",
       "2  0.003827  2.331909  3.497834\n",
       "3  0.003825  2.331893  3.497803\n",
       "4  0.003825  2.331893  3.497803"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[soap_cols[:3]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "soap_f0   -0.876251\n",
       "soap_f1   -6.578335\n",
       "soap_f2   -3.898043\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[soap_cols[:3]].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../input/structure_with_acsf_soap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model number test\n",
      "Loading datasets\n",
      "Done loading datasets\n",
      "Creating samplers\n",
      "Creating optimizer\n",
      "Creating updater and trainer\n",
      "Extending trainer with evaluators\n",
      "Extending trainer with snapshot writer\n",
      "Extending trainer with ExponentialShift\n",
      "Extending trainer with progress bar\n",
      "Loading trainer from snapshot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robmulla/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/training/triggers/interval_trigger.py:92: UserWarning: The previous value of iteration is not saved. IntervalTrigger guesses it using current iteration. If this trigger is not called at every iteration, it may not work correctly.\n",
      "  'The previous value of iteration is not saved. '\n",
      "/home/robmulla/anaconda3/envs/kaggle/lib/python3.6/site-packages/chainer/training/triggers/interval_trigger.py:104: UserWarning: The previous value of epoch_detail is not saved. IntervalTrigger uses the value of trainer.updater.previous_epoch_detail. If this trigger is not called at every iteration, it may not work correctly.\n",
      "  'The previous value of epoch_detail is not saved. '\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "- add soap and acsf\n",
    "\"\"\"\n",
    "\n",
    "#######################\n",
    "### SETTINGS\n",
    "#######################\n",
    "\n",
    "MODEL_NUMBER = 'test'\n",
    "print(f'Running Model number {MODEL_NUMBER}')\n",
    "DEVICE = 0\n",
    "DEVICE_cupy = '@cupy:0'\n",
    "FILTER_TYPES = None # ['2JHC'] # Set to None for all types\n",
    "TRAIN_PCT = 0.9\n",
    "INPUT_DIR = '../input'\n",
    "BATCH_SIZE = 8\n",
    "CREATE_DATASET = False\n",
    "SAVE_DATASETS = False\n",
    "LOAD_DATASETS = True\n",
    "DATASET_DIR = '../scripts/schnet/datasets_acsf_soap'\n",
    "ALPHA = 1e-3 # Try to make it run fast for testing\n",
    "\n",
    "import os\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    os.makedirs(DATASET_DIR)\n",
    "\n",
    "### IMPORTS\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chainer\n",
    "import chainer_chemistry\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############\n",
    "## LOAD DATASET\n",
    "###############\n",
    "\n",
    "def load_dataset(filter_type=None):\n",
    "\n",
    "    train = pd.merge(pd.read_csv(f'{INPUT_DIR}/train.csv'),\n",
    "                     pd.read_csv(f'{INPUT_DIR}/scalar_coupling_contributions.csv'))\n",
    "\n",
    "    test = pd.read_csv(f'{INPUT_DIR}/test.csv')\n",
    "\n",
    "    counts = train['molecule_name'].value_counts()\n",
    "    moles = list(counts.index)\n",
    "\n",
    "    random.shuffle(moles)\n",
    "\n",
    "    num_train = int(len(moles) * TRAIN_PCT)\n",
    "    train_moles = sorted(moles[:num_train])\n",
    "    valid_moles = sorted(moles[num_train:])\n",
    "    test_moles = sorted(list(set(test['molecule_name'])))\n",
    "\n",
    "    valid = train.query('molecule_name not in @train_moles').copy()\n",
    "    train = train.query('molecule_name in @train_moles').copy()\n",
    "\n",
    "    train.sort_values('molecule_name', inplace=True)\n",
    "    valid.sort_values('molecule_name', inplace=True)\n",
    "    test.sort_values('molecule_name', inplace=True)\n",
    "    \n",
    "    if filter_type is not None:\n",
    "        train = train.loc[train['type'].isin(filter_type)]\n",
    "        valid = valid.loc[valid['type'].isin(filter_type)]\n",
    "        test = test.loc[test['type'].isin(filter_type)]\n",
    "        \n",
    "        train_moles = list(set(train['molecule_name']))\n",
    "        test_moles = list(set(test['molecule_name']))\n",
    "        valid_moles = list(set(valid['molecule_name']))\n",
    "\n",
    "    return train, valid, test, train_moles, valid_moles, test_moles\n",
    "\n",
    "#####################\n",
    "# DEFINE GRAPH CLASS\n",
    "#####################\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "class Graph:\n",
    "\n",
    "    def __init__(self, points_df, list_atoms):\n",
    "\n",
    "        self.points = points_df[['x', 'y', 'z']].values\n",
    "\n",
    "        self._dists = distance.cdist(self.points, self.points)\n",
    "\n",
    "        self.adj = self._dists < 1.5\n",
    "        self.num_nodes = len(points_df)\n",
    "\n",
    "        self.atoms = points_df['atom']\n",
    "        dict_atoms = {at: i for i, at in enumerate(list_atoms)}\n",
    "\n",
    "        atom_index = [dict_atoms[atom] for atom in self.atoms]\n",
    "        one_hot = np.identity(len(dict_atoms))[atom_index]\n",
    "\n",
    "        bond = np.sum(self.adj, 1) - 1\n",
    "        bonds = np.identity(len(dict_atoms))[bond - 1]\n",
    "\n",
    "        soap_cols = [c for c in points_df.columns if 'soap' in c]\n",
    "        acsf_cols = [c for c in points_df.columns if 'acsf' in c]\n",
    "\n",
    "        soap = points_df[soap_cols].values\n",
    "        acsf = points_df[acsf_cols].values\n",
    "        self._array = np.concatenate([one_hot, bonds, soap, acsf], axis=1).astype(np.float32)\n",
    "\n",
    "    @property\n",
    "    def input_array(self):\n",
    "        return self._array\n",
    "\n",
    "    @property\n",
    "    def dists(self):\n",
    "        return self._dists.astype(np.float32)\n",
    "\n",
    "########################\n",
    "# CONVERT DATA TO GRAPH\n",
    "########################\n",
    "\n",
    "from chainer.datasets.dict_dataset import DictDataset\n",
    "\n",
    "if CREATE_DATASET:\n",
    "    print('Creating dataset')\n",
    "    train, valid, test, train_moles, valid_moles, test_moles = load_dataset(filter_type=FILTER_TYPES)\n",
    "\n",
    "    train_gp = train.groupby('molecule_name')\n",
    "    valid_gp = valid.groupby('molecule_name')\n",
    "    test_gp = test.groupby('molecule_name')\n",
    "\n",
    "    structures = pd.read_csv(f'{INPUT_DIR}/structure_with_acsf_soap.csv')\n",
    "    structures_groups = structures.groupby('molecule_name')\n",
    "\n",
    "    list_atoms = list(set(structures['atom']))\n",
    "    print('list of atoms')\n",
    "    print(list_atoms)\n",
    "        \n",
    "    train_graphs = list()\n",
    "    train_targets = list()\n",
    "    print('preprocess training molecules ...')\n",
    "    for mole in tqdm(train_moles):\n",
    "        train_graphs.append(Graph(structures_groups.get_group(mole), list_atoms))\n",
    "        train_targets.append(train_gp.get_group(mole))\n",
    "\n",
    "    valid_graphs = list()\n",
    "    valid_targets = list()\n",
    "    print('preprocess validation molecules ...')\n",
    "    for mole in tqdm(valid_moles):\n",
    "        valid_graphs.append(Graph(structures_groups.get_group(mole), list_atoms))\n",
    "        valid_targets.append(valid_gp.get_group(mole))\n",
    "\n",
    "    test_graphs = list()\n",
    "    test_targets = list()\n",
    "    print('preprocess test molecules ...')\n",
    "    for mole in tqdm(test_moles):\n",
    "        test_graphs.append(Graph(structures_groups.get_group(mole), list_atoms))\n",
    "        test_targets.append(test_gp.get_group(mole))\n",
    "\n",
    "    train_dataset = DictDataset(graphs=train_graphs, targets=train_targets)\n",
    "    valid_dataset = DictDataset(graphs=valid_graphs, targets=valid_targets)\n",
    "    test_dataset = DictDataset(graphs=test_graphs, targets=test_targets)\n",
    "\n",
    "    if SAVE_DATASETS:\n",
    "        print('Saving datasets')\n",
    "        with open(f'{DATASET_DIR}/train_dataset.pickle', 'wb') as handle:\n",
    "            pickle.dump(train_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(f'{DATASET_DIR}/valid_dataset.pickle', 'wb') as handle:\n",
    "            pickle.dump(valid_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(f'{DATASET_DIR}/test_dataset.pickle', 'wb') as handle:\n",
    "            pickle.dump(test_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        moles = [train_moles, test_moles, valid_moles]\n",
    "        with open(f'{DATASET_DIR}/moles.pickle', 'wb') as handle:\n",
    "            pickle.dump(moles, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "if LOAD_DATASETS:\n",
    "    print('Loading datasets')\n",
    "    with open(f'{DATASET_DIR}/train_dataset.pickle', 'rb') as handle:\n",
    "        train_dataset = pickle.load(handle)\n",
    "    with open(f'{DATASET_DIR}/valid_dataset.pickle', 'rb') as handle:\n",
    "        valid_dataset = pickle.load(handle)\n",
    "    with open(f'{DATASET_DIR}/test_dataset.pickle', 'rb') as handle:\n",
    "        test_dataset = pickle.load(handle)\n",
    "    with open(f'{DATASET_DIR}/moles.pickle', 'rb') as handle:\n",
    "        moles = pickle.load(handle)\n",
    "        train_moles = moles[0]\n",
    "        test_moles = moles[1]\n",
    "        valid_moles = moles[2]\n",
    "    structures = pd.read_csv(f'{INPUT_DIR}/structures.csv')\n",
    "    structures_groups = structures.groupby('molecule_name')\n",
    "    print('Done loading datasets')\n",
    "\n",
    "########################\n",
    "# Build SchNet model\n",
    "########################\n",
    "\n",
    "from chainer import reporter\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer_chemistry.links import SchNetUpdate\n",
    "from chainer_chemistry.links import GraphLinear, GraphBatchNormalization\n",
    "\n",
    "class SchNetUpdateBN(SchNetUpdate):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SchNetUpdateBN, self).__init__(*args, **kwargs)\n",
    "        with self.init_scope():\n",
    "            self.bn = GraphBatchNormalization(args[0])\n",
    "\n",
    "    def __call__(self, h, adj, **kwargs):\n",
    "        v = self.linear[0](h)\n",
    "        v = self.cfconv(v, adj)\n",
    "        v = self.linear[1](v)\n",
    "        v = F.softplus(v)\n",
    "        v = self.linear[2](v)\n",
    "        return h + self.bn(v)\n",
    "\n",
    "class SchNet(chainer.Chain):\n",
    "\n",
    "    def __init__(self, num_layer=3):\n",
    "        super(SchNet, self).__init__()\n",
    "\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.gn = GraphLinear(512)\n",
    "            for l in range(self.num_layer):\n",
    "                self.add_link('sch{}'.format(l), SchNetUpdateBN(512))\n",
    "\n",
    "            self.interaction1 = L.Linear(128)\n",
    "            self.interaction2 = L.Linear(128)\n",
    "            self.interaction3 = L.Linear(4)\n",
    "\n",
    "    def __call__(self, input_array, dists, pairs_index, targets):\n",
    "\n",
    "        out = self.predict(input_array, dists, pairs_index)\n",
    "        loss = F.mean_absolute_error(out, targets)\n",
    "        reporter.report({'loss': loss}, self)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, input_array, dists, pairs_index, **kwargs):\n",
    "\n",
    "        h = self.gn(input_array)\n",
    "\n",
    "        for l in range(self.num_layer):\n",
    "            h = self['sch{}'.format(l)](h, dists)\n",
    "\n",
    "        h = F.concat((h, input_array), axis=2)\n",
    "\n",
    "        concat = F.concat([\n",
    "            h[pairs_index[:, 0], pairs_index[:, 1], :],\n",
    "            h[pairs_index[:, 0], pairs_index[:, 2], :],\n",
    "            F.expand_dims(dists[pairs_index[:, 0],\n",
    "                                pairs_index[:, 1],\n",
    "                                pairs_index[:, 2]], 1)\n",
    "        ], axis=1)\n",
    "\n",
    "        h1 = F.leaky_relu(self.interaction1(concat))\n",
    "        h2 = F.leaky_relu(self.interaction2(h1))\n",
    "        out = self.interaction3(h2)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = SchNet(num_layer=3)\n",
    "model.to_gpu(device=DEVICE)\n",
    "\n",
    "########################\n",
    "## MAKE SAMPLERS\n",
    "########################\n",
    "\n",
    "from chainer.iterators import OrderSampler\n",
    "\n",
    "class SameSizeSampler(OrderSampler):\n",
    "\n",
    "    def __init__(self, structures_groups, moles, batch_size,\n",
    "                 random_state=None, use_remainder=False):\n",
    "\n",
    "        self.structures_groups = structures_groups\n",
    "        self.moles = moles\n",
    "        self.batch_size = batch_size\n",
    "        if random_state is None:\n",
    "            random_state = np.random.random.__self__\n",
    "        self._random = random_state\n",
    "        self.use_remainder = use_remainder\n",
    "\n",
    "    def __call__(self, current_order, current_position):\n",
    "\n",
    "        batches = list()\n",
    "\n",
    "        atom_counts = pd.DataFrame()\n",
    "        atom_counts['mol_index'] = np.arange(len(self.moles))\n",
    "        atom_counts['molecular_name'] = self.moles\n",
    "        atom_counts['num_atom'] = [len(self.structures_groups.get_group(mol))\n",
    "                                   for mol in self.moles]\n",
    "\n",
    "        num_atom_counts = atom_counts['num_atom'].value_counts()\n",
    "\n",
    "        for count, num_mol in num_atom_counts.to_dict().items():\n",
    "            if self.use_remainder:\n",
    "                num_batch_for_this = -(-num_mol // self.batch_size)\n",
    "            else:\n",
    "                num_batch_for_this = num_mol // self.batch_size\n",
    "\n",
    "            target_mols = atom_counts.query('num_atom==@count')['mol_index'].values\n",
    "            random.shuffle(target_mols)\n",
    "\n",
    "            devider = np.arange(0, len(target_mols), self.batch_size)\n",
    "            devider = np.append(devider, 99999)\n",
    "\n",
    "            if self.use_remainder:\n",
    "                target_mols = np.append(\n",
    "                    target_mols,\n",
    "                    np.repeat(target_mols[-1], -len(target_mols) % self.batch_size))\n",
    "\n",
    "            for b in range(num_batch_for_this):\n",
    "                batches.append(target_mols[devider[b]:devider[b + 1]])\n",
    "\n",
    "        random.shuffle(batches)\n",
    "        batches = np.concatenate(batches).astype(np.int32)\n",
    "\n",
    "        return batches\n",
    "\n",
    "print('Creating samplers')\n",
    "batch_size = BATCH_SIZE\n",
    "train_sampler = SameSizeSampler(structures_groups, train_moles, batch_size)\n",
    "valid_sampler = SameSizeSampler(structures_groups, valid_moles, batch_size,\n",
    "                                use_remainder=True)\n",
    "test_sampler = SameSizeSampler(structures_groups, test_moles, batch_size,\n",
    "                               use_remainder=True)\n",
    "\n",
    "\n",
    "############################\n",
    "## MAKE ITERATORS, \n",
    "\n",
    "############################\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(\n",
    "    train_dataset, batch_size, order_sampler=train_sampler)\n",
    "\n",
    "valid_iter = chainer.iterators.SerialIterator(\n",
    "    valid_dataset, batch_size, repeat=False, order_sampler=valid_sampler)\n",
    "\n",
    "test_iter = chainer.iterators.SerialIterator(\n",
    "    test_dataset, batch_size, repeat=False, order_sampler=test_sampler)\n",
    "\n",
    "from chainer import optimizers\n",
    "print('Creating optimizer')\n",
    "optimizer = optimizers.Adam(alpha=ALPHA)\n",
    "optimizer.setup(model)\n",
    "\n",
    "###############################\n",
    "# MAKE UPDATER\n",
    "###############################\n",
    "\n",
    "from chainer import training\n",
    "from chainer.dataset import to_device\n",
    "\n",
    "def coupling_converter(batch, device):\n",
    "\n",
    "    list_array = list()\n",
    "    list_dists = list()\n",
    "    list_targets = list()\n",
    "    list_pairs_index = list()\n",
    "\n",
    "    with_target = 'fc' in batch[0]['targets'].columns\n",
    "\n",
    "    for i, d in enumerate(batch):\n",
    "        list_array.append(d['graphs'].input_array)\n",
    "        list_dists.append(d['graphs'].dists)\n",
    "        if with_target:\n",
    "            list_targets.append(\n",
    "                d['targets'][['fc', 'sd', 'pso', 'dso']].values.astype(np.float32))\n",
    "\n",
    "        sample_index = np.full((len(d['targets']), 1), i)\n",
    "        atom_index = d['targets'][['atom_index_0', 'atom_index_1']].values\n",
    "\n",
    "        list_pairs_index.append(np.concatenate([sample_index, atom_index], axis=1))\n",
    "\n",
    "    input_array = to_device(device, np.stack(list_array))\n",
    "    dists = to_device(device, np.stack(list_dists))\n",
    "    pairs_index = np.concatenate(list_pairs_index)\n",
    "\n",
    "    array = {'input_array': input_array, 'dists': dists, 'pairs_index': pairs_index}\n",
    "\n",
    "    if with_target:\n",
    "        array['targets'] = to_device(device, np.concatenate(list_targets))\n",
    "\n",
    "    return array\n",
    "\n",
    "print('Creating updater and trainer')\n",
    "updater = training.StandardUpdater(train_iter, optimizer,\n",
    "                                   converter=coupling_converter, device=DEVICE)\n",
    "trainer = training.Trainer(updater, (200, 'epoch'), out=\"result\")\n",
    "\n",
    "\n",
    "#############\n",
    "## EVALUATOR\n",
    "#############\n",
    "\n",
    "from chainer.training.extensions import Evaluator\n",
    "from chainer import cuda\n",
    "\n",
    "class TypeWiseEvaluator(Evaluator):\n",
    "\n",
    "    def __init__(self, iterator, target, converter, device, name,\n",
    "                 is_validate=False, is_submit=False):\n",
    "\n",
    "        super(TypeWiseEvaluator, self).__init__(\n",
    "            iterator, target, converter=converter, device=device)\n",
    "\n",
    "        self.is_validate = is_validate\n",
    "        self.is_submit = is_submit\n",
    "        self.name = name\n",
    "\n",
    "    def calc_score(self, df_truth, pred):\n",
    "\n",
    "        target_types = list(set(df_truth['type']))\n",
    "\n",
    "        diff = df_truth['scalar_coupling_constant'] - pred\n",
    "\n",
    "        scores = 0\n",
    "        metrics = {}\n",
    "\n",
    "        for target_type in target_types:\n",
    "\n",
    "            target_pair = df_truth['type'] == target_type\n",
    "            score_exp = np.mean(np.abs(diff[target_pair]))\n",
    "            scores += np.log(score_exp)\n",
    "            metrics[target_type] = np.log(score_exp)\n",
    "\n",
    "\n",
    "        metrics['ALL_LogMAE'] = scores / len(target_types)\n",
    "\n",
    "        observation = {}\n",
    "        with reporter.report_scope(observation):\n",
    "            reporter.report(metrics, self._targets['main'])\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def evaluate(self):\n",
    "        iterator = self._iterators['main']\n",
    "        eval_func = self._targets['main']\n",
    "\n",
    "        iterator.reset()\n",
    "        it = iterator\n",
    "\n",
    "        y_total = []\n",
    "        t_total = []\n",
    "\n",
    "        for batch in it:\n",
    "            in_arrays = self.converter(batch, self.device)\n",
    "            with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "                y = eval_func.predict(**in_arrays)\n",
    "\n",
    "            y_data = cuda.to_cpu(y.data)\n",
    "            y_total.append(y_data)\n",
    "            t_total.extend([d['targets'] for d in batch])\n",
    "\n",
    "        df_truth = pd.concat(t_total, axis=0)\n",
    "        y_pred = np.sum(np.concatenate(y_total), axis=1)\n",
    "\n",
    "        if self.is_submit:\n",
    "            submit = pd.DataFrame()\n",
    "            submit['id'] = df_truth['id']\n",
    "            submit['scalar_coupling_constant'] = y_pred\n",
    "            submit.drop_duplicates(subset='id', inplace=True)\n",
    "            submit.sort_values('id', inplace=True)\n",
    "            submit.to_csv(f'kernel_schnet_{MODEL_NUMBER}.csv', index=False)\n",
    "\n",
    "        if self.is_validate:\n",
    "            return self.calc_score(df_truth, y_pred)\n",
    "\n",
    "        return {}\n",
    "\n",
    "print('Extending trainer with evaluators')\n",
    "\n",
    "trainer.extend(\n",
    "    TypeWiseEvaluator(iterator=valid_iter, target=model, converter=coupling_converter, \n",
    "                      name='valid', device=DEVICE, is_validate=True))\n",
    "trainer.extend(\n",
    "    TypeWiseEvaluator(iterator=test_iter, target=model, converter=coupling_converter,\n",
    "                      name='test', device=DEVICE, is_submit=True))\n",
    "\n",
    "\n",
    "#######################\n",
    "# Snapshot\n",
    "########################\n",
    "print('Extending trainer with snapshot writer')\n",
    "from chainer.training import extensions\n",
    "writer = extensions.snapshot_writers.ProcessWriter()\n",
    "trainer.extend(extensions.snapshot(filename=f'SCHNET{MODEL_NUMBER}' + '_epoch_{.updater.epoch}.mod',\n",
    "                                    writer=writer),\n",
    "                                    trigger=(1, 'epoch'))\n",
    "\n",
    "######################\n",
    "## OTHER EXTENTIONS\n",
    "######################\n",
    "\n",
    "print('Extending trainer with ExponentialShift')\n",
    "\n",
    "trainer.extend(training.extensions.ExponentialShift('alpha', 0.99999))\n",
    "\n",
    "from chainer.training import make_extension\n",
    "\n",
    "def stop_train_mode(trigger):\n",
    "    @make_extension(trigger=trigger)\n",
    "    def _stop_train_mode(_):\n",
    "        chainer.config.train = False\n",
    "    return _stop_train_mode\n",
    "\n",
    "trainer.extend(stop_train_mode(trigger=(1, 'epoch')))\n",
    "\n",
    "trainer.extend(\n",
    "    training.extensions.observe_value(\n",
    "        'alpha', lambda tr: tr.updater.get_optimizer('main').alpha))\n",
    "\n",
    "trainer.extend(training.extensions.LogReport(filename=f'SCHNET_{MODEL_NUMBER}.log'))\n",
    "trainer.extend(training.extensions.PrintReport(\n",
    "    ['epoch', 'elapsed_time', 'main/loss', 'valid/main/ALL_LogMAE',\n",
    "     'valid/main/1JHC', 'valid/main/2JHC', 'valid/main/3JHC', 'valid/main/1JHN',\n",
    "     'valid/main/2JHN', 'valid/main/3JHN', 'valid/main/2JHH', 'valid/main/3JHH', 'alpha']))\n",
    "\n",
    "#########\n",
    "# Progress Bar\n",
    "##########\n",
    "print('Extending trainer with progress bar')\n",
    "\n",
    "trainer.extend(chainer.training.extensions.ProgressBar())\n",
    "\n",
    "###### LOAD PREVIOUS MODEL\n",
    "print('Loading trainer from snapshot...')\n",
    "\n",
    "#trainer.load('results/SCHNET104_epoch_8')\n",
    "chainer.serializers.load_npz('../scripts/schnet/result/SCHNET101_epoch_17', trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x7ff788903a20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.updater.get_optimizer('main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exshift = trainer.get_extension('ExponentialShift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exshift.initialize(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = trainer.updater.get_optimizer('main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.updater.get_optimizer('main').alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.training.extensions.exponential_shift.ExponentialShift at 0x7ff7886c7198>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
