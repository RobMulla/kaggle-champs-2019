{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/EdsonAvelar/auc/master/molecular_banner.png\" width=1900px height=400px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Molecular Properties\n",
    "\n",
    "<h3 style=\"color:red\">If this Kernel Helps You! Please UP VOTE! üòÅ</h3>\n",
    "\n",
    "<h3> Can you measure the magnetic interactions between a pair of atoms? </h3>\n",
    "\n",
    "This kernel is a combination of multiple kernels. The goal is to organize and explain the code to beginner competitors like me.<br>\n",
    "This Kernels creates lots of new features and uses lightgbm as model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "**1. [Problem Definition](#id1)** <br>\n",
    "**2. [Get the Data (Collect / Obtain)](#id2)** <br>\n",
    "**3. [Load the Dataset](#id3)** <br>\n",
    "**4. [Data Pre-processing](#id4)** <br>\n",
    "**5. [Model](#id5)** <br>\n",
    "**6. [Visualization and Analysis of Results](#id6)** <br>\n",
    "**7. [Submittion](#id7)** <br>\n",
    "**8. [References](#ref)** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id1\"></a> <br> \n",
    "# **1. Problem Definition:** \n",
    "\n",
    "This challenge aims to predict interactions between atoms. The main task is develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant)<br>\n",
    "\n",
    "In this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.\n",
    "\n",
    "**Data**\n",
    "* **train.csv** - the training set, where the first column (molecule_name) is the name of the molecule where the coupling constant originates, the second (atom_index_0) and third column (atom_index_1) is the atom indices of the atom-pair creating the coupling and the fourth column (**scalar_coupling_constant**) is the scalar coupling constant that we want to be able to predict\n",
    "* **test.csv** - the test set; same info as train, without the target variable\n",
    "* **sample_submission.csv** - a sample submission file in the correct format\n",
    "* **structures.csv** - this file contains the same information as the individual xyz structure files, but in a single file\n",
    "\n",
    "**Additional Data**<br>\n",
    "*NOTE: additional data is provided for the molecules in Train only!*\n",
    "* **scalar_coupling_contributions.csv** - The scalar coupling constants in train.csv are a sum of four terms. The first column (**molecule_name**) are the name of the molecule, the second (**atom_index_0**) and third column (**atom_index_1**) are the atom indices of the atom-pair, the fourth column indicates the **type** of coupling, the fifth column (**fc**) is the Fermi Contact contribution, the sixth column (**sd**) is the Spin-dipolar contribution, the seventh column (**pso**) is the Paramagnetic spin-orbit contribution and the eighth column (**dso**) is the Diamagnetic spin-orbit contribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id2\"></a> <br> \n",
    "# **2. Get the Data (Collect / Obtain):** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All imports used in this kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('notebook')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "import altair as alt\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import altair as alt\n",
    "from altair.vega import v3\n",
    "from IPython.display import HTML\n",
    "alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All function used in this kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>    requirejs.config({\n",
       "        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
       "        paths: {'vega': 'https://cdn.jsdelivr.net/npm/vega@v3.3.1?noext', 'vega-lib': 'https://cdn.jsdelivr.net/npm/vega-lib?noext', 'vega-lite': 'https://cdn.jsdelivr.net/npm/vega-lite@v2.6.0?noext', 'vega-embed': 'https://cdn.jsdelivr.net/npm/vega-embed@3?noext'}\n",
       "    });\n",
       "    </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n",
    "def prepare_altair():\n",
    "    \"\"\"\n",
    "    Helper function to prepare altair for working.\n",
    "    \"\"\"\n",
    "    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\n",
    "    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n",
    "    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n",
    "    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n",
    "    noext = \"?noext\"\n",
    "    \n",
    "    paths = {\n",
    "        'vega': vega_url + noext,\n",
    "        'vega-lib': vega_lib_url + noext,\n",
    "        'vega-lite': vega_lite_url + noext,\n",
    "        'vega-embed': vega_embed_url + noext\n",
    "    }\n",
    "    \n",
    "    workaround = f\"\"\"    requirejs.config({{\n",
    "        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
    "        paths: {paths}\n",
    "    }});\n",
    "    \"\"\"\n",
    "    \n",
    "    return workaround\n",
    "    \n",
    "\n",
    "def add_autoincrement(render_func):\n",
    "    # Keep track of unique <div/> IDs\n",
    "    cache = {}\n",
    "    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n",
    "        if autoincrement:\n",
    "            if id in cache:\n",
    "                counter = 1 + cache[id]\n",
    "                cache[id] = counter\n",
    "            else:\n",
    "                cache[id] = 0\n",
    "            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n",
    "        else:\n",
    "            if id not in cache:\n",
    "                cache[id] = 0\n",
    "            actual_id = id\n",
    "        return render_func(chart, id=actual_id)\n",
    "    # Cache will stay outside and \n",
    "    return wrapped\n",
    "           \n",
    "\n",
    "@add_autoincrement\n",
    "def render(chart, id=\"vega-chart\"):\n",
    "    \"\"\"\n",
    "    Helper function to plot altair visualizations.\n",
    "    \"\"\"\n",
    "    chart_str = \"\"\"\n",
    "    <div id=\"{id}\"></div><script>\n",
    "    require([\"vega-embed\"], function(vg_embed) {{\n",
    "        const spec = {chart};     \n",
    "        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n",
    "        console.log(\"anything?\");\n",
    "    }});\n",
    "    console.log(\"really...anything?\");\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(\n",
    "        chart_str.format(\n",
    "            id=id,\n",
    "            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "    \n",
    "\n",
    "@jit\n",
    "def fast_auc(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    nfalse = 0\n",
    "    auc = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n):\n",
    "        y_i = y_true[i]\n",
    "        nfalse += (1 - y_i)\n",
    "        auc += y_i * nfalse\n",
    "    auc /= (nfalse * (n - nfalse))\n",
    "    return auc\n",
    "\n",
    "\n",
    "def eval_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast auc eval function for lgb.\n",
    "    \"\"\"\n",
    "    return 'auc', fast_auc(y_true, y_pred), True\n",
    "\n",
    "\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
    "    \n",
    "\n",
    "def train_model_regression(X, X_test, y, params, folds, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n",
    "    \n",
    "\n",
    "\n",
    "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns == None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n",
    "                        'catboost_metric_name': 'AUC',\n",
    "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
    "                    }\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros((len(X), len(set(y.values))))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros((len(X_test), oof.shape[1]))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid)\n",
    "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid\n",
    "        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "# setting up altair\n",
    "workaround = prepare_altair()\n",
    "HTML(\"\".join((\n",
    "    \"<script>\",\n",
    "    workaround,\n",
    "    \"</script>\",\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id3\"></a> <br> \n",
    "# **3. Load the Dataset** \n",
    "\n",
    "Let's load all necessary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape is -> rows: 4658147 cols:6\n",
      "Test dataset shape is  -> rows: 2505542 cols:5\n",
      "Sub dataset shape is  -> rows: 2505542 cols:2\n",
      "Structures dataset shape is  -> rows: 2358657 cols:6\n",
      "Scalar_coupling_contributions dataset shape is  -> rows: 4658147 cols:8\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "structures = pd.read_csv('../input/structures.csv')\n",
    "scalar_coupling_contributions = pd.read_csv('../input/scalar_coupling_contributions.csv')\n",
    "\n",
    "print('Train dataset shape is -> rows: {} cols:{}'.format(train.shape[0],train.shape[1]))\n",
    "print('Test dataset shape is  -> rows: {} cols:{}'.format(test.shape[0],test.shape[1]))\n",
    "print('Sub dataset shape is  -> rows: {} cols:{}'.format(sub.shape[0],sub.shape[1]))\n",
    "print('Structures dataset shape is  -> rows: {} cols:{}'.format(structures.shape[0],structures.shape[1]))\n",
    "print('Scalar_coupling_contributions dataset shape is  -> rows: {} cols:{}'.format(scalar_coupling_contributions.shape[0],\n",
    "                                                                                   scalar_coupling_contributions.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an fast model/feature evaluation, get only 10% of dataset. Final submission must remove/coments this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_default = 40000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsize = round(0.10*train.shape[0])\\ntrain = train[:size]\\ntest = test[:size]\\nsub = sub[:size]\\nstructures = structures[:size]\\nscalar_coupling_contributions = scalar_coupling_contributions[:size]\\n\\nprint('Train dataset shape is now rows: {} cols:{}'.format(train.shape[0],train.shape[1]))\\nprint('Test dataset shape is now rows: {} cols:{}'.format(test.shape[0],test.shape[1]))\\nprint('Sub dataset shape is now rows: {} cols:{}'.format(sub.shape[0],sub.shape[1]))\\nprint('Structures dataset shape is now rows: {} cols:{}'.format(structures.shape[0],structures.shape[1]))\\nprint('Scalar_coupling_contributions dataset shape is now rows: {} cols:{}'.format(scalar_coupling_contributions.shape[0],\\n                                                                                   scalar_coupling_contributions.shape[1]))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "size = round(0.10*train.shape[0])\n",
    "train = train[:size]\n",
    "test = test[:size]\n",
    "sub = sub[:size]\n",
    "structures = structures[:size]\n",
    "scalar_coupling_contributions = scalar_coupling_contributions[:size]\n",
    "\n",
    "print('Train dataset shape is now rows: {} cols:{}'.format(train.shape[0],train.shape[1]))\n",
    "print('Test dataset shape is now rows: {} cols:{}'.format(test.shape[0],test.shape[1]))\n",
    "print('Sub dataset shape is now rows: {} cols:{}'.format(sub.shape[0],sub.shape[1]))\n",
    "print('Structures dataset shape is now rows: {} cols:{}'.format(structures.shape[0],structures.shape[1]))\n",
    "print('Scalar_coupling_contributions dataset shape is now rows: {} cols:{}'.format(scalar_coupling_contributions.shape[0],\n",
    "                                                                                   scalar_coupling_contributions.shape[1]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importante things to know is that the scalar coupling constants in train.csv are a sum of four terms. \n",
    "```\n",
    "* fc is the Fermi Contact contribution\n",
    "* sd is the Spin-dipolar contribution\n",
    "* pso is the Paramagnetic spin-orbit contribution\n",
    "* dso is the Diamagnetic spin-orbit contribution. \n",
    "```\n",
    "Let's merge this into train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, scalar_coupling_contributions, how = 'left',\n",
    "                  left_on  = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'],\n",
    "                  right_on = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807599999999994</td>\n",
       "      <td>83.022400000000005</td>\n",
       "      <td>0.254579</td>\n",
       "      <td>1.25862</td>\n",
       "      <td>0.272010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.257000000000000</td>\n",
       "      <td>-11.034700000000001</td>\n",
       "      <td>0.352978</td>\n",
       "      <td>2.85839</td>\n",
       "      <td>-3.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254799999999999</td>\n",
       "      <td>-11.032500000000001</td>\n",
       "      <td>0.352944</td>\n",
       "      <td>2.85852</td>\n",
       "      <td>-3.433870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300000000001</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "      <td>0.352934</td>\n",
       "      <td>2.85855</td>\n",
       "      <td>-3.433930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807400000000001</td>\n",
       "      <td>83.022199999999998</td>\n",
       "      <td>0.254585</td>\n",
       "      <td>1.25861</td>\n",
       "      <td>0.272013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254099999999999</td>\n",
       "      <td>-11.031700000000001</td>\n",
       "      <td>0.352932</td>\n",
       "      <td>2.85856</td>\n",
       "      <td>-3.433950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254799999999999</td>\n",
       "      <td>-11.032400000000001</td>\n",
       "      <td>0.352943</td>\n",
       "      <td>2.85853</td>\n",
       "      <td>-3.433870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809299999999993</td>\n",
       "      <td>83.024100000000004</td>\n",
       "      <td>0.254634</td>\n",
       "      <td>1.25856</td>\n",
       "      <td>0.272012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300000000001</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "      <td>0.352943</td>\n",
       "      <td>2.85856</td>\n",
       "      <td>-3.433930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809500000000000</td>\n",
       "      <td>83.024299999999997</td>\n",
       "      <td>0.254628</td>\n",
       "      <td>1.25856</td>\n",
       "      <td>0.272012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     molecule_name  atom_index_0  atom_index_1  type  scalar_coupling_constant                  fc        sd      pso       dso\n",
       "0   0  dsgdb9nsd_000001             1             0  1JHC        84.807599999999994  83.022400000000005  0.254579  1.25862  0.272010\n",
       "1   1  dsgdb9nsd_000001             1             2  2JHH       -11.257000000000000 -11.034700000000001  0.352978  2.85839 -3.433600\n",
       "2   2  dsgdb9nsd_000001             1             3  2JHH       -11.254799999999999 -11.032500000000001  0.352944  2.85852 -3.433870\n",
       "3   3  dsgdb9nsd_000001             1             4  2JHH       -11.254300000000001 -11.031900000000000  0.352934  2.85855 -3.433930\n",
       "4   4  dsgdb9nsd_000001             2             0  1JHC        84.807400000000001  83.022199999999998  0.254585  1.25861  0.272013\n",
       "5   5  dsgdb9nsd_000001             2             3  2JHH       -11.254099999999999 -11.031700000000001  0.352932  2.85856 -3.433950\n",
       "6   6  dsgdb9nsd_000001             2             4  2JHH       -11.254799999999999 -11.032400000000001  0.352943  2.85853 -3.433870\n",
       "7   7  dsgdb9nsd_000001             3             0  1JHC        84.809299999999993  83.024100000000004  0.254634  1.25856  0.272012\n",
       "8   8  dsgdb9nsd_000001             3             4  2JHH       -11.254300000000001 -11.031900000000000  0.352943  2.85856 -3.433930\n",
       "9   9  dsgdb9nsd_000001             4             0  1JHC        84.809500000000000  83.024299999999997  0.254628  1.25856  0.272012"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4658152</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4658153</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4658154</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4658155</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4658156</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     molecule_name  atom_index_0  atom_index_1  type\n",
       "0  4658147  dsgdb9nsd_000004             2             0  2JHC\n",
       "1  4658148  dsgdb9nsd_000004             2             1  1JHC\n",
       "2  4658149  dsgdb9nsd_000004             2             3  3JHH\n",
       "3  4658150  dsgdb9nsd_000004             3             0  1JHC\n",
       "4  4658151  dsgdb9nsd_000004             3             1  2JHC\n",
       "5  4658152  dsgdb9nsd_000015             3             0  1JHC\n",
       "6  4658153  dsgdb9nsd_000015             3             2  3JHC\n",
       "7  4658154  dsgdb9nsd_000015             3             4  2JHH\n",
       "8  4658155  dsgdb9nsd_000015             3             5  2JHH\n",
       "9  4658156  dsgdb9nsd_000015             4             0  1JHC"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>83.022400000000005</td>\n",
       "      <td>0.254579</td>\n",
       "      <td>1.25862</td>\n",
       "      <td>0.272010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.034700000000001</td>\n",
       "      <td>0.352978</td>\n",
       "      <td>2.85839</td>\n",
       "      <td>-3.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.032500000000001</td>\n",
       "      <td>0.352944</td>\n",
       "      <td>2.85852</td>\n",
       "      <td>-3.433870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "      <td>0.352934</td>\n",
       "      <td>2.85855</td>\n",
       "      <td>-3.433930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>83.022199999999998</td>\n",
       "      <td>0.254585</td>\n",
       "      <td>1.25861</td>\n",
       "      <td>0.272013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  atom_index_0  atom_index_1  type                  fc        sd      pso       dso\n",
       "0  dsgdb9nsd_000001             1             0  1JHC  83.022400000000005  0.254579  1.25862  0.272010\n",
       "1  dsgdb9nsd_000001             1             2  2JHH -11.034700000000001  0.352978  2.85839 -3.433600\n",
       "2  dsgdb9nsd_000001             1             3  2JHH -11.032500000000001  0.352944  2.85852 -3.433870\n",
       "3  dsgdb9nsd_000001             1             4  2JHH -11.031900000000000  0.352934  2.85855 -3.433930\n",
       "4  dsgdb9nsd_000001             2             0  1JHC  83.022199999999998  0.254585  1.25861  0.272013"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_coupling_contributions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train['scalar_coupling_constant'] and scalar_coupling_contributions['fc']` quite similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>fc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.807599999999994</td>\n",
       "      <td>83.022400000000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-11.257000000000000</td>\n",
       "      <td>-11.034700000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-11.254799999999999</td>\n",
       "      <td>-11.032500000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-11.254300000000001</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.807400000000001</td>\n",
       "      <td>83.022199999999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-11.254099999999999</td>\n",
       "      <td>-11.031700000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-11.254799999999999</td>\n",
       "      <td>-11.032400000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>84.809299999999993</td>\n",
       "      <td>83.024100000000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-11.254300000000001</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>84.809500000000000</td>\n",
       "      <td>83.024299999999997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scalar_coupling_constant                  fc\n",
       "0        84.807599999999994  83.022400000000005\n",
       "1       -11.257000000000000 -11.034700000000001\n",
       "2       -11.254799999999999 -11.032500000000001\n",
       "3       -11.254300000000001 -11.031900000000000\n",
       "4        84.807400000000001  83.022199999999998\n",
       "5       -11.254099999999999 -11.031700000000001\n",
       "6       -11.254799999999999 -11.032400000000001\n",
       "7        84.809299999999993  83.024100000000004\n",
       "8       -11.254300000000001 -11.031900000000000\n",
       "9        84.809500000000000  83.024299999999997"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(objs=[train['scalar_coupling_constant'],scalar_coupling_contributions['fc'] ],axis=1)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based in others ideais we can:<br>\n",
    "\n",
    "- train a model to predict `fc` feature;\n",
    "- add this feature to train and test and train the same model to compare performance;\n",
    "- train a better model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id4\"></a> <br> \n",
    "# **4. Data Pre-processing** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use this great kernel to get x,y,z position. https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 512299/2358657 [00:00<00:00, 2507352.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 0.43, 'C': 0.8200000000000001, 'N': 0.8, 'O': 0.78, 'F': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2358657/2358657 [00:00<00:00, 3146128.16it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2358657/2358657 [00:00<00:00, 3165034.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>EN</th>\n",
       "      <th>rad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.0126981359</td>\n",
       "      <td>1.0858041580</td>\n",
       "      <td>0.0080009958</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.0021504160</td>\n",
       "      <td>-0.0060313176</td>\n",
       "      <td>0.0019761204</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0117308430</td>\n",
       "      <td>1.4637511620</td>\n",
       "      <td>0.0002765748</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5408150690</td>\n",
       "      <td>1.4475266140</td>\n",
       "      <td>-0.8766437152</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5238136345</td>\n",
       "      <td>1.4379326440</td>\n",
       "      <td>0.9063972942</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  atom_index atom             x             y             z    EN   rad\n",
       "0  dsgdb9nsd_000001           0    C -0.0126981359  1.0858041580  0.0080009958  2.55  0.82\n",
       "1  dsgdb9nsd_000001           1    H  0.0021504160 -0.0060313176  0.0019761204  2.20  0.43\n",
       "2  dsgdb9nsd_000001           2    H  1.0117308430  1.4637511620  0.0002765748  2.20  0.43\n",
       "3  dsgdb9nsd_000001           3    H -0.5408150690  1.4475266140 -0.8766437152  2.20  0.43\n",
       "4  dsgdb9nsd_000001           4    H -0.5238136345  1.4379326440  0.9063972942  2.20  0.43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "atomic_radius = {'H':0.38, 'C':0.77, 'N':0.75, 'O':0.73, 'F':0.71} # Without fudge factor\n",
    "\n",
    "fudge_factor = 0.05\n",
    "atomic_radius = {k:v + fudge_factor for k,v in atomic_radius.items()}\n",
    "print(atomic_radius)\n",
    "\n",
    "electronegativity = {'H':2.2, 'C':2.55, 'N':3.04, 'O':3.44, 'F':3.98}\n",
    "\n",
    "#structures = pd.read_csv(structures, dtype={'atom_index':np.int8})\n",
    "\n",
    "atoms = structures['atom'].values\n",
    "atoms_en = [electronegativity[x] for x in tqdm(atoms)]\n",
    "atoms_rad = [atomic_radius[x] for x in tqdm(atoms)]\n",
    "\n",
    "structures['EN'] = atoms_en\n",
    "structures['rad'] = atoms_rad\n",
    "\n",
    "display(structures.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chemical Bond Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating bonds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:06<00:00,  4.36it/s]\n",
      "  2%|‚ñè         | 35508/2358657 [00:00<00:13, 177340.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting and condensing bonds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2358657/2358657 [00:13<00:00, 169771.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2358657/2358657 [00:16<00:00, 139822.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>EN</th>\n",
       "      <th>rad</th>\n",
       "      <th>n_bonds</th>\n",
       "      <th>bond_lengths_mean</th>\n",
       "      <th>bond_lengths_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.0126981359</td>\n",
       "      <td>1.0858041580</td>\n",
       "      <td>0.0080009958</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4</td>\n",
       "      <td>1.091949701309204</td>\n",
       "      <td>0.000002762467830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.0021504160</td>\n",
       "      <td>-0.0060313176</td>\n",
       "      <td>0.0019761204</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091953039169312</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0117308430</td>\n",
       "      <td>1.4637511620</td>\n",
       "      <td>0.0002765748</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091951608657837</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5408150690</td>\n",
       "      <td>1.4475266140</td>\n",
       "      <td>-0.8766437152</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091946363449097</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5238136345</td>\n",
       "      <td>1.4379326440</td>\n",
       "      <td>0.9063972942</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091947555541992</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.0404260543</td>\n",
       "      <td>1.0241077530</td>\n",
       "      <td>0.0625637998</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1.017194986343384</td>\n",
       "      <td>0.000009144474461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.0172574639</td>\n",
       "      <td>0.0125452063</td>\n",
       "      <td>-0.0273771593</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.017189979553223</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>0.9157893661</td>\n",
       "      <td>1.3587451950</td>\n",
       "      <td>-0.0287577581</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.017187237739563</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5202777357</td>\n",
       "      <td>1.3435321260</td>\n",
       "      <td>-0.7755426124</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.017207860946655</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dsgdb9nsd_000003</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.0343604951</td>\n",
       "      <td>0.9775395708</td>\n",
       "      <td>0.0076015923</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2</td>\n",
       "      <td>0.962106823921204</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dsgdb9nsd_000003</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.0647664923</td>\n",
       "      <td>0.0205721989</td>\n",
       "      <td>0.0015346341</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.962106823921204</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dsgdb9nsd_000003</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>0.8717903737</td>\n",
       "      <td>1.3007924050</td>\n",
       "      <td>0.0006931336</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.962106823921204</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.5995394918</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>1.130589008331299</td>\n",
       "      <td>0.068489968776703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.5995394918</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>1.130589008331299</td>\n",
       "      <td>0.068489968776703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>-1.6616385860</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062099099159241</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>1.6616385860</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062099099159241</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dsgdb9nsd_000005</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.0133239314</td>\n",
       "      <td>1.1324657150</td>\n",
       "      <td>0.0082758861</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>1.109173059463501</td>\n",
       "      <td>0.042574942111969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dsgdb9nsd_000005</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0023107217</td>\n",
       "      <td>-0.0191585871</td>\n",
       "      <td>0.0019287305</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1.151747941970825</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dsgdb9nsd_000005</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.0278026991</td>\n",
       "      <td>2.1989492960</td>\n",
       "      <td>0.0141537903</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.066598057746887</td>\n",
       "      <td>0.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dsgdb9nsd_000007</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.0187040036</td>\n",
       "      <td>1.5255820150</td>\n",
       "      <td>0.0104328082</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4</td>\n",
       "      <td>1.203627109527588</td>\n",
       "      <td>0.188217103481293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       molecule_name  atom_index atom             x             y             z    EN   rad  n_bonds  bond_lengths_mean   bond_lengths_std\n",
       "0   dsgdb9nsd_000001           0    C -0.0126981359  1.0858041580  0.0080009958  2.55  0.82        4  1.091949701309204  0.000002762467830\n",
       "1   dsgdb9nsd_000001           1    H  0.0021504160 -0.0060313176  0.0019761204  2.20  0.43        1  1.091953039169312  0.000000000000000\n",
       "2   dsgdb9nsd_000001           2    H  1.0117308430  1.4637511620  0.0002765748  2.20  0.43        1  1.091951608657837  0.000000000000000\n",
       "3   dsgdb9nsd_000001           3    H -0.5408150690  1.4475266140 -0.8766437152  2.20  0.43        1  1.091946363449097  0.000000000000000\n",
       "4   dsgdb9nsd_000001           4    H -0.5238136345  1.4379326440  0.9063972942  2.20  0.43        1  1.091947555541992  0.000000000000000\n",
       "5   dsgdb9nsd_000002           0    N -0.0404260543  1.0241077530  0.0625637998  3.04  0.80        3  1.017194986343384  0.000009144474461\n",
       "6   dsgdb9nsd_000002           1    H  0.0172574639  0.0125452063 -0.0273771593  2.20  0.43        1  1.017189979553223  0.000000000000000\n",
       "7   dsgdb9nsd_000002           2    H  0.9157893661  1.3587451950 -0.0287577581  2.20  0.43        1  1.017187237739563  0.000000000000000\n",
       "8   dsgdb9nsd_000002           3    H -0.5202777357  1.3435321260 -0.7755426124  2.20  0.43        1  1.017207860946655  0.000000000000000\n",
       "9   dsgdb9nsd_000003           0    O -0.0343604951  0.9775395708  0.0076015923  3.44  0.78        2  0.962106823921204  0.000000000000000\n",
       "10  dsgdb9nsd_000003           1    H  0.0647664923  0.0205721989  0.0015346341  2.20  0.43        1  0.962106823921204  0.000000000000000\n",
       "11  dsgdb9nsd_000003           2    H  0.8717903737  1.3007924050  0.0006931336  2.20  0.43        1  0.962106823921204  0.000000000000000\n",
       "12  dsgdb9nsd_000004           0    C  0.5995394918  0.0000000000  1.0000000000  2.55  0.82        2  1.130589008331299  0.068489968776703\n",
       "13  dsgdb9nsd_000004           1    C -0.5995394918  0.0000000000  1.0000000000  2.55  0.82        2  1.130589008331299  0.068489968776703\n",
       "14  dsgdb9nsd_000004           2    H -1.6616385860  0.0000000000  1.0000000000  2.20  0.43        1  1.062099099159241  0.000000000000000\n",
       "15  dsgdb9nsd_000004           3    H  1.6616385860  0.0000000000  1.0000000000  2.20  0.43        1  1.062099099159241  0.000000000000000\n",
       "16  dsgdb9nsd_000005           0    C -0.0133239314  1.1324657150  0.0082758861  2.55  0.82        2  1.109173059463501  0.042574942111969\n",
       "17  dsgdb9nsd_000005           1    N  0.0023107217 -0.0191585871  0.0019287305  3.04  0.80        1  1.151747941970825  0.000000000000000\n",
       "18  dsgdb9nsd_000005           2    H -0.0278026991  2.1989492960  0.0141537903  2.20  0.43        1  1.066598057746887  0.000000000000000\n",
       "19  dsgdb9nsd_000007           0    C -0.0187040036  1.5255820150  0.0104328082  2.55  0.82        4  1.203627109527588  0.188217103481293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_atom = structures['atom_index'].values\n",
    "p = structures[['x', 'y', 'z']].values\n",
    "p_compare = p\n",
    "m = structures['molecule_name'].values\n",
    "m_compare = m\n",
    "r = structures['rad'].values\n",
    "r_compare = r\n",
    "\n",
    "source_row = np.arange(len(structures))\n",
    "max_atoms = 28\n",
    "\n",
    "bonds = np.zeros((len(structures)+1, max_atoms+1), dtype=np.int8)\n",
    "bond_dists = np.zeros((len(structures)+1, max_atoms+1), dtype=np.float32)\n",
    "\n",
    "print('Calculating bonds')\n",
    "\n",
    "for i in tqdm(range(max_atoms-1)):\n",
    "    p_compare = np.roll(p_compare, -1, axis=0)\n",
    "    m_compare = np.roll(m_compare, -1, axis=0)\n",
    "    r_compare = np.roll(r_compare, -1, axis=0)\n",
    "    \n",
    "    mask = np.where(m == m_compare, 1, 0) #Are we still comparing atoms in the same molecule?\n",
    "    dists = np.linalg.norm(p - p_compare, axis=1) * mask\n",
    "    r_bond = r + r_compare\n",
    "    \n",
    "    bond = np.where(np.logical_and(dists > 0.0001, dists < r_bond), 1, 0)\n",
    "    \n",
    "    source_row = source_row\n",
    "    target_row = source_row + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "    target_row = np.where(np.logical_or(target_row > len(structures), mask==0), len(structures), target_row) #If invalid target, write to dummy row\n",
    "    \n",
    "    source_atom = i_atom\n",
    "    target_atom = i_atom + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "    target_atom = np.where(np.logical_or(target_atom > max_atoms, mask==0), max_atoms, target_atom) #If invalid target, write to dummy col\n",
    "    \n",
    "    bonds[(source_row, target_atom)] = bond\n",
    "    bonds[(target_row, source_atom)] = bond\n",
    "    bond_dists[(source_row, target_atom)] = dists\n",
    "    bond_dists[(target_row, source_atom)] = dists\n",
    "\n",
    "bonds = np.delete(bonds, axis=0, obj=-1) #Delete dummy row\n",
    "bonds = np.delete(bonds, axis=1, obj=-1) #Delete dummy col\n",
    "bond_dists = np.delete(bond_dists, axis=0, obj=-1) #Delete dummy row\n",
    "bond_dists = np.delete(bond_dists, axis=1, obj=-1) #Delete dummy col\n",
    "\n",
    "print('Counting and condensing bonds')\n",
    "\n",
    "bonds_numeric = [[i for i,x in enumerate(row) if x] for row in tqdm(bonds)]\n",
    "bond_lengths = [[dist for i,dist in enumerate(row) if i in bonds_numeric[j]] for j,row in enumerate(tqdm(bond_dists))]\n",
    "bond_lengths_mean = [ np.mean(x) for x in bond_lengths]\n",
    "bond_lengths_std = [ np.std(x) for x in bond_lengths]\n",
    "n_bonds = [len(x) for x in bonds_numeric]\n",
    "\n",
    "#bond_data = {'bond_' + str(i):col for i, col in enumerate(np.transpose(bonds))}\n",
    "#bond_data.update({'bonds_numeric':bonds_numeric, 'n_bonds':n_bonds})\n",
    "\n",
    "bond_data = {'n_bonds':n_bonds, 'bond_lengths_mean': bond_lengths_mean,'bond_lengths_std':bond_lengths_std }\n",
    "bond_df = pd.DataFrame(bond_data)\n",
    "structures = structures.join(bond_df)\n",
    "display(structures.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df, atom_idx):\n",
    "    df = pd.merge(df, structures, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    #df = df.drop('atom_index', axis=1)\n",
    "    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "train = map_atom_info(train, 0)\n",
    "train = map_atom_info(train, 1)\n",
    "\n",
    "test = map_atom_info(test, 0)\n",
    "test = map_atom_info(test, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the distance between atoms first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "train_p_0 = train[['x_0', 'y_0', 'z_0']].values\n",
    "train_p_1 = train[['x_1', 'y_1', 'z_1']].values\n",
    "test_p_0 = test[['x_0', 'y_0', 'z_0']].values\n",
    "test_p_1 = test[['x_1', 'y_1', 'z_1']].values\n",
    "\n",
    "train['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\n",
    "test['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n",
    "train['dist_x'] = (train['x_0'] - train['x_1']) ** 2\n",
    "test['dist_x'] = (test['x_0'] - test['x_1']) ** 2\n",
    "train['dist_y'] = (train['y_0'] - train['y_1']) ** 2\n",
    "test['dist_y'] = (test['y_0'] - test['y_1']) ** 2\n",
    "train['dist_z'] = (train['z_0'] - train['z_1']) ** 2\n",
    "test['dist_z'] = (test['z_0'] - test['z_1']) ** 2\n",
    "\n",
    "train['type_0'] = train['type'].apply(lambda x: x[0])\n",
    "test['type_0'] = test['type'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 950.66 Mb (69.9% reduction)\n",
      "Mem. usage decreased to 477.89 Mb (70.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "def create_features(df):\n",
    "    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n",
    "    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n",
    "    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n",
    "    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n",
    "    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n",
    "    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n",
    "    df[f'molecule_atom_index_0_x_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['x_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_y_1_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_y_1_mean_diff'] = df[f'molecule_atom_index_0_y_1_mean'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_mean_div'] = df[f'molecule_atom_index_0_y_1_mean'] / df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_max'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('max')\n",
    "    df[f'molecule_atom_index_0_y_1_max_diff'] = df[f'molecule_atom_index_0_y_1_max'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_z_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['z_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_mean_div'] = df[f'molecule_atom_index_0_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_0_dist_max_diff'] = df[f'molecule_atom_index_0_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max_div'] = df[f'molecule_atom_index_0_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min_div'] = df[f'molecule_atom_index_0_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_std_diff'] = df[f'molecule_atom_index_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std_div'] = df[f'molecule_atom_index_0_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean_div'] = df[f'molecule_atom_index_1_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_1_dist_max_diff'] = df[f'molecule_atom_index_1_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max_div'] = df[f'molecule_atom_index_1_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min_div'] = df[f'molecule_atom_index_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_1_dist_std_diff'] = df[f'molecule_atom_index_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std_div'] = df[f'molecule_atom_index_1_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_mean'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_1_dist_min'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_1_dist_min_diff'] = df[f'molecule_atom_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_1_dist_min_div'] = df[f'molecule_atom_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_std'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_1_dist_std_diff'] = df[f'molecule_atom_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_0_dist_std'] = df.groupby(['molecule_name', 'type_0'])['dist'].transform('std')\n",
    "    df[f'molecule_type_0_dist_std_diff'] = df[f'molecule_type_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean_div'] = df[f'molecule_type_dist_mean'] / df['dist']\n",
    "    df[f'molecule_type_dist_max'] = df.groupby(['molecule_name', 'type'])['dist'].transform('max')\n",
    "    df[f'molecule_type_dist_min'] = df.groupby(['molecule_name', 'type'])['dist'].transform('min')\n",
    "    df[f'molecule_type_dist_std'] = df.groupby(['molecule_name', 'type'])['dist'].transform('std')\n",
    "    df[f'molecule_type_dist_std_diff'] = df[f'molecule_type_dist_std'] - df['dist']\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "train = create_features(train)\n",
    "test = create_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df_1,df_2, atom_idx):\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_closest(df_train):\n",
    "    #I apologize for my poor coding skill. Please make the better one.\n",
    "    df_temp=df_train.loc[:,[\"molecule_name\",\"atom_index_0\",\"atom_index_1\",\"dist\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "    df_temp_=df_temp.copy()\n",
    "    df_temp_= df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "    df_temp=pd.concat(objs=[df_temp,df_temp_],axis=0)\n",
    "\n",
    "    df_temp[\"min_distance\"]=df_temp.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df_temp= df_temp[df_temp[\"min_distance\"]==df_temp[\"dist\"]]\n",
    "\n",
    "    df_temp=df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n",
    "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                     'atom_index_1': 'atom_index_closest',\n",
    "                                     'distance': 'distance_closest',\n",
    "                                     'x_1': 'x_closest',\n",
    "                                     'y_1': 'y_closest',\n",
    "                                     'z_1': 'z_closest'})\n",
    "\n",
    "    for atom_idx in [0,1]:\n",
    "        df_train = map_atom_info(df_train,df_temp, atom_idx)\n",
    "        df_train = df_train.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                            'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                            'x_closest': f'x_closest_{atom_idx}',\n",
    "                                            'y_closest': f'y_closest_{atom_idx}',\n",
    "                                            'z_closest': f'z_closest_{atom_idx}'})\n",
    "    return df_train\n",
    "\n",
    "#dtrain = create_closest(train)\n",
    "#dtest = create_closest(test)\n",
    "#print('dtrain size',dtrain.shape)\n",
    "#print('dtest size',dtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine angles calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cos_features(df):\n",
    "    df[\"distance_0\"]=((df['x_0']-df['x_closest_0'])**2+(df['y_0']-df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1/2)\n",
    "    df[\"distance_1\"]=((df['x_1']-df['x_closest_1'])**2+(df['y_1']-df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1/2)\n",
    "    df[\"vec_0_x\"]=(df['x_0']-df['x_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_y\"]=(df['y_0']-df['y_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_z\"]=(df['z_0']-df['z_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_1_x\"]=(df['x_1']-df['x_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_y\"]=(df['y_1']-df['y_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_z\"]=(df['z_1']-df['z_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_x\"]=(df['x_1']-df['x_0'])/df[\"dist\"]\n",
    "    df[\"vec_y\"]=(df['y_1']-df['y_0'])/df[\"dist\"]\n",
    "    df[\"vec_z\"]=(df['z_1']-df['z_0'])/df[\"dist\"]\n",
    "    df[\"cos_0_1\"]=df[\"vec_0_x\"]*df[\"vec_1_x\"]+df[\"vec_0_y\"]*df[\"vec_1_y\"]+df[\"vec_0_z\"]*df[\"vec_1_z\"]\n",
    "    df[\"cos_0\"]=df[\"vec_0_x\"]*df[\"vec_x\"]+df[\"vec_0_y\"]*df[\"vec_y\"]+df[\"vec_0_z\"]*df[\"vec_z\"]\n",
    "    df[\"cos_1\"]=df[\"vec_1_x\"]*df[\"vec_x\"]+df[\"vec_1_y\"]*df[\"vec_y\"]+df[\"vec_1_z\"]*df[\"vec_z\"]\n",
    "    df=df.drop(['vec_0_x','vec_0_y','vec_0_z','vec_1_x','vec_1_y','vec_1_z','vec_x','vec_y','vec_z'], axis=1)\n",
    "    return df\n",
    "    \n",
    "#train = add_cos_features(train)\n",
    "#test = add_cos_features(test)\n",
    "\n",
    "#print('train size',train.shape)\n",
    "#print('test size',test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping molecule_name and encode atom_0, atom_1 and type_0.<br>\n",
    "**@TODO:** Try others encoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_cols_list = ['id','molecule_name','sd','pso','dso']\n",
    "def del_cols(df, cols):\n",
    "    del_cols_list_ = [l for l in del_cols_list if l in df]\n",
    "    df = df.drop(del_cols_list_,axis=1)\n",
    "    return df\n",
    "\n",
    "train = del_cols(train,del_cols_list)\n",
    "test = del_cols(test,del_cols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categoric_single(df):\n",
    "    lbl = LabelEncoder()\n",
    "    cat_cols=[]\n",
    "    try:\n",
    "        cat_cols = df.describe(include=['O']).columns.tolist()\n",
    "        for cat in cat_cols:\n",
    "            df[cat] = lbl.fit_transform(list(df[cat].values))\n",
    "    except Exception as e:\n",
    "        print('error: ', str(e) )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categoric(dtrain,dtest):\n",
    "    lbl = LabelEncoder()\n",
    "    objs_n = len(dtrain)\n",
    "    dfmerge = pd.concat(objs=[dtrain,dtest],axis=0)\n",
    "    cat_cols=[]\n",
    "    try:\n",
    "        cat_cols = dfmerge.describe(include=['O']).columns.tolist()\n",
    "        for cat in cat_cols:\n",
    "            dfmerge[cat] = lbl.fit_transform(list(dfmerge[cat].values))\n",
    "    except Exception as e:\n",
    "        print('error: ', str(e) )\n",
    "\n",
    "    dtrain = dfmerge[:objs_n]\n",
    "    dtest = dfmerge[objs_n:]\n",
    "    return dtrain,dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = encode_categoric_single(train)\n",
    "test = encode_categoric_single(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fc = train['fc']\n",
    "X = train.drop(['scalar_coupling_constant','fc'],axis=1)\n",
    "y = train['scalar_coupling_constant']\n",
    "\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size (4658147, 81)\n",
      "X_test size (2505542, 81)\n",
      "dtest size (2505542, 81)\n",
      "y_fc size (4658147,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X size',X.shape)\n",
    "print('X_test size',X_test.shape)\n",
    "print('dtest size',test.shape)\n",
    "print('y_fc size',y_fc.shape)\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_columns = ['type',\n",
    " 'bond_lengths_mean_y',\n",
    " 'bond_lengths_std_y',\n",
    " 'bond_lengths_mean_x',\n",
    " 'molecule_atom_index_0_dist_min_div',\n",
    " 'molecule_atom_index_0_dist_std_div',\n",
    " 'molecule_atom_index_0_dist_mean',\n",
    " 'molecule_atom_index_0_dist_max',\n",
    " 'dist_y',\n",
    " 'molecule_atom_index_1_dist_std_diff',\n",
    " 'z_0',\n",
    " 'molecule_type_dist_min',\n",
    " 'molecule_atom_index_0_y_1_mean_div',\n",
    " 'dist_x',\n",
    " 'x_0',\n",
    " 'y_0',\n",
    " 'molecule_type_dist_std',\n",
    " 'molecule_atom_index_0_y_1_std',\n",
    " 'molecule_dist_mean',\n",
    " 'molecule_atom_index_0_dist_std_diff',\n",
    " 'dist_z',\n",
    " 'molecule_atom_index_0_dist_std',\n",
    " 'molecule_atom_index_0_x_1_std',\n",
    " 'molecule_type_dist_std_diff',\n",
    " 'molecule_type_0_dist_std',\n",
    " 'dist',\n",
    " 'molecule_atom_index_0_dist_mean_diff',\n",
    " 'molecule_atom_index_1_dist_min_div',\n",
    " 'molecule_atom_index_1_dist_mean_diff',\n",
    " 'y_1',\n",
    " 'molecule_type_dist_mean_div',\n",
    " 'molecule_dist_max',\n",
    " 'molecule_atom_index_0_dist_mean_div',\n",
    " 'z_1',\n",
    " 'molecule_atom_index_0_z_1_std',\n",
    " 'molecule_atom_index_1_dist_mean_div',\n",
    " 'molecule_atom_index_1_dist_min_diff',\n",
    " 'molecule_atom_index_1_dist_mean',\n",
    " 'molecule_atom_index_1_dist_min',\n",
    " 'molecule_atom_index_1_dist_max',\n",
    " 'molecule_type_0_dist_std_diff',\n",
    " 'molecule_atom_index_0_dist_min_diff',\n",
    " 'molecule_type_dist_mean_diff',\n",
    " 'x_1',\n",
    " 'molecule_atom_index_0_y_1_max',\n",
    " 'molecule_atom_index_0_y_1_mean_diff',\n",
    " 'molecule_atom_1_dist_std_diff',\n",
    " 'molecule_atom_index_0_y_1_mean',\n",
    " 'molecule_atom_1_dist_std',\n",
    " 'molecule_type_dist_max']\n",
    "\n",
    "X = X[good_columns].copy()\n",
    "X_test = X_test[good_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id5\"></a> <br> \n",
    "# **5. Model** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=529)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create out of fold feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 50,\n",
    "          'min_child_samples': 79,\n",
    "          'min_data_in_leaf': 100,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.2,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of type 0\n",
      "Fold 1 started at Mon Jun 24 13:02:12 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.66062\tvalid_1's l1: 1.80967\n",
      "[1000]\ttraining's l1: 1.42089\tvalid_1's l1: 1.66497\n",
      "[1500]\ttraining's l1: 1.27228\tvalid_1's l1: 1.59704\n",
      "[2000]\ttraining's l1: 1.15874\tvalid_1's l1: 1.55092\n",
      "[2500]\ttraining's l1: 1.06732\tvalid_1's l1: 1.51955\n",
      "[3000]\ttraining's l1: 0.990048\tvalid_1's l1: 1.49616\n",
      "[3500]\ttraining's l1: 0.923692\tvalid_1's l1: 1.47901\n",
      "[4000]\ttraining's l1: 0.864388\tvalid_1's l1: 1.46403\n",
      "[4500]\ttraining's l1: 0.811321\tvalid_1's l1: 1.45174\n",
      "[5000]\ttraining's l1: 0.763467\tvalid_1's l1: 1.44132\n",
      "[5500]\ttraining's l1: 0.720757\tvalid_1's l1: 1.43334\n",
      "[6000]\ttraining's l1: 0.681302\tvalid_1's l1: 1.42701\n",
      "[6500]\ttraining's l1: 0.644752\tvalid_1's l1: 1.42041\n",
      "[7000]\ttraining's l1: 0.611297\tvalid_1's l1: 1.41496\n",
      "[7500]\ttraining's l1: 0.580285\tvalid_1's l1: 1.41\n",
      "[8000]\ttraining's l1: 0.551526\tvalid_1's l1: 1.40597\n",
      "[8500]\ttraining's l1: 0.524705\tvalid_1's l1: 1.40241\n",
      "[9000]\ttraining's l1: 0.499497\tvalid_1's l1: 1.39903\n",
      "[9500]\ttraining's l1: 0.475606\tvalid_1's l1: 1.3958\n",
      "[10000]\ttraining's l1: 0.453568\tvalid_1's l1: 1.39265\n",
      "[10500]\ttraining's l1: 0.432746\tvalid_1's l1: 1.39035\n",
      "[11000]\ttraining's l1: 0.412933\tvalid_1's l1: 1.38812\n",
      "[11500]\ttraining's l1: 0.394489\tvalid_1's l1: 1.38602\n",
      "[12000]\ttraining's l1: 0.37727\tvalid_1's l1: 1.3843\n",
      "[12500]\ttraining's l1: 0.360857\tvalid_1's l1: 1.38268\n",
      "[13000]\ttraining's l1: 0.345475\tvalid_1's l1: 1.38103\n",
      "[13500]\ttraining's l1: 0.330743\tvalid_1's l1: 1.37961\n",
      "[14000]\ttraining's l1: 0.316573\tvalid_1's l1: 1.37853\n",
      "[14500]\ttraining's l1: 0.303195\tvalid_1's l1: 1.37731\n",
      "[15000]\ttraining's l1: 0.290691\tvalid_1's l1: 1.37624\n",
      "[15500]\ttraining's l1: 0.278697\tvalid_1's l1: 1.37503\n",
      "[16000]\ttraining's l1: 0.267271\tvalid_1's l1: 1.37411\n",
      "[16500]\ttraining's l1: 0.256342\tvalid_1's l1: 1.3732\n",
      "[17000]\ttraining's l1: 0.245909\tvalid_1's l1: 1.37237\n",
      "[17500]\ttraining's l1: 0.23613\tvalid_1's l1: 1.37146\n",
      "[18000]\ttraining's l1: 0.226714\tvalid_1's l1: 1.37076\n",
      "[18500]\ttraining's l1: 0.217802\tvalid_1's l1: 1.3701\n",
      "[19000]\ttraining's l1: 0.209317\tvalid_1's l1: 1.36943\n",
      "[19500]\ttraining's l1: 0.201113\tvalid_1's l1: 1.36887\n",
      "[20000]\ttraining's l1: 0.193357\tvalid_1's l1: 1.36835\n",
      "[20500]\ttraining's l1: 0.185865\tvalid_1's l1: 1.36774\n",
      "[21000]\ttraining's l1: 0.178748\tvalid_1's l1: 1.36718\n",
      "[21500]\ttraining's l1: 0.171938\tvalid_1's l1: 1.36677\n",
      "[22000]\ttraining's l1: 0.165423\tvalid_1's l1: 1.36638\n",
      "[22500]\ttraining's l1: 0.159171\tvalid_1's l1: 1.36594\n",
      "[23000]\ttraining's l1: 0.153205\tvalid_1's l1: 1.36562\n",
      "[23500]\ttraining's l1: 0.147546\tvalid_1's l1: 1.36527\n",
      "[24000]\ttraining's l1: 0.142077\tvalid_1's l1: 1.36488\n",
      "[24500]\ttraining's l1: 0.136834\tvalid_1's l1: 1.3645\n",
      "[25000]\ttraining's l1: 0.131763\tvalid_1's l1: 1.36419\n",
      "[25500]\ttraining's l1: 0.126876\tvalid_1's l1: 1.36382\n",
      "[26000]\ttraining's l1: 0.122295\tvalid_1's l1: 1.36355\n",
      "[26500]\ttraining's l1: 0.117895\tvalid_1's l1: 1.36331\n",
      "[27000]\ttraining's l1: 0.113654\tvalid_1's l1: 1.36304\n",
      "[27500]\ttraining's l1: 0.109564\tvalid_1's l1: 1.36285\n",
      "[28000]\ttraining's l1: 0.10563\tvalid_1's l1: 1.36273\n",
      "[28500]\ttraining's l1: 0.101876\tvalid_1's l1: 1.36258\n",
      "[29000]\ttraining's l1: 0.0982782\tvalid_1's l1: 1.36239\n",
      "[29500]\ttraining's l1: 0.0948148\tvalid_1's l1: 1.3622\n",
      "[30000]\ttraining's l1: 0.0915073\tvalid_1's l1: 1.36203\n",
      "[30500]\ttraining's l1: 0.0882946\tvalid_1's l1: 1.36188\n",
      "[31000]\ttraining's l1: 0.085194\tvalid_1's l1: 1.36168\n",
      "[31500]\ttraining's l1: 0.0821831\tvalid_1's l1: 1.3615\n",
      "[32000]\ttraining's l1: 0.0793443\tvalid_1's l1: 1.36138\n",
      "[32500]\ttraining's l1: 0.0766005\tvalid_1's l1: 1.36126\n",
      "[33000]\ttraining's l1: 0.0739556\tvalid_1's l1: 1.36114\n",
      "[33500]\ttraining's l1: 0.0714342\tvalid_1's l1: 1.36103\n",
      "[34000]\ttraining's l1: 0.0689802\tvalid_1's l1: 1.36091\n",
      "[34500]\ttraining's l1: 0.0666083\tvalid_1's l1: 1.36077\n",
      "[35000]\ttraining's l1: 0.06432\tvalid_1's l1: 1.3607\n",
      "[35500]\ttraining's l1: 0.0621394\tvalid_1's l1: 1.36056\n",
      "[36000]\ttraining's l1: 0.0600777\tvalid_1's l1: 1.36047\n",
      "[36500]\ttraining's l1: 0.0580516\tvalid_1's l1: 1.36035\n",
      "[37000]\ttraining's l1: 0.0561154\tvalid_1's l1: 1.36026\n",
      "[37500]\ttraining's l1: 0.0542437\tvalid_1's l1: 1.3602\n",
      "[38000]\ttraining's l1: 0.0524326\tvalid_1's l1: 1.36013\n",
      "[38500]\ttraining's l1: 0.0506771\tvalid_1's l1: 1.36006\n",
      "[39000]\ttraining's l1: 0.0489878\tvalid_1's l1: 1.36\n",
      "[39500]\ttraining's l1: 0.0473801\tvalid_1's l1: 1.35994\n",
      "[40000]\ttraining's l1: 0.0458159\tvalid_1's l1: 1.35988\n",
      "[40500]\ttraining's l1: 0.0443084\tvalid_1's l1: 1.35984\n",
      "[41000]\ttraining's l1: 0.0428296\tvalid_1's l1: 1.35975\n",
      "[41500]\ttraining's l1: 0.0414257\tvalid_1's l1: 1.3597\n",
      "[42000]\ttraining's l1: 0.0400887\tvalid_1's l1: 1.35965\n",
      "[42500]\ttraining's l1: 0.0387851\tvalid_1's l1: 1.35958\n",
      "[43000]\ttraining's l1: 0.0375194\tvalid_1's l1: 1.35955\n",
      "[43500]\ttraining's l1: 0.0363167\tvalid_1's l1: 1.35946\n",
      "Early stopping, best iteration is:\n",
      "[43639]\ttraining's l1: 0.0359927\tvalid_1's l1: 1.35945\n",
      "Fold 2 started at Mon Jun 24 13:24:04 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.65497\tvalid_1's l1: 1.80404\n",
      "[1000]\ttraining's l1: 1.42432\tvalid_1's l1: 1.66862\n",
      "[1500]\ttraining's l1: 1.27296\tvalid_1's l1: 1.59604\n",
      "[2000]\ttraining's l1: 1.16122\tvalid_1's l1: 1.55115\n",
      "[2500]\ttraining's l1: 1.06875\tvalid_1's l1: 1.51772\n",
      "[3000]\ttraining's l1: 0.99141\tvalid_1's l1: 1.4937\n",
      "[3500]\ttraining's l1: 0.924896\tvalid_1's l1: 1.47555\n",
      "[4000]\ttraining's l1: 0.866456\tvalid_1's l1: 1.46206\n",
      "[4500]\ttraining's l1: 0.813684\tvalid_1's l1: 1.44993\n",
      "[5000]\ttraining's l1: 0.766126\tvalid_1's l1: 1.43985\n",
      "[5500]\ttraining's l1: 0.722973\tvalid_1's l1: 1.43156\n",
      "[6000]\ttraining's l1: 0.683296\tvalid_1's l1: 1.42451\n",
      "[6500]\ttraining's l1: 0.646667\tvalid_1's l1: 1.41734\n",
      "[7000]\ttraining's l1: 0.613088\tvalid_1's l1: 1.41202\n",
      "[7500]\ttraining's l1: 0.581552\tvalid_1's l1: 1.40688\n",
      "[8000]\ttraining's l1: 0.553003\tvalid_1's l1: 1.40292\n",
      "[8500]\ttraining's l1: 0.525598\tvalid_1's l1: 1.39907\n",
      "[9000]\ttraining's l1: 0.500292\tvalid_1's l1: 1.39516\n",
      "[9500]\ttraining's l1: 0.476711\tvalid_1's l1: 1.39212\n",
      "[10000]\ttraining's l1: 0.454434\tvalid_1's l1: 1.38907\n",
      "[10500]\ttraining's l1: 0.433604\tvalid_1's l1: 1.38654\n",
      "[11000]\ttraining's l1: 0.414103\tvalid_1's l1: 1.38405\n",
      "[11500]\ttraining's l1: 0.395737\tvalid_1's l1: 1.38213\n",
      "[12000]\ttraining's l1: 0.378454\tvalid_1's l1: 1.38039\n",
      "[12500]\ttraining's l1: 0.361785\tvalid_1's l1: 1.37858\n",
      "[13000]\ttraining's l1: 0.346201\tvalid_1's l1: 1.37713\n",
      "[13500]\ttraining's l1: 0.331465\tvalid_1's l1: 1.37538\n",
      "[14000]\ttraining's l1: 0.317555\tvalid_1's l1: 1.37402\n",
      "[14500]\ttraining's l1: 0.304298\tvalid_1's l1: 1.37261\n",
      "[15000]\ttraining's l1: 0.291511\tvalid_1's l1: 1.37112\n",
      "[15500]\ttraining's l1: 0.279559\tvalid_1's l1: 1.37001\n",
      "[16000]\ttraining's l1: 0.268069\tvalid_1's l1: 1.36899\n",
      "[16500]\ttraining's l1: 0.257069\tvalid_1's l1: 1.36795\n",
      "[17000]\ttraining's l1: 0.246664\tvalid_1's l1: 1.36704\n",
      "[17500]\ttraining's l1: 0.236708\tvalid_1's l1: 1.36622\n",
      "[18000]\ttraining's l1: 0.227305\tvalid_1's l1: 1.36555\n",
      "[18500]\ttraining's l1: 0.218236\tvalid_1's l1: 1.36494\n",
      "[19000]\ttraining's l1: 0.209653\tvalid_1's l1: 1.36434\n",
      "[19500]\ttraining's l1: 0.201608\tvalid_1's l1: 1.36366\n",
      "[20000]\ttraining's l1: 0.193755\tvalid_1's l1: 1.36316\n",
      "[20500]\ttraining's l1: 0.186287\tvalid_1's l1: 1.36248\n",
      "[21000]\ttraining's l1: 0.179091\tvalid_1's l1: 1.36196\n",
      "[21500]\ttraining's l1: 0.172354\tvalid_1's l1: 1.36146\n",
      "[22000]\ttraining's l1: 0.165802\tvalid_1's l1: 1.36107\n",
      "[22500]\ttraining's l1: 0.159577\tvalid_1's l1: 1.36061\n",
      "[23000]\ttraining's l1: 0.153652\tvalid_1's l1: 1.36026\n",
      "[23500]\ttraining's l1: 0.147977\tvalid_1's l1: 1.35989\n",
      "[24000]\ttraining's l1: 0.142496\tvalid_1's l1: 1.3596\n",
      "[24500]\ttraining's l1: 0.137211\tvalid_1's l1: 1.35921\n",
      "[25000]\ttraining's l1: 0.132153\tvalid_1's l1: 1.35889\n",
      "[25500]\ttraining's l1: 0.127249\tvalid_1's l1: 1.35855\n",
      "[26000]\ttraining's l1: 0.122589\tvalid_1's l1: 1.3583\n",
      "[26500]\ttraining's l1: 0.118145\tvalid_1's l1: 1.35798\n",
      "[27000]\ttraining's l1: 0.11389\tvalid_1's l1: 1.35774\n",
      "[27500]\ttraining's l1: 0.109784\tvalid_1's l1: 1.35745\n",
      "[28000]\ttraining's l1: 0.105861\tvalid_1's l1: 1.35717\n",
      "[28500]\ttraining's l1: 0.102034\tvalid_1's l1: 1.35698\n",
      "[29000]\ttraining's l1: 0.0984139\tvalid_1's l1: 1.35675\n",
      "[29500]\ttraining's l1: 0.0948836\tvalid_1's l1: 1.35656\n",
      "[30000]\ttraining's l1: 0.0915272\tvalid_1's l1: 1.35636\n",
      "[30500]\ttraining's l1: 0.0883016\tvalid_1's l1: 1.3562\n",
      "[31000]\ttraining's l1: 0.0851965\tvalid_1's l1: 1.35614\n",
      "[31500]\ttraining's l1: 0.0822157\tvalid_1's l1: 1.35602\n",
      "[32000]\ttraining's l1: 0.0793575\tvalid_1's l1: 1.35589\n",
      "[32500]\ttraining's l1: 0.0765951\tvalid_1's l1: 1.35579\n",
      "[33000]\ttraining's l1: 0.0739342\tvalid_1's l1: 1.35569\n",
      "[33500]\ttraining's l1: 0.0713761\tvalid_1's l1: 1.35555\n",
      "[34000]\ttraining's l1: 0.0689243\tvalid_1's l1: 1.35543\n",
      "[34500]\ttraining's l1: 0.0666003\tvalid_1's l1: 1.35532\n",
      "[35000]\ttraining's l1: 0.0643365\tvalid_1's l1: 1.35521\n",
      "[35500]\ttraining's l1: 0.0621088\tvalid_1's l1: 1.35508\n",
      "[36000]\ttraining's l1: 0.0600312\tvalid_1's l1: 1.35497\n",
      "[36500]\ttraining's l1: 0.0579941\tvalid_1's l1: 1.35487\n",
      "[37000]\ttraining's l1: 0.0560321\tvalid_1's l1: 1.35476\n",
      "[37500]\ttraining's l1: 0.0541577\tvalid_1's l1: 1.35465\n",
      "[38000]\ttraining's l1: 0.0523471\tvalid_1's l1: 1.35455\n",
      "[38500]\ttraining's l1: 0.0505987\tvalid_1's l1: 1.35444\n",
      "Early stopping, best iteration is:\n",
      "[38482]\ttraining's l1: 0.0506631\tvalid_1's l1: 1.35443\n",
      "Fold 3 started at Mon Jun 24 13:40:56 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.65309\tvalid_1's l1: 1.79665\n",
      "[1000]\ttraining's l1: 1.41985\tvalid_1's l1: 1.66056\n",
      "[1500]\ttraining's l1: 1.2706\tvalid_1's l1: 1.59179\n",
      "[2000]\ttraining's l1: 1.16083\tvalid_1's l1: 1.55028\n",
      "[2500]\ttraining's l1: 1.06796\tvalid_1's l1: 1.51651\n",
      "[3000]\ttraining's l1: 0.991382\tvalid_1's l1: 1.49404\n",
      "[3500]\ttraining's l1: 0.923796\tvalid_1's l1: 1.47608\n",
      "[4000]\ttraining's l1: 0.864493\tvalid_1's l1: 1.46218\n",
      "[4500]\ttraining's l1: 0.812246\tvalid_1's l1: 1.45099\n",
      "[5000]\ttraining's l1: 0.764368\tvalid_1's l1: 1.44043\n",
      "[5500]\ttraining's l1: 0.720747\tvalid_1's l1: 1.43161\n",
      "[6000]\ttraining's l1: 0.68169\tvalid_1's l1: 1.42482\n",
      "[6500]\ttraining's l1: 0.645383\tvalid_1's l1: 1.41833\n",
      "[7000]\ttraining's l1: 0.612198\tvalid_1's l1: 1.41316\n",
      "[7500]\ttraining's l1: 0.580963\tvalid_1's l1: 1.40873\n",
      "[8000]\ttraining's l1: 0.552284\tvalid_1's l1: 1.40472\n",
      "[8500]\ttraining's l1: 0.524999\tvalid_1's l1: 1.40096\n",
      "[9000]\ttraining's l1: 0.49952\tvalid_1's l1: 1.39757\n",
      "[9500]\ttraining's l1: 0.476308\tvalid_1's l1: 1.39459\n",
      "[10000]\ttraining's l1: 0.454203\tvalid_1's l1: 1.39217\n",
      "[10500]\ttraining's l1: 0.433395\tvalid_1's l1: 1.38964\n",
      "[11000]\ttraining's l1: 0.413933\tvalid_1's l1: 1.38717\n",
      "[11500]\ttraining's l1: 0.395476\tvalid_1's l1: 1.38488\n",
      "[12000]\ttraining's l1: 0.377948\tvalid_1's l1: 1.38328\n",
      "[12500]\ttraining's l1: 0.361514\tvalid_1's l1: 1.38162\n",
      "[13000]\ttraining's l1: 0.345881\tvalid_1's l1: 1.38012\n",
      "[13500]\ttraining's l1: 0.331057\tvalid_1's l1: 1.37871\n",
      "[14000]\ttraining's l1: 0.317062\tvalid_1's l1: 1.3775\n",
      "[14500]\ttraining's l1: 0.303618\tvalid_1's l1: 1.37635\n",
      "[15000]\ttraining's l1: 0.291149\tvalid_1's l1: 1.3751\n",
      "[15500]\ttraining's l1: 0.279129\tvalid_1's l1: 1.3739\n",
      "[16000]\ttraining's l1: 0.267727\tvalid_1's l1: 1.37299\n",
      "[16500]\ttraining's l1: 0.256932\tvalid_1's l1: 1.37209\n",
      "[17000]\ttraining's l1: 0.246501\tvalid_1's l1: 1.37126\n",
      "[17500]\ttraining's l1: 0.236628\tvalid_1's l1: 1.37039\n",
      "[18000]\ttraining's l1: 0.227357\tvalid_1's l1: 1.36979\n",
      "[18500]\ttraining's l1: 0.218399\tvalid_1's l1: 1.36907\n",
      "[19000]\ttraining's l1: 0.209928\tvalid_1's l1: 1.36845\n",
      "[19500]\ttraining's l1: 0.201734\tvalid_1's l1: 1.36781\n",
      "[20000]\ttraining's l1: 0.194038\tvalid_1's l1: 1.36718\n",
      "[20500]\ttraining's l1: 0.186518\tvalid_1's l1: 1.36665\n",
      "[21000]\ttraining's l1: 0.179404\tvalid_1's l1: 1.3663\n",
      "[21500]\ttraining's l1: 0.172527\tvalid_1's l1: 1.36582\n",
      "[22000]\ttraining's l1: 0.166082\tvalid_1's l1: 1.36537\n",
      "[22500]\ttraining's l1: 0.159894\tvalid_1's l1: 1.36498\n",
      "[23000]\ttraining's l1: 0.153838\tvalid_1's l1: 1.36456\n",
      "[23500]\ttraining's l1: 0.148062\tvalid_1's l1: 1.36419\n",
      "[24000]\ttraining's l1: 0.142554\tvalid_1's l1: 1.36378\n",
      "[24500]\ttraining's l1: 0.137275\tvalid_1's l1: 1.36354\n",
      "[25000]\ttraining's l1: 0.132225\tvalid_1's l1: 1.36329\n",
      "[25500]\ttraining's l1: 0.127345\tvalid_1's l1: 1.36302\n",
      "[26000]\ttraining's l1: 0.122695\tvalid_1's l1: 1.36278\n",
      "[26500]\ttraining's l1: 0.118196\tvalid_1's l1: 1.36259\n",
      "[27000]\ttraining's l1: 0.113939\tvalid_1's l1: 1.3623\n",
      "[27500]\ttraining's l1: 0.109835\tvalid_1's l1: 1.36205\n",
      "[28000]\ttraining's l1: 0.10588\tvalid_1's l1: 1.3618\n",
      "[28500]\ttraining's l1: 0.102085\tvalid_1's l1: 1.36159\n",
      "[29000]\ttraining's l1: 0.0984807\tvalid_1's l1: 1.36139\n",
      "[29500]\ttraining's l1: 0.0949318\tvalid_1's l1: 1.36112\n",
      "[30000]\ttraining's l1: 0.0915615\tvalid_1's l1: 1.36096\n",
      "[30500]\ttraining's l1: 0.0883343\tvalid_1's l1: 1.36084\n",
      "[31000]\ttraining's l1: 0.0852398\tvalid_1's l1: 1.36067\n",
      "[31500]\ttraining's l1: 0.0822178\tvalid_1's l1: 1.36056\n",
      "[32000]\ttraining's l1: 0.0793349\tvalid_1's l1: 1.36046\n",
      "[32500]\ttraining's l1: 0.0765828\tvalid_1's l1: 1.36036\n",
      "[33000]\ttraining's l1: 0.0739544\tvalid_1's l1: 1.36023\n",
      "[33500]\ttraining's l1: 0.0713957\tvalid_1's l1: 1.36014\n",
      "[34000]\ttraining's l1: 0.0689587\tvalid_1's l1: 1.36001\n",
      "[34500]\ttraining's l1: 0.0665765\tvalid_1's l1: 1.3599\n",
      "[35000]\ttraining's l1: 0.0643259\tvalid_1's l1: 1.35979\n",
      "[35500]\ttraining's l1: 0.0621614\tvalid_1's l1: 1.35969\n",
      "[36000]\ttraining's l1: 0.0600778\tvalid_1's l1: 1.35958\n",
      "[36500]\ttraining's l1: 0.0580418\tvalid_1's l1: 1.35944\n",
      "[37000]\ttraining's l1: 0.0560939\tvalid_1's l1: 1.35934\n",
      "[37500]\ttraining's l1: 0.0542058\tvalid_1's l1: 1.35922\n",
      "[38000]\ttraining's l1: 0.0524153\tvalid_1's l1: 1.35913\n",
      "[38500]\ttraining's l1: 0.0506608\tvalid_1's l1: 1.35905\n",
      "[39000]\ttraining's l1: 0.048959\tvalid_1's l1: 1.35898\n",
      "[39500]\ttraining's l1: 0.0473402\tvalid_1's l1: 1.3589\n",
      "[40000]\ttraining's l1: 0.0457693\tvalid_1's l1: 1.35883\n",
      "[40500]\ttraining's l1: 0.0442642\tvalid_1's l1: 1.35877\n",
      "[41000]\ttraining's l1: 0.0428186\tvalid_1's l1: 1.35871\n",
      "[41500]\ttraining's l1: 0.0414019\tvalid_1's l1: 1.35866\n",
      "[42000]\ttraining's l1: 0.0400604\tvalid_1's l1: 1.35862\n",
      "[42500]\ttraining's l1: 0.0387566\tvalid_1's l1: 1.35857\n",
      "[43000]\ttraining's l1: 0.0375063\tvalid_1's l1: 1.35853\n",
      "[43500]\ttraining's l1: 0.0362851\tvalid_1's l1: 1.35848\n",
      "[44000]\ttraining's l1: 0.0351292\tvalid_1's l1: 1.35843\n",
      "[44500]\ttraining's l1: 0.0340018\tvalid_1's l1: 1.35839\n",
      "[45000]\ttraining's l1: 0.0329087\tvalid_1's l1: 1.35835\n",
      "[45500]\ttraining's l1: 0.0318466\tvalid_1's l1: 1.3583\n",
      "[46000]\ttraining's l1: 0.0308244\tvalid_1's l1: 1.35825\n",
      "[46500]\ttraining's l1: 0.0298583\tvalid_1's l1: 1.3582\n",
      "[47000]\ttraining's l1: 0.0289029\tvalid_1's l1: 1.35816\n",
      "[47500]\ttraining's l1: 0.0279852\tvalid_1's l1: 1.35812\n",
      "[48000]\ttraining's l1: 0.0271195\tvalid_1's l1: 1.35808\n",
      "[48500]\ttraining's l1: 0.0262631\tvalid_1's l1: 1.35805\n",
      "[49000]\ttraining's l1: 0.0254547\tvalid_1's l1: 1.35803\n",
      "Early stopping, best iteration is:\n",
      "[48807]\ttraining's l1: 0.0257612\tvalid_1's l1: 1.35803\n",
      "Fold 4 started at Mon Jun 24 14:02:27 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.65511\tvalid_1's l1: 1.80154\n",
      "[1000]\ttraining's l1: 1.41748\tvalid_1's l1: 1.66042\n",
      "[1500]\ttraining's l1: 1.26841\tvalid_1's l1: 1.59102\n",
      "[2000]\ttraining's l1: 1.15673\tvalid_1's l1: 1.54842\n",
      "[2500]\ttraining's l1: 1.066\tvalid_1's l1: 1.51893\n",
      "[3000]\ttraining's l1: 0.988603\tvalid_1's l1: 1.49502\n",
      "[3500]\ttraining's l1: 0.921409\tvalid_1's l1: 1.47695\n",
      "[4000]\ttraining's l1: 0.862758\tvalid_1's l1: 1.46363\n",
      "[4500]\ttraining's l1: 0.810401\tvalid_1's l1: 1.45232\n",
      "[5000]\ttraining's l1: 0.762161\tvalid_1's l1: 1.44241\n",
      "[5500]\ttraining's l1: 0.718881\tvalid_1's l1: 1.43385\n",
      "[6000]\ttraining's l1: 0.679658\tvalid_1's l1: 1.42617\n",
      "[6500]\ttraining's l1: 0.643394\tvalid_1's l1: 1.41987\n",
      "[7000]\ttraining's l1: 0.609718\tvalid_1's l1: 1.41428\n",
      "[7500]\ttraining's l1: 0.579098\tvalid_1's l1: 1.40949\n",
      "[8000]\ttraining's l1: 0.550117\tvalid_1's l1: 1.40464\n",
      "[8500]\ttraining's l1: 0.523276\tvalid_1's l1: 1.40024\n",
      "[9000]\ttraining's l1: 0.498059\tvalid_1's l1: 1.39678\n",
      "[9500]\ttraining's l1: 0.47428\tvalid_1's l1: 1.39322\n",
      "[10000]\ttraining's l1: 0.452382\tvalid_1's l1: 1.39076\n",
      "[10500]\ttraining's l1: 0.431743\tvalid_1's l1: 1.38838\n",
      "[11000]\ttraining's l1: 0.412491\tvalid_1's l1: 1.38628\n",
      "[11500]\ttraining's l1: 0.393932\tvalid_1's l1: 1.38408\n",
      "[12000]\ttraining's l1: 0.376552\tvalid_1's l1: 1.38197\n",
      "[12500]\ttraining's l1: 0.36016\tvalid_1's l1: 1.38014\n",
      "[13000]\ttraining's l1: 0.344659\tvalid_1's l1: 1.37855\n",
      "[13500]\ttraining's l1: 0.329836\tvalid_1's l1: 1.37714\n",
      "[14000]\ttraining's l1: 0.315928\tvalid_1's l1: 1.37578\n",
      "[14500]\ttraining's l1: 0.302784\tvalid_1's l1: 1.37449\n",
      "[15000]\ttraining's l1: 0.290179\tvalid_1's l1: 1.3732\n",
      "[15500]\ttraining's l1: 0.278202\tvalid_1's l1: 1.37209\n",
      "[16000]\ttraining's l1: 0.266927\tvalid_1's l1: 1.37118\n",
      "[16500]\ttraining's l1: 0.256124\tvalid_1's l1: 1.37013\n",
      "[17000]\ttraining's l1: 0.245913\tvalid_1's l1: 1.36931\n",
      "[17500]\ttraining's l1: 0.236126\tvalid_1's l1: 1.36851\n",
      "[18000]\ttraining's l1: 0.226657\tvalid_1's l1: 1.36788\n",
      "[18500]\ttraining's l1: 0.217714\tvalid_1's l1: 1.36704\n",
      "[19000]\ttraining's l1: 0.209198\tvalid_1's l1: 1.36642\n",
      "[19500]\ttraining's l1: 0.201067\tvalid_1's l1: 1.36575\n",
      "[20000]\ttraining's l1: 0.193145\tvalid_1's l1: 1.36514\n",
      "[20500]\ttraining's l1: 0.185669\tvalid_1's l1: 1.36477\n",
      "[21000]\ttraining's l1: 0.178594\tvalid_1's l1: 1.36421\n",
      "[21500]\ttraining's l1: 0.171746\tvalid_1's l1: 1.3638\n",
      "[22000]\ttraining's l1: 0.16522\tvalid_1's l1: 1.36328\n",
      "[22500]\ttraining's l1: 0.158971\tvalid_1's l1: 1.3629\n",
      "[23000]\ttraining's l1: 0.153049\tvalid_1's l1: 1.36241\n",
      "[23500]\ttraining's l1: 0.147343\tvalid_1's l1: 1.362\n",
      "[24000]\ttraining's l1: 0.141858\tvalid_1's l1: 1.36166\n",
      "[24500]\ttraining's l1: 0.136658\tvalid_1's l1: 1.36136\n",
      "[25000]\ttraining's l1: 0.131616\tvalid_1's l1: 1.36104\n",
      "[25500]\ttraining's l1: 0.126764\tvalid_1's l1: 1.36082\n",
      "[26000]\ttraining's l1: 0.122131\tvalid_1's l1: 1.36061\n",
      "[26500]\ttraining's l1: 0.117707\tvalid_1's l1: 1.36041\n",
      "[27000]\ttraining's l1: 0.113469\tvalid_1's l1: 1.36013\n",
      "[27500]\ttraining's l1: 0.109414\tvalid_1's l1: 1.35993\n",
      "[28000]\ttraining's l1: 0.105468\tvalid_1's l1: 1.35966\n",
      "[28500]\ttraining's l1: 0.101692\tvalid_1's l1: 1.35946\n",
      "[29000]\ttraining's l1: 0.098038\tvalid_1's l1: 1.35929\n",
      "[29500]\ttraining's l1: 0.094573\tvalid_1's l1: 1.35913\n",
      "[30000]\ttraining's l1: 0.0912184\tvalid_1's l1: 1.35903\n",
      "[30500]\ttraining's l1: 0.0879993\tvalid_1's l1: 1.35881\n",
      "[31000]\ttraining's l1: 0.0848868\tvalid_1's l1: 1.35865\n",
      "[31500]\ttraining's l1: 0.0819517\tvalid_1's l1: 1.35849\n",
      "[32000]\ttraining's l1: 0.0790884\tvalid_1's l1: 1.35832\n",
      "[32500]\ttraining's l1: 0.0763432\tvalid_1's l1: 1.35815\n",
      "[33000]\ttraining's l1: 0.0737508\tvalid_1's l1: 1.35808\n",
      "[33500]\ttraining's l1: 0.0712177\tvalid_1's l1: 1.35795\n",
      "[34000]\ttraining's l1: 0.0687926\tvalid_1's l1: 1.3578\n",
      "[34500]\ttraining's l1: 0.0664329\tvalid_1's l1: 1.35766\n",
      "[35000]\ttraining's l1: 0.0641595\tvalid_1's l1: 1.35757\n",
      "[35500]\ttraining's l1: 0.0619629\tvalid_1's l1: 1.3575\n",
      "[36000]\ttraining's l1: 0.0598757\tvalid_1's l1: 1.35739\n",
      "[36500]\ttraining's l1: 0.0578567\tvalid_1's l1: 1.35727\n",
      "[37000]\ttraining's l1: 0.0558916\tvalid_1's l1: 1.35719\n",
      "[37500]\ttraining's l1: 0.0540298\tvalid_1's l1: 1.35713\n",
      "[38000]\ttraining's l1: 0.0522404\tvalid_1's l1: 1.35705\n",
      "[38500]\ttraining's l1: 0.0505036\tvalid_1's l1: 1.35699\n",
      "[39000]\ttraining's l1: 0.048828\tvalid_1's l1: 1.35687\n",
      "Early stopping, best iteration is:\n",
      "[39129]\ttraining's l1: 0.0484001\tvalid_1's l1: 1.35686\n",
      "Fold 5 started at Mon Jun 24 14:19:18 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.65174\tvalid_1's l1: 1.79252\n",
      "[1000]\ttraining's l1: 1.41775\tvalid_1's l1: 1.65491\n",
      "[1500]\ttraining's l1: 1.26988\tvalid_1's l1: 1.5857\n",
      "[2000]\ttraining's l1: 1.15672\tvalid_1's l1: 1.54085\n",
      "[2500]\ttraining's l1: 1.06551\tvalid_1's l1: 1.51042\n",
      "[3000]\ttraining's l1: 0.989215\tvalid_1's l1: 1.4881\n",
      "[3500]\ttraining's l1: 0.922744\tvalid_1's l1: 1.47025\n",
      "[4000]\ttraining's l1: 0.863997\tvalid_1's l1: 1.45615\n",
      "[4500]\ttraining's l1: 0.81136\tvalid_1's l1: 1.44411\n",
      "[5000]\ttraining's l1: 0.763926\tvalid_1's l1: 1.43436\n",
      "[5500]\ttraining's l1: 0.720571\tvalid_1's l1: 1.42616\n",
      "[6000]\ttraining's l1: 0.681094\tvalid_1's l1: 1.41911\n",
      "[6500]\ttraining's l1: 0.644597\tvalid_1's l1: 1.41265\n",
      "[7000]\ttraining's l1: 0.61132\tvalid_1's l1: 1.40752\n",
      "[7500]\ttraining's l1: 0.580603\tvalid_1's l1: 1.40292\n",
      "[8000]\ttraining's l1: 0.551401\tvalid_1's l1: 1.39846\n",
      "[8500]\ttraining's l1: 0.524615\tvalid_1's l1: 1.39431\n",
      "[9000]\ttraining's l1: 0.499252\tvalid_1's l1: 1.39079\n",
      "[9500]\ttraining's l1: 0.475421\tvalid_1's l1: 1.38804\n",
      "[10000]\ttraining's l1: 0.453306\tvalid_1's l1: 1.38531\n",
      "[10500]\ttraining's l1: 0.432436\tvalid_1's l1: 1.38249\n",
      "[11000]\ttraining's l1: 0.412902\tvalid_1's l1: 1.38008\n",
      "[11500]\ttraining's l1: 0.394508\tvalid_1's l1: 1.37813\n",
      "[12000]\ttraining's l1: 0.376926\tvalid_1's l1: 1.37618\n",
      "[12500]\ttraining's l1: 0.36054\tvalid_1's l1: 1.37443\n",
      "[13000]\ttraining's l1: 0.345163\tvalid_1's l1: 1.37305\n",
      "[13500]\ttraining's l1: 0.330414\tvalid_1's l1: 1.37195\n",
      "[14000]\ttraining's l1: 0.316225\tvalid_1's l1: 1.37066\n",
      "[14500]\ttraining's l1: 0.302883\tvalid_1's l1: 1.36945\n",
      "[15000]\ttraining's l1: 0.290306\tvalid_1's l1: 1.36805\n",
      "[15500]\ttraining's l1: 0.278248\tvalid_1's l1: 1.36698\n",
      "[16000]\ttraining's l1: 0.266772\tvalid_1's l1: 1.36609\n",
      "[16500]\ttraining's l1: 0.255958\tvalid_1's l1: 1.36505\n",
      "[17000]\ttraining's l1: 0.245688\tvalid_1's l1: 1.36416\n",
      "[17500]\ttraining's l1: 0.235737\tvalid_1's l1: 1.36334\n",
      "[18000]\ttraining's l1: 0.226509\tvalid_1's l1: 1.36256\n",
      "[18500]\ttraining's l1: 0.217531\tvalid_1's l1: 1.36189\n",
      "[19000]\ttraining's l1: 0.208915\tvalid_1's l1: 1.3612\n",
      "[19500]\ttraining's l1: 0.200846\tvalid_1's l1: 1.36048\n",
      "[20000]\ttraining's l1: 0.193098\tvalid_1's l1: 1.3599\n",
      "[20500]\ttraining's l1: 0.185551\tvalid_1's l1: 1.35942\n",
      "[21000]\ttraining's l1: 0.178449\tvalid_1's l1: 1.35897\n",
      "[21500]\ttraining's l1: 0.171672\tvalid_1's l1: 1.35846\n",
      "[22000]\ttraining's l1: 0.165169\tvalid_1's l1: 1.35806\n",
      "[22500]\ttraining's l1: 0.158886\tvalid_1's l1: 1.35765\n",
      "[23000]\ttraining's l1: 0.15285\tvalid_1's l1: 1.35723\n",
      "[23500]\ttraining's l1: 0.147155\tvalid_1's l1: 1.3569\n",
      "[24000]\ttraining's l1: 0.141623\tvalid_1's l1: 1.3566\n",
      "[24500]\ttraining's l1: 0.136379\tvalid_1's l1: 1.35627\n",
      "[25000]\ttraining's l1: 0.131284\tvalid_1's l1: 1.35598\n",
      "[25500]\ttraining's l1: 0.126513\tvalid_1's l1: 1.35574\n",
      "[26000]\ttraining's l1: 0.121879\tvalid_1's l1: 1.35552\n",
      "[26500]\ttraining's l1: 0.117454\tvalid_1's l1: 1.35523\n",
      "[27000]\ttraining's l1: 0.113241\tvalid_1's l1: 1.35498\n",
      "[27500]\ttraining's l1: 0.109155\tvalid_1's l1: 1.35477\n",
      "[28000]\ttraining's l1: 0.105254\tvalid_1's l1: 1.35456\n",
      "[28500]\ttraining's l1: 0.101486\tvalid_1's l1: 1.35436\n",
      "[29000]\ttraining's l1: 0.0978531\tvalid_1's l1: 1.35419\n",
      "[29500]\ttraining's l1: 0.0943719\tvalid_1's l1: 1.35406\n",
      "[30000]\ttraining's l1: 0.0910466\tvalid_1's l1: 1.35388\n",
      "[30500]\ttraining's l1: 0.0878381\tvalid_1's l1: 1.35378\n",
      "[31000]\ttraining's l1: 0.0847751\tvalid_1's l1: 1.3536\n",
      "[31500]\ttraining's l1: 0.0818209\tvalid_1's l1: 1.35344\n",
      "[32000]\ttraining's l1: 0.0789689\tvalid_1's l1: 1.35335\n",
      "[32500]\ttraining's l1: 0.0762439\tvalid_1's l1: 1.35324\n",
      "[33000]\ttraining's l1: 0.0736136\tvalid_1's l1: 1.35316\n",
      "[33500]\ttraining's l1: 0.0711048\tvalid_1's l1: 1.35306\n",
      "[34000]\ttraining's l1: 0.0686614\tvalid_1's l1: 1.35293\n",
      "[34500]\ttraining's l1: 0.0663269\tvalid_1's l1: 1.35286\n",
      "[35000]\ttraining's l1: 0.0640159\tvalid_1's l1: 1.35277\n",
      "[35500]\ttraining's l1: 0.0618462\tvalid_1's l1: 1.35266\n",
      "[36000]\ttraining's l1: 0.0597368\tvalid_1's l1: 1.35258\n",
      "[36500]\ttraining's l1: 0.0577381\tvalid_1's l1: 1.3525\n",
      "[37000]\ttraining's l1: 0.0558075\tvalid_1's l1: 1.35239\n",
      "Early stopping, best iteration is:\n",
      "[37162]\ttraining's l1: 0.0552\tvalid_1's l1: 1.35235\n",
      "CV mean score: 0.3047, std: 0.0019.\n",
      "Training of type 3\n",
      "Fold 1 started at Mon Jun 24 14:35:06 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.366772\tvalid_1's l1: 0.434243\n",
      "[1000]\ttraining's l1: 0.298103\tvalid_1's l1: 0.401683\n",
      "[1500]\ttraining's l1: 0.255336\tvalid_1's l1: 0.386591\n",
      "[2000]\ttraining's l1: 0.224019\tvalid_1's l1: 0.377187\n",
      "[2500]\ttraining's l1: 0.198938\tvalid_1's l1: 0.370791\n",
      "[3000]\ttraining's l1: 0.178672\tvalid_1's l1: 0.366389\n",
      "[3500]\ttraining's l1: 0.161308\tvalid_1's l1: 0.363164\n",
      "[4000]\ttraining's l1: 0.146461\tvalid_1's l1: 0.36034\n",
      "[4500]\ttraining's l1: 0.13354\tvalid_1's l1: 0.358183\n",
      "[5000]\ttraining's l1: 0.122211\tvalid_1's l1: 0.356521\n",
      "[5500]\ttraining's l1: 0.112087\tvalid_1's l1: 0.355121\n",
      "[6000]\ttraining's l1: 0.10304\tvalid_1's l1: 0.353901\n",
      "[6500]\ttraining's l1: 0.0949241\tvalid_1's l1: 0.35285\n",
      "[7000]\ttraining's l1: 0.0876493\tvalid_1's l1: 0.352038\n",
      "[7500]\ttraining's l1: 0.0810646\tvalid_1's l1: 0.35128\n",
      "[8000]\ttraining's l1: 0.0750314\tvalid_1's l1: 0.350589\n",
      "[8500]\ttraining's l1: 0.0695818\tvalid_1's l1: 0.350057\n",
      "[9000]\ttraining's l1: 0.06467\tvalid_1's l1: 0.349557\n",
      "[9500]\ttraining's l1: 0.0602116\tvalid_1's l1: 0.349181\n",
      "[10000]\ttraining's l1: 0.0560831\tvalid_1's l1: 0.34879\n",
      "[10500]\ttraining's l1: 0.0522303\tvalid_1's l1: 0.348537\n",
      "[11000]\ttraining's l1: 0.0487026\tvalid_1's l1: 0.348284\n",
      "[11500]\ttraining's l1: 0.0454697\tvalid_1's l1: 0.348063\n",
      "[12000]\ttraining's l1: 0.0424967\tvalid_1's l1: 0.347818\n",
      "[12500]\ttraining's l1: 0.0397449\tvalid_1's l1: 0.347639\n",
      "[13000]\ttraining's l1: 0.0372169\tvalid_1's l1: 0.347462\n",
      "[13500]\ttraining's l1: 0.0348651\tvalid_1's l1: 0.347323\n",
      "[14000]\ttraining's l1: 0.0326871\tvalid_1's l1: 0.347216\n",
      "[14500]\ttraining's l1: 0.0306322\tvalid_1's l1: 0.347148\n",
      "[15000]\ttraining's l1: 0.0287471\tvalid_1's l1: 0.347025\n",
      "[15500]\ttraining's l1: 0.0269921\tvalid_1's l1: 0.346916\n",
      "[16000]\ttraining's l1: 0.0253622\tvalid_1's l1: 0.346808\n",
      "[16500]\ttraining's l1: 0.0238397\tvalid_1's l1: 0.346716\n",
      "[17000]\ttraining's l1: 0.0224169\tvalid_1's l1: 0.346623\n",
      "[17500]\ttraining's l1: 0.0210904\tvalid_1's l1: 0.346553\n",
      "[18000]\ttraining's l1: 0.0198547\tvalid_1's l1: 0.346493\n",
      "[18500]\ttraining's l1: 0.0187137\tvalid_1's l1: 0.346428\n",
      "[19000]\ttraining's l1: 0.0176476\tvalid_1's l1: 0.346376\n",
      "[19500]\ttraining's l1: 0.0166614\tvalid_1's l1: 0.346325\n",
      "[20000]\ttraining's l1: 0.0157367\tvalid_1's l1: 0.346283\n",
      "[20500]\ttraining's l1: 0.0148695\tvalid_1's l1: 0.346234\n",
      "[21000]\ttraining's l1: 0.0140483\tvalid_1's l1: 0.346215\n",
      "[21500]\ttraining's l1: 0.0132856\tvalid_1's l1: 0.346182\n",
      "[22000]\ttraining's l1: 0.0125753\tvalid_1's l1: 0.346141\n",
      "[22500]\ttraining's l1: 0.0119093\tvalid_1's l1: 0.34609\n",
      "[23000]\ttraining's l1: 0.0112951\tvalid_1's l1: 0.346064\n",
      "[23500]\ttraining's l1: 0.0107124\tvalid_1's l1: 0.346031\n",
      "[24000]\ttraining's l1: 0.0101756\tvalid_1's l1: 0.346\n",
      "[24500]\ttraining's l1: 0.00966285\tvalid_1's l1: 0.345979\n",
      "[25000]\ttraining's l1: 0.00917846\tvalid_1's l1: 0.34595\n",
      "[25500]\ttraining's l1: 0.00872354\tvalid_1's l1: 0.34593\n",
      "[26000]\ttraining's l1: 0.00830176\tvalid_1's l1: 0.345913\n",
      "[26500]\ttraining's l1: 0.00790082\tvalid_1's l1: 0.345888\n",
      "Early stopping, best iteration is:\n",
      "[26580]\ttraining's l1: 0.00783994\tvalid_1's l1: 0.345884\n",
      "Fold 2 started at Mon Jun 24 14:46:05 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.368014\tvalid_1's l1: 0.431567\n",
      "[1000]\ttraining's l1: 0.298394\tvalid_1's l1: 0.399292\n",
      "[1500]\ttraining's l1: 0.256173\tvalid_1's l1: 0.384896\n",
      "[2000]\ttraining's l1: 0.224453\tvalid_1's l1: 0.375816\n",
      "[2500]\ttraining's l1: 0.199373\tvalid_1's l1: 0.370089\n",
      "[3000]\ttraining's l1: 0.178765\tvalid_1's l1: 0.365499\n",
      "[3500]\ttraining's l1: 0.161619\tvalid_1's l1: 0.362517\n",
      "[4000]\ttraining's l1: 0.146769\tvalid_1's l1: 0.360024\n",
      "[4500]\ttraining's l1: 0.133946\tvalid_1's l1: 0.358015\n",
      "[5000]\ttraining's l1: 0.122525\tvalid_1's l1: 0.356209\n",
      "[5500]\ttraining's l1: 0.112444\tvalid_1's l1: 0.354864\n",
      "[6000]\ttraining's l1: 0.103405\tvalid_1's l1: 0.353757\n",
      "[6500]\ttraining's l1: 0.0953752\tvalid_1's l1: 0.352908\n",
      "[7000]\ttraining's l1: 0.0879626\tvalid_1's l1: 0.352147\n",
      "[7500]\ttraining's l1: 0.0814398\tvalid_1's l1: 0.351405\n",
      "[8000]\ttraining's l1: 0.0755795\tvalid_1's l1: 0.350795\n",
      "[8500]\ttraining's l1: 0.0700425\tvalid_1's l1: 0.350216\n",
      "[9000]\ttraining's l1: 0.0650423\tvalid_1's l1: 0.349831\n",
      "[9500]\ttraining's l1: 0.0604563\tvalid_1's l1: 0.349509\n",
      "[10000]\ttraining's l1: 0.0562646\tvalid_1's l1: 0.349137\n",
      "[10500]\ttraining's l1: 0.0523659\tvalid_1's l1: 0.348817\n",
      "[11000]\ttraining's l1: 0.0488102\tvalid_1's l1: 0.348592\n",
      "[11500]\ttraining's l1: 0.0455439\tvalid_1's l1: 0.348374\n",
      "[12000]\ttraining's l1: 0.0425731\tvalid_1's l1: 0.348119\n",
      "[12500]\ttraining's l1: 0.0398312\tvalid_1's l1: 0.347928\n",
      "[13000]\ttraining's l1: 0.0372628\tvalid_1's l1: 0.347701\n",
      "[13500]\ttraining's l1: 0.0348995\tvalid_1's l1: 0.347567\n",
      "[14000]\ttraining's l1: 0.0326824\tvalid_1's l1: 0.347422\n",
      "[14500]\ttraining's l1: 0.0306562\tvalid_1's l1: 0.34732\n",
      "[15000]\ttraining's l1: 0.0287644\tvalid_1's l1: 0.347203\n",
      "[15500]\ttraining's l1: 0.0270116\tvalid_1's l1: 0.347126\n",
      "[16000]\ttraining's l1: 0.0253967\tvalid_1's l1: 0.347064\n",
      "[16500]\ttraining's l1: 0.0238765\tvalid_1's l1: 0.346963\n",
      "[17000]\ttraining's l1: 0.0224528\tvalid_1's l1: 0.346889\n",
      "[17500]\ttraining's l1: 0.0211368\tvalid_1's l1: 0.34683\n",
      "[18000]\ttraining's l1: 0.0199179\tvalid_1's l1: 0.346775\n",
      "[18500]\ttraining's l1: 0.0187777\tvalid_1's l1: 0.346711\n",
      "[19000]\ttraining's l1: 0.017722\tvalid_1's l1: 0.346655\n",
      "[19500]\ttraining's l1: 0.0167167\tvalid_1's l1: 0.346603\n",
      "[20000]\ttraining's l1: 0.015779\tvalid_1's l1: 0.34656\n",
      "[20500]\ttraining's l1: 0.0149045\tvalid_1's l1: 0.346507\n",
      "[21000]\ttraining's l1: 0.0140915\tvalid_1's l1: 0.346453\n",
      "[21500]\ttraining's l1: 0.0133342\tvalid_1's l1: 0.346416\n",
      "[22000]\ttraining's l1: 0.0126276\tvalid_1's l1: 0.346383\n",
      "[22500]\ttraining's l1: 0.0119575\tvalid_1's l1: 0.346349\n",
      "[23000]\ttraining's l1: 0.0113271\tvalid_1's l1: 0.346318\n",
      "[23500]\ttraining's l1: 0.0107409\tvalid_1's l1: 0.34629\n",
      "[24000]\ttraining's l1: 0.0101962\tvalid_1's l1: 0.346267\n",
      "[24500]\ttraining's l1: 0.00968052\tvalid_1's l1: 0.346249\n",
      "[25000]\ttraining's l1: 0.00919932\tvalid_1's l1: 0.346216\n",
      "[25500]\ttraining's l1: 0.00874052\tvalid_1's l1: 0.346197\n",
      "[26000]\ttraining's l1: 0.00831282\tvalid_1's l1: 0.346183\n",
      "[26500]\ttraining's l1: 0.00791682\tvalid_1's l1: 0.346168\n",
      "[27000]\ttraining's l1: 0.00754339\tvalid_1's l1: 0.346145\n",
      "[27500]\ttraining's l1: 0.0071872\tvalid_1's l1: 0.346133\n",
      "[28000]\ttraining's l1: 0.00685874\tvalid_1's l1: 0.346113\n",
      "[28500]\ttraining's l1: 0.00654645\tvalid_1's l1: 0.346104\n",
      "[29000]\ttraining's l1: 0.00625467\tvalid_1's l1: 0.346093\n",
      "[29500]\ttraining's l1: 0.0059811\tvalid_1's l1: 0.346088\n",
      "Early stopping, best iteration is:\n",
      "[29344]\ttraining's l1: 0.00606481\tvalid_1's l1: 0.346086\n",
      "Fold 3 started at Mon Jun 24 14:57:49 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.368827\tvalid_1's l1: 0.4344\n",
      "[1000]\ttraining's l1: 0.300266\tvalid_1's l1: 0.402077\n",
      "[1500]\ttraining's l1: 0.256522\tvalid_1's l1: 0.386736\n",
      "[2000]\ttraining's l1: 0.225168\tvalid_1's l1: 0.378215\n",
      "[2500]\ttraining's l1: 0.200129\tvalid_1's l1: 0.371681\n",
      "[3000]\ttraining's l1: 0.17977\tvalid_1's l1: 0.367065\n",
      "[3500]\ttraining's l1: 0.162464\tvalid_1's l1: 0.363573\n",
      "[4000]\ttraining's l1: 0.147611\tvalid_1's l1: 0.360938\n",
      "[4500]\ttraining's l1: 0.134563\tvalid_1's l1: 0.358786\n",
      "[5000]\ttraining's l1: 0.123181\tvalid_1's l1: 0.357062\n",
      "[5500]\ttraining's l1: 0.113007\tvalid_1's l1: 0.355836\n",
      "[6000]\ttraining's l1: 0.104067\tvalid_1's l1: 0.35468\n",
      "[6500]\ttraining's l1: 0.0958912\tvalid_1's l1: 0.353719\n",
      "[7000]\ttraining's l1: 0.0885796\tvalid_1's l1: 0.353003\n",
      "[7500]\ttraining's l1: 0.082012\tvalid_1's l1: 0.352298\n",
      "[8000]\ttraining's l1: 0.0759422\tvalid_1's l1: 0.351826\n",
      "[8500]\ttraining's l1: 0.0705273\tvalid_1's l1: 0.35132\n",
      "[9000]\ttraining's l1: 0.0655286\tvalid_1's l1: 0.350911\n",
      "[9500]\ttraining's l1: 0.0609736\tvalid_1's l1: 0.35057\n",
      "[10000]\ttraining's l1: 0.0567351\tvalid_1's l1: 0.350238\n",
      "[10500]\ttraining's l1: 0.0528757\tvalid_1's l1: 0.349896\n",
      "[11000]\ttraining's l1: 0.0493138\tvalid_1's l1: 0.349637\n",
      "[11500]\ttraining's l1: 0.0460587\tvalid_1's l1: 0.349453\n",
      "[12000]\ttraining's l1: 0.0430085\tvalid_1's l1: 0.349199\n",
      "[12500]\ttraining's l1: 0.0402667\tvalid_1's l1: 0.349042\n",
      "[13000]\ttraining's l1: 0.0376831\tvalid_1's l1: 0.34888\n",
      "[13500]\ttraining's l1: 0.0352888\tvalid_1's l1: 0.348731\n",
      "[14000]\ttraining's l1: 0.0330809\tvalid_1's l1: 0.348552\n",
      "[14500]\ttraining's l1: 0.031022\tvalid_1's l1: 0.348452\n",
      "[15000]\ttraining's l1: 0.0290946\tvalid_1's l1: 0.348325\n",
      "[15500]\ttraining's l1: 0.0273115\tvalid_1's l1: 0.348244\n",
      "[16000]\ttraining's l1: 0.025668\tvalid_1's l1: 0.348149\n",
      "[16500]\ttraining's l1: 0.0241313\tvalid_1's l1: 0.348039\n",
      "[17000]\ttraining's l1: 0.0226828\tvalid_1's l1: 0.347986\n",
      "[17500]\ttraining's l1: 0.02135\tvalid_1's l1: 0.347929\n",
      "[18000]\ttraining's l1: 0.0201124\tvalid_1's l1: 0.347885\n",
      "[18500]\ttraining's l1: 0.0189491\tvalid_1's l1: 0.347818\n",
      "[19000]\ttraining's l1: 0.0178634\tvalid_1's l1: 0.347769\n",
      "[19500]\ttraining's l1: 0.0168586\tvalid_1's l1: 0.347716\n",
      "[20000]\ttraining's l1: 0.0159209\tvalid_1's l1: 0.347667\n",
      "[20500]\ttraining's l1: 0.0150392\tvalid_1's l1: 0.347636\n",
      "[21000]\ttraining's l1: 0.0142197\tvalid_1's l1: 0.347594\n",
      "[21500]\ttraining's l1: 0.0134475\tvalid_1's l1: 0.347556\n",
      "[22000]\ttraining's l1: 0.0127333\tvalid_1's l1: 0.347518\n",
      "[22500]\ttraining's l1: 0.0120548\tvalid_1's l1: 0.347484\n",
      "[23000]\ttraining's l1: 0.0114275\tvalid_1's l1: 0.34746\n",
      "[23500]\ttraining's l1: 0.0108343\tvalid_1's l1: 0.347429\n",
      "[24000]\ttraining's l1: 0.0102806\tvalid_1's l1: 0.347418\n",
      "[24500]\ttraining's l1: 0.00976059\tvalid_1's l1: 0.347388\n",
      "[25000]\ttraining's l1: 0.00927395\tvalid_1's l1: 0.347374\n",
      "[25500]\ttraining's l1: 0.00881606\tvalid_1's l1: 0.347357\n",
      "[26000]\ttraining's l1: 0.00838173\tvalid_1's l1: 0.347341\n",
      "[26500]\ttraining's l1: 0.00798213\tvalid_1's l1: 0.347321\n",
      "[27000]\ttraining's l1: 0.00760312\tvalid_1's l1: 0.347299\n",
      "[27500]\ttraining's l1: 0.00724687\tvalid_1's l1: 0.347283\n",
      "[28000]\ttraining's l1: 0.00691452\tvalid_1's l1: 0.347275\n",
      "[28500]\ttraining's l1: 0.00660296\tvalid_1's l1: 0.347264\n",
      "Early stopping, best iteration is:\n",
      "[28446]\ttraining's l1: 0.00663515\tvalid_1's l1: 0.347262\n",
      "Fold 4 started at Mon Jun 24 15:09:28 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.369747\tvalid_1's l1: 0.433736\n",
      "[1000]\ttraining's l1: 0.300948\tvalid_1's l1: 0.403066\n",
      "[1500]\ttraining's l1: 0.258077\tvalid_1's l1: 0.38893\n",
      "[2000]\ttraining's l1: 0.225585\tvalid_1's l1: 0.379804\n",
      "[2500]\ttraining's l1: 0.200751\tvalid_1's l1: 0.373834\n",
      "[3000]\ttraining's l1: 0.180093\tvalid_1's l1: 0.369615\n",
      "[3500]\ttraining's l1: 0.162789\tvalid_1's l1: 0.366345\n",
      "[4000]\ttraining's l1: 0.147845\tvalid_1's l1: 0.363725\n",
      "[4500]\ttraining's l1: 0.134772\tvalid_1's l1: 0.361759\n",
      "[5000]\ttraining's l1: 0.123267\tvalid_1's l1: 0.360194\n",
      "[5500]\ttraining's l1: 0.113027\tvalid_1's l1: 0.358729\n",
      "[6000]\ttraining's l1: 0.104058\tvalid_1's l1: 0.357571\n",
      "[6500]\ttraining's l1: 0.0959426\tvalid_1's l1: 0.35676\n",
      "[7000]\ttraining's l1: 0.0885785\tvalid_1's l1: 0.3559\n",
      "[7500]\ttraining's l1: 0.0819403\tvalid_1's l1: 0.355229\n",
      "[8000]\ttraining's l1: 0.0759036\tvalid_1's l1: 0.354606\n",
      "[8500]\ttraining's l1: 0.0703978\tvalid_1's l1: 0.354061\n",
      "[9000]\ttraining's l1: 0.0653858\tvalid_1's l1: 0.353611\n",
      "[9500]\ttraining's l1: 0.0608166\tvalid_1's l1: 0.353219\n",
      "[10000]\ttraining's l1: 0.0565692\tvalid_1's l1: 0.352849\n",
      "[10500]\ttraining's l1: 0.052701\tvalid_1's l1: 0.352581\n",
      "[11000]\ttraining's l1: 0.049148\tvalid_1's l1: 0.35236\n",
      "[11500]\ttraining's l1: 0.0459072\tvalid_1's l1: 0.352081\n",
      "[12000]\ttraining's l1: 0.0428663\tvalid_1's l1: 0.351926\n",
      "[12500]\ttraining's l1: 0.0400847\tvalid_1's l1: 0.351756\n",
      "[13000]\ttraining's l1: 0.0374999\tvalid_1's l1: 0.351599\n",
      "[13500]\ttraining's l1: 0.035124\tvalid_1's l1: 0.351466\n",
      "[14000]\ttraining's l1: 0.0329104\tvalid_1's l1: 0.351331\n",
      "[14500]\ttraining's l1: 0.0308647\tvalid_1's l1: 0.351212\n",
      "[15000]\ttraining's l1: 0.0289576\tvalid_1's l1: 0.351124\n",
      "[15500]\ttraining's l1: 0.0271963\tvalid_1's l1: 0.35104\n",
      "[16000]\ttraining's l1: 0.0255572\tvalid_1's l1: 0.350919\n",
      "[16500]\ttraining's l1: 0.0240293\tvalid_1's l1: 0.350841\n",
      "[17000]\ttraining's l1: 0.0226029\tvalid_1's l1: 0.350775\n",
      "[17500]\ttraining's l1: 0.0212588\tvalid_1's l1: 0.350708\n",
      "[18000]\ttraining's l1: 0.0200375\tvalid_1's l1: 0.350656\n",
      "[18500]\ttraining's l1: 0.018893\tvalid_1's l1: 0.350599\n",
      "[19000]\ttraining's l1: 0.0178328\tvalid_1's l1: 0.350539\n",
      "[19500]\ttraining's l1: 0.0168295\tvalid_1's l1: 0.350481\n",
      "[20000]\ttraining's l1: 0.015896\tvalid_1's l1: 0.350442\n",
      "[20500]\ttraining's l1: 0.0150103\tvalid_1's l1: 0.350385\n",
      "[21000]\ttraining's l1: 0.0141785\tvalid_1's l1: 0.350348\n",
      "[21500]\ttraining's l1: 0.0134182\tvalid_1's l1: 0.350307\n",
      "[22000]\ttraining's l1: 0.0126954\tvalid_1's l1: 0.350274\n",
      "[22500]\ttraining's l1: 0.0120183\tvalid_1's l1: 0.350241\n",
      "[23000]\ttraining's l1: 0.0113943\tvalid_1's l1: 0.350218\n",
      "[23500]\ttraining's l1: 0.0107966\tvalid_1's l1: 0.350185\n",
      "[24000]\ttraining's l1: 0.0102409\tvalid_1's l1: 0.350157\n",
      "Early stopping, best iteration is:\n",
      "[23919]\ttraining's l1: 0.0103288\tvalid_1's l1: 0.350154\n",
      "Fold 5 started at Mon Jun 24 15:18:52 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.371405\tvalid_1's l1: 0.433313\n",
      "[1000]\ttraining's l1: 0.301655\tvalid_1's l1: 0.401694\n",
      "[1500]\ttraining's l1: 0.258045\tvalid_1's l1: 0.386605\n",
      "[2000]\ttraining's l1: 0.22567\tvalid_1's l1: 0.376555\n",
      "[2500]\ttraining's l1: 0.200724\tvalid_1's l1: 0.370449\n",
      "[3000]\ttraining's l1: 0.180305\tvalid_1's l1: 0.366176\n",
      "[3500]\ttraining's l1: 0.162857\tvalid_1's l1: 0.362885\n",
      "[4000]\ttraining's l1: 0.147788\tvalid_1's l1: 0.360092\n",
      "[4500]\ttraining's l1: 0.134798\tvalid_1's l1: 0.357942\n",
      "[5000]\ttraining's l1: 0.123368\tvalid_1's l1: 0.356256\n",
      "[5500]\ttraining's l1: 0.113124\tvalid_1's l1: 0.354654\n",
      "[6000]\ttraining's l1: 0.10405\tvalid_1's l1: 0.353492\n",
      "[6500]\ttraining's l1: 0.0959899\tvalid_1's l1: 0.352679\n",
      "[7000]\ttraining's l1: 0.0886557\tvalid_1's l1: 0.351844\n",
      "[7500]\ttraining's l1: 0.0820533\tvalid_1's l1: 0.35114\n",
      "[8000]\ttraining's l1: 0.0760465\tvalid_1's l1: 0.350434\n",
      "[8500]\ttraining's l1: 0.070547\tvalid_1's l1: 0.349859\n",
      "[9000]\ttraining's l1: 0.0655599\tvalid_1's l1: 0.349387\n",
      "[9500]\ttraining's l1: 0.0609241\tvalid_1's l1: 0.348981\n",
      "[10000]\ttraining's l1: 0.0566924\tvalid_1's l1: 0.34867\n",
      "[10500]\ttraining's l1: 0.0528341\tvalid_1's l1: 0.348353\n",
      "[11000]\ttraining's l1: 0.0492684\tvalid_1's l1: 0.348071\n",
      "[11500]\ttraining's l1: 0.0459819\tvalid_1's l1: 0.347822\n",
      "[12000]\ttraining's l1: 0.0429378\tvalid_1's l1: 0.347653\n",
      "[12500]\ttraining's l1: 0.040144\tvalid_1's l1: 0.347419\n",
      "[13000]\ttraining's l1: 0.0375632\tvalid_1's l1: 0.347288\n",
      "[13500]\ttraining's l1: 0.0351736\tvalid_1's l1: 0.34709\n",
      "[14000]\ttraining's l1: 0.032968\tvalid_1's l1: 0.346963\n",
      "[14500]\ttraining's l1: 0.0309373\tvalid_1's l1: 0.346855\n",
      "[15000]\ttraining's l1: 0.0290285\tvalid_1's l1: 0.346733\n",
      "[15500]\ttraining's l1: 0.0272513\tvalid_1's l1: 0.346648\n",
      "[16000]\ttraining's l1: 0.0256013\tvalid_1's l1: 0.346564\n",
      "[16500]\ttraining's l1: 0.0240666\tvalid_1's l1: 0.346474\n",
      "[17000]\ttraining's l1: 0.0226256\tvalid_1's l1: 0.346402\n",
      "[17500]\ttraining's l1: 0.0213056\tvalid_1's l1: 0.346365\n",
      "[18000]\ttraining's l1: 0.020083\tvalid_1's l1: 0.346309\n",
      "[18500]\ttraining's l1: 0.0189148\tvalid_1's l1: 0.346254\n",
      "[19000]\ttraining's l1: 0.0178346\tvalid_1's l1: 0.346179\n",
      "[19500]\ttraining's l1: 0.0168296\tvalid_1's l1: 0.346128\n",
      "[20000]\ttraining's l1: 0.0158844\tvalid_1's l1: 0.346083\n",
      "[20500]\ttraining's l1: 0.0150112\tvalid_1's l1: 0.34604\n",
      "[21000]\ttraining's l1: 0.0141889\tvalid_1's l1: 0.346023\n",
      "[21500]\ttraining's l1: 0.0134165\tvalid_1's l1: 0.34598\n",
      "[22000]\ttraining's l1: 0.0127023\tvalid_1's l1: 0.34594\n",
      "[22500]\ttraining's l1: 0.0120231\tvalid_1's l1: 0.34591\n",
      "[23000]\ttraining's l1: 0.0113943\tvalid_1's l1: 0.345874\n",
      "[23500]\ttraining's l1: 0.0108062\tvalid_1's l1: 0.345837\n",
      "[24000]\ttraining's l1: 0.0102486\tvalid_1's l1: 0.345817\n",
      "[24500]\ttraining's l1: 0.00973914\tvalid_1's l1: 0.345792\n",
      "[25000]\ttraining's l1: 0.00925328\tvalid_1's l1: 0.345766\n",
      "[25500]\ttraining's l1: 0.00879896\tvalid_1's l1: 0.345749\n",
      "[26000]\ttraining's l1: 0.00837064\tvalid_1's l1: 0.345731\n",
      "[26500]\ttraining's l1: 0.00796734\tvalid_1's l1: 0.34571\n",
      "[27000]\ttraining's l1: 0.00759508\tvalid_1's l1: 0.345695\n",
      "[27500]\ttraining's l1: 0.00724253\tvalid_1's l1: 0.345672\n",
      "[28000]\ttraining's l1: 0.00691124\tvalid_1's l1: 0.345658\n",
      "[28500]\ttraining's l1: 0.00660081\tvalid_1's l1: 0.345646\n",
      "Early stopping, best iteration is:\n",
      "[28521]\ttraining's l1: 0.00658844\tvalid_1's l1: 0.345644\n",
      "CV mean score: -1.0584, std: 0.0048.\n",
      "Training of type 1\n",
      "Fold 1 started at Mon Jun 24 15:30:12 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.324791\tvalid_1's l1: 0.63769\n",
      "[1000]\ttraining's l1: 0.185492\tvalid_1's l1: 0.618836\n",
      "[1500]\ttraining's l1: 0.114046\tvalid_1's l1: 0.612416\n",
      "[2000]\ttraining's l1: 0.0728097\tvalid_1's l1: 0.609666\n",
      "[2500]\ttraining's l1: 0.048189\tvalid_1's l1: 0.607945\n",
      "[3000]\ttraining's l1: 0.0324111\tvalid_1's l1: 0.607452\n",
      "[3500]\ttraining's l1: 0.0224274\tvalid_1's l1: 0.606886\n",
      "[4000]\ttraining's l1: 0.0157979\tvalid_1's l1: 0.606739\n",
      "Early stopping, best iteration is:\n",
      "[3928]\ttraining's l1: 0.0166037\tvalid_1's l1: 0.606705\n",
      "Fold 2 started at Mon Jun 24 15:31:06 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.323058\tvalid_1's l1: 0.628536\n",
      "[1000]\ttraining's l1: 0.183129\tvalid_1's l1: 0.60975\n",
      "[1500]\ttraining's l1: 0.112147\tvalid_1's l1: 0.602322\n",
      "[2000]\ttraining's l1: 0.0720963\tvalid_1's l1: 0.599055\n",
      "[2500]\ttraining's l1: 0.0477122\tvalid_1's l1: 0.597983\n",
      "[3000]\ttraining's l1: 0.0319955\tvalid_1's l1: 0.597196\n",
      "[3500]\ttraining's l1: 0.0221121\tvalid_1's l1: 0.596784\n",
      "Early stopping, best iteration is:\n",
      "[3748]\ttraining's l1: 0.018524\tvalid_1's l1: 0.596632\n",
      "Fold 3 started at Mon Jun 24 15:32:07 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.328708\tvalid_1's l1: 0.639558\n",
      "[1000]\ttraining's l1: 0.18529\tvalid_1's l1: 0.617012\n",
      "[1500]\ttraining's l1: 0.113196\tvalid_1's l1: 0.611791\n",
      "[2000]\ttraining's l1: 0.0720869\tvalid_1's l1: 0.609051\n",
      "[2500]\ttraining's l1: 0.0474224\tvalid_1's l1: 0.607598\n",
      "Early stopping, best iteration is:\n",
      "[2460]\ttraining's l1: 0.0489204\tvalid_1's l1: 0.607506\n",
      "Fold 4 started at Mon Jun 24 15:32:46 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.325154\tvalid_1's l1: 0.641276\n",
      "[1000]\ttraining's l1: 0.184732\tvalid_1's l1: 0.620212\n",
      "[1500]\ttraining's l1: 0.113415\tvalid_1's l1: 0.612295\n",
      "[2000]\ttraining's l1: 0.0725852\tvalid_1's l1: 0.609103\n",
      "[2500]\ttraining's l1: 0.0479432\tvalid_1's l1: 0.607743\n",
      "[3000]\ttraining's l1: 0.0321717\tvalid_1's l1: 0.606959\n",
      "[3500]\ttraining's l1: 0.0221313\tvalid_1's l1: 0.606717\n",
      "[4000]\ttraining's l1: 0.0155914\tvalid_1's l1: 0.606553\n",
      "Early stopping, best iteration is:\n",
      "[3808]\ttraining's l1: 0.01779\tvalid_1's l1: 0.60651\n",
      "Fold 5 started at Mon Jun 24 15:33:44 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.324363\tvalid_1's l1: 0.632208\n",
      "[1000]\ttraining's l1: 0.186675\tvalid_1's l1: 0.610838\n",
      "[1500]\ttraining's l1: 0.114895\tvalid_1's l1: 0.603372\n",
      "[2000]\ttraining's l1: 0.0733104\tvalid_1's l1: 0.59993\n",
      "[2500]\ttraining's l1: 0.0481275\tvalid_1's l1: 0.598392\n",
      "[3000]\ttraining's l1: 0.032488\tvalid_1's l1: 0.597345\n",
      "[3500]\ttraining's l1: 0.0223761\tvalid_1's l1: 0.596878\n",
      "[4000]\ttraining's l1: 0.0157706\tvalid_1's l1: 0.596687\n",
      "[4500]\ttraining's l1: 0.011317\tvalid_1's l1: 0.596478\n",
      "[5000]\ttraining's l1: 0.00837006\tvalid_1's l1: 0.596386\n",
      "[5500]\ttraining's l1: 0.00636252\tvalid_1's l1: 0.596279\n",
      "[6000]\ttraining's l1: 0.0049682\tvalid_1's l1: 0.596214\n",
      "[6500]\ttraining's l1: 0.00401147\tvalid_1's l1: 0.596168\n",
      "[7000]\ttraining's l1: 0.00332306\tvalid_1's l1: 0.596159\n",
      "Early stopping, best iteration is:\n",
      "[6862]\ttraining's l1: 0.00349097\tvalid_1's l1: 0.59615\n",
      "CV mean score: -0.5064, std: 0.0086.\n",
      "Training of type 4\n",
      "Fold 1 started at Mon Jun 24 15:35:30 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.289226\tvalid_1's l1: 0.424453\n",
      "[1000]\ttraining's l1: 0.200466\tvalid_1's l1: 0.401712\n",
      "[1500]\ttraining's l1: 0.149238\tvalid_1's l1: 0.391384\n",
      "[2000]\ttraining's l1: 0.114718\tvalid_1's l1: 0.386748\n",
      "[2500]\ttraining's l1: 0.0900363\tvalid_1's l1: 0.383683\n",
      "[3000]\ttraining's l1: 0.0717077\tvalid_1's l1: 0.381815\n",
      "[3500]\ttraining's l1: 0.0577936\tvalid_1's l1: 0.380604\n",
      "[4000]\ttraining's l1: 0.0469122\tvalid_1's l1: 0.379788\n",
      "[4500]\ttraining's l1: 0.0385527\tvalid_1's l1: 0.379152\n",
      "[5000]\ttraining's l1: 0.0318631\tvalid_1's l1: 0.378723\n",
      "[5500]\ttraining's l1: 0.0264643\tvalid_1's l1: 0.378436\n",
      "Early stopping, best iteration is:\n",
      "[5575]\ttraining's l1: 0.0257598\tvalid_1's l1: 0.378384\n",
      "Fold 2 started at Mon Jun 24 15:36:51 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.288757\tvalid_1's l1: 0.432308\n",
      "[1000]\ttraining's l1: 0.199514\tvalid_1's l1: 0.40538\n",
      "[1500]\ttraining's l1: 0.14814\tvalid_1's l1: 0.395323\n",
      "[2000]\ttraining's l1: 0.113646\tvalid_1's l1: 0.389942\n",
      "[2500]\ttraining's l1: 0.0891348\tvalid_1's l1: 0.386837\n",
      "[3000]\ttraining's l1: 0.0711701\tvalid_1's l1: 0.38495\n",
      "[3500]\ttraining's l1: 0.0574905\tvalid_1's l1: 0.383651\n",
      "[4000]\ttraining's l1: 0.0467608\tvalid_1's l1: 0.382988\n",
      "[4500]\ttraining's l1: 0.0383332\tvalid_1's l1: 0.382191\n",
      "[5000]\ttraining's l1: 0.0316595\tvalid_1's l1: 0.38168\n",
      "[5500]\ttraining's l1: 0.0263137\tvalid_1's l1: 0.381338\n",
      "[6000]\ttraining's l1: 0.0220517\tvalid_1's l1: 0.381097\n",
      "[6500]\ttraining's l1: 0.0185874\tvalid_1's l1: 0.380866\n",
      "[7000]\ttraining's l1: 0.0157434\tvalid_1's l1: 0.380641\n",
      "[7500]\ttraining's l1: 0.0133979\tvalid_1's l1: 0.380529\n",
      "[8000]\ttraining's l1: 0.0114874\tvalid_1's l1: 0.380484\n",
      "[8500]\ttraining's l1: 0.00990634\tvalid_1's l1: 0.38041\n",
      "[9000]\ttraining's l1: 0.00859586\tvalid_1's l1: 0.380329\n",
      "[9500]\ttraining's l1: 0.00749916\tvalid_1's l1: 0.380276\n",
      "[10000]\ttraining's l1: 0.00659175\tvalid_1's l1: 0.380229\n",
      "[10500]\ttraining's l1: 0.00581772\tvalid_1's l1: 0.380204\n",
      "[11000]\ttraining's l1: 0.00517278\tvalid_1's l1: 0.380184\n",
      "[11500]\ttraining's l1: 0.00462313\tvalid_1's l1: 0.38016\n",
      "Early stopping, best iteration is:\n",
      "[11541]\ttraining's l1: 0.00458268\tvalid_1's l1: 0.380158\n",
      "Fold 3 started at Mon Jun 24 15:39:34 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.290504\tvalid_1's l1: 0.425397\n",
      "[1000]\ttraining's l1: 0.20149\tvalid_1's l1: 0.399851\n",
      "[1500]\ttraining's l1: 0.149724\tvalid_1's l1: 0.390394\n",
      "[2000]\ttraining's l1: 0.114733\tvalid_1's l1: 0.385486\n",
      "[2500]\ttraining's l1: 0.0900802\tvalid_1's l1: 0.382679\n",
      "[3000]\ttraining's l1: 0.0718406\tvalid_1's l1: 0.380804\n",
      "[3500]\ttraining's l1: 0.0578351\tvalid_1's l1: 0.379537\n",
      "[4000]\ttraining's l1: 0.0471636\tvalid_1's l1: 0.378808\n",
      "[4500]\ttraining's l1: 0.0387742\tvalid_1's l1: 0.378226\n",
      "[5000]\ttraining's l1: 0.0320511\tvalid_1's l1: 0.377764\n",
      "[5500]\ttraining's l1: 0.0266707\tvalid_1's l1: 0.377431\n",
      "[6000]\ttraining's l1: 0.0223754\tvalid_1's l1: 0.377099\n",
      "[6500]\ttraining's l1: 0.0188592\tvalid_1's l1: 0.376859\n",
      "[7000]\ttraining's l1: 0.0159742\tvalid_1's l1: 0.376705\n",
      "[7500]\ttraining's l1: 0.0136189\tvalid_1's l1: 0.376601\n",
      "[8000]\ttraining's l1: 0.0116713\tvalid_1's l1: 0.376491\n",
      "[8500]\ttraining's l1: 0.0100596\tvalid_1's l1: 0.376421\n",
      "[9000]\ttraining's l1: 0.00871735\tvalid_1's l1: 0.37634\n",
      "[9500]\ttraining's l1: 0.00760698\tvalid_1's l1: 0.376288\n",
      "[10000]\ttraining's l1: 0.0066777\tvalid_1's l1: 0.376259\n",
      "[10500]\ttraining's l1: 0.00589569\tvalid_1's l1: 0.376214\n",
      "[11000]\ttraining's l1: 0.00524356\tvalid_1's l1: 0.376186\n",
      "[11500]\ttraining's l1: 0.00468793\tvalid_1's l1: 0.37616\n",
      "[12000]\ttraining's l1: 0.00422389\tvalid_1's l1: 0.376138\n",
      "[12500]\ttraining's l1: 0.00382581\tvalid_1's l1: 0.376112\n",
      "[13000]\ttraining's l1: 0.00348647\tvalid_1's l1: 0.3761\n",
      "[13500]\ttraining's l1: 0.0031971\tvalid_1's l1: 0.376086\n",
      "Early stopping, best iteration is:\n",
      "[13730]\ttraining's l1: 0.00307593\tvalid_1's l1: 0.376082\n",
      "Fold 4 started at Mon Jun 24 15:42:43 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.28805\tvalid_1's l1: 0.427463\n",
      "[1000]\ttraining's l1: 0.199509\tvalid_1's l1: 0.402168\n",
      "[1500]\ttraining's l1: 0.148387\tvalid_1's l1: 0.393165\n",
      "[2000]\ttraining's l1: 0.114017\tvalid_1's l1: 0.388526\n",
      "[2500]\ttraining's l1: 0.0892599\tvalid_1's l1: 0.385145\n",
      "[3000]\ttraining's l1: 0.0710845\tvalid_1's l1: 0.383397\n",
      "[3500]\ttraining's l1: 0.0572303\tvalid_1's l1: 0.382269\n",
      "[4000]\ttraining's l1: 0.0465752\tvalid_1's l1: 0.381623\n",
      "[4500]\ttraining's l1: 0.0382268\tvalid_1's l1: 0.380915\n",
      "[5000]\ttraining's l1: 0.0315663\tvalid_1's l1: 0.380418\n",
      "[5500]\ttraining's l1: 0.0262688\tvalid_1's l1: 0.380045\n",
      "[6000]\ttraining's l1: 0.0219959\tvalid_1's l1: 0.379803\n",
      "[6500]\ttraining's l1: 0.0185268\tvalid_1's l1: 0.379625\n",
      "[7000]\ttraining's l1: 0.0157175\tvalid_1's l1: 0.379409\n",
      "[7500]\ttraining's l1: 0.0134063\tvalid_1's l1: 0.379291\n",
      "[8000]\ttraining's l1: 0.0114744\tvalid_1's l1: 0.379177\n",
      "[8500]\ttraining's l1: 0.00989321\tvalid_1's l1: 0.379097\n",
      "[9000]\ttraining's l1: 0.00859191\tvalid_1's l1: 0.379029\n",
      "[9500]\ttraining's l1: 0.00748984\tvalid_1's l1: 0.378993\n",
      "[10000]\ttraining's l1: 0.00657065\tvalid_1's l1: 0.378933\n",
      "[10500]\ttraining's l1: 0.00580905\tvalid_1's l1: 0.378893\n",
      "[11000]\ttraining's l1: 0.00516709\tvalid_1's l1: 0.378851\n",
      "[11500]\ttraining's l1: 0.00462291\tvalid_1's l1: 0.378831\n",
      "[12000]\ttraining's l1: 0.00416738\tvalid_1's l1: 0.378818\n",
      "[12500]\ttraining's l1: 0.00377734\tvalid_1's l1: 0.378802\n",
      "[13000]\ttraining's l1: 0.00344235\tvalid_1's l1: 0.378792\n",
      "Early stopping, best iteration is:\n",
      "[12900]\ttraining's l1: 0.00350558\tvalid_1's l1: 0.378789\n",
      "Fold 5 started at Mon Jun 24 15:45:47 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.287861\tvalid_1's l1: 0.427683\n",
      "[1000]\ttraining's l1: 0.199038\tvalid_1's l1: 0.40123\n",
      "[1500]\ttraining's l1: 0.147983\tvalid_1's l1: 0.391842\n",
      "[2000]\ttraining's l1: 0.113741\tvalid_1's l1: 0.386976\n",
      "[2500]\ttraining's l1: 0.0892657\tvalid_1's l1: 0.384183\n",
      "[3000]\ttraining's l1: 0.0711563\tvalid_1's l1: 0.382319\n",
      "[3500]\ttraining's l1: 0.0574515\tvalid_1's l1: 0.381155\n",
      "[4000]\ttraining's l1: 0.0467252\tvalid_1's l1: 0.380446\n",
      "[4500]\ttraining's l1: 0.0383783\tvalid_1's l1: 0.379793\n",
      "[5000]\ttraining's l1: 0.0317554\tvalid_1's l1: 0.379219\n",
      "[5500]\ttraining's l1: 0.0265168\tvalid_1's l1: 0.378829\n",
      "[6000]\ttraining's l1: 0.0222926\tvalid_1's l1: 0.3785\n",
      "[6500]\ttraining's l1: 0.0188657\tvalid_1's l1: 0.378299\n",
      "[7000]\ttraining's l1: 0.0160587\tvalid_1's l1: 0.378155\n",
      "[7500]\ttraining's l1: 0.0137523\tvalid_1's l1: 0.378039\n",
      "[8000]\ttraining's l1: 0.0118263\tvalid_1's l1: 0.377915\n",
      "[8500]\ttraining's l1: 0.0101937\tvalid_1's l1: 0.377814\n",
      "[9000]\ttraining's l1: 0.00882752\tvalid_1's l1: 0.377772\n",
      "[9500]\ttraining's l1: 0.00768523\tvalid_1's l1: 0.377705\n",
      "[10000]\ttraining's l1: 0.00674065\tvalid_1's l1: 0.377669\n",
      "[10500]\ttraining's l1: 0.0059565\tvalid_1's l1: 0.377636\n",
      "[11000]\ttraining's l1: 0.0052923\tvalid_1's l1: 0.377607\n",
      "[11500]\ttraining's l1: 0.00474354\tvalid_1's l1: 0.377582\n",
      "[12000]\ttraining's l1: 0.00427521\tvalid_1's l1: 0.377564\n",
      "Early stopping, best iteration is:\n",
      "[12100]\ttraining's l1: 0.00419073\tvalid_1's l1: 0.37756\n",
      "CV mean score: -0.9724, std: 0.0036.\n",
      "Training of type 2\n",
      "Fold 1 started at Mon Jun 24 15:48:43 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.846728\tvalid_1's l1: 0.89467\n",
      "[1000]\ttraining's l1: 0.737342\tvalid_1's l1: 0.820064\n",
      "[1500]\ttraining's l1: 0.668331\tvalid_1's l1: 0.779266\n",
      "[2000]\ttraining's l1: 0.618296\tvalid_1's l1: 0.754608\n",
      "[2500]\ttraining's l1: 0.577913\tvalid_1's l1: 0.736595\n",
      "[3000]\ttraining's l1: 0.543972\tvalid_1's l1: 0.723126\n",
      "[3500]\ttraining's l1: 0.514395\tvalid_1's l1: 0.71264\n",
      "[4000]\ttraining's l1: 0.488042\tvalid_1's l1: 0.703812\n",
      "[4500]\ttraining's l1: 0.464095\tvalid_1's l1: 0.695995\n",
      "[5000]\ttraining's l1: 0.442716\tvalid_1's l1: 0.690082\n",
      "[5500]\ttraining's l1: 0.422653\tvalid_1's l1: 0.684447\n",
      "[6000]\ttraining's l1: 0.404135\tvalid_1's l1: 0.679741\n",
      "[6500]\ttraining's l1: 0.387215\tvalid_1's l1: 0.675426\n",
      "[7000]\ttraining's l1: 0.371508\tvalid_1's l1: 0.671702\n",
      "[7500]\ttraining's l1: 0.356445\tvalid_1's l1: 0.668287\n",
      "[8000]\ttraining's l1: 0.34289\tvalid_1's l1: 0.665412\n",
      "[8500]\ttraining's l1: 0.330031\tvalid_1's l1: 0.662765\n",
      "[9000]\ttraining's l1: 0.317629\tvalid_1's l1: 0.660368\n",
      "[9500]\ttraining's l1: 0.305993\tvalid_1's l1: 0.6583\n",
      "[10000]\ttraining's l1: 0.295122\tvalid_1's l1: 0.656421\n",
      "[10500]\ttraining's l1: 0.284676\tvalid_1's l1: 0.654545\n",
      "[11000]\ttraining's l1: 0.274723\tvalid_1's l1: 0.652739\n",
      "[11500]\ttraining's l1: 0.265276\tvalid_1's l1: 0.651086\n",
      "[12000]\ttraining's l1: 0.256284\tvalid_1's l1: 0.649631\n",
      "[12500]\ttraining's l1: 0.247596\tvalid_1's l1: 0.6483\n",
      "[13000]\ttraining's l1: 0.239355\tvalid_1's l1: 0.647023\n",
      "[13500]\ttraining's l1: 0.23162\tvalid_1's l1: 0.645825\n",
      "[14000]\ttraining's l1: 0.224024\tvalid_1's l1: 0.644767\n",
      "[14500]\ttraining's l1: 0.21682\tvalid_1's l1: 0.6439\n",
      "[15000]\ttraining's l1: 0.209981\tvalid_1's l1: 0.642974\n",
      "[15500]\ttraining's l1: 0.203394\tvalid_1's l1: 0.642093\n",
      "[16000]\ttraining's l1: 0.197007\tvalid_1's l1: 0.641157\n",
      "[16500]\ttraining's l1: 0.190938\tvalid_1's l1: 0.640354\n",
      "[17000]\ttraining's l1: 0.185011\tvalid_1's l1: 0.639583\n",
      "[17500]\ttraining's l1: 0.179375\tvalid_1's l1: 0.638833\n",
      "[18000]\ttraining's l1: 0.174032\tvalid_1's l1: 0.638138\n",
      "[18500]\ttraining's l1: 0.168814\tvalid_1's l1: 0.637534\n",
      "[19000]\ttraining's l1: 0.163812\tvalid_1's l1: 0.636935\n",
      "[19500]\ttraining's l1: 0.158948\tvalid_1's l1: 0.636454\n",
      "[20000]\ttraining's l1: 0.154336\tvalid_1's l1: 0.635904\n",
      "[20500]\ttraining's l1: 0.149839\tvalid_1's l1: 0.635486\n",
      "[21000]\ttraining's l1: 0.145542\tvalid_1's l1: 0.634971\n",
      "[21500]\ttraining's l1: 0.141355\tvalid_1's l1: 0.63451\n",
      "[22000]\ttraining's l1: 0.13731\tvalid_1's l1: 0.634032\n",
      "[22500]\ttraining's l1: 0.133438\tvalid_1's l1: 0.633645\n",
      "[23000]\ttraining's l1: 0.129658\tvalid_1's l1: 0.633228\n",
      "[23500]\ttraining's l1: 0.12603\tvalid_1's l1: 0.632885\n",
      "[24000]\ttraining's l1: 0.122507\tvalid_1's l1: 0.632527\n",
      "[24500]\ttraining's l1: 0.119125\tvalid_1's l1: 0.632241\n",
      "[25000]\ttraining's l1: 0.115863\tvalid_1's l1: 0.631917\n",
      "[25500]\ttraining's l1: 0.112705\tvalid_1's l1: 0.631582\n",
      "[26000]\ttraining's l1: 0.1096\tvalid_1's l1: 0.631288\n",
      "[26500]\ttraining's l1: 0.106637\tvalid_1's l1: 0.631007\n",
      "[27000]\ttraining's l1: 0.103733\tvalid_1's l1: 0.630763\n",
      "[27500]\ttraining's l1: 0.100923\tvalid_1's l1: 0.630542\n",
      "[28000]\ttraining's l1: 0.0981902\tvalid_1's l1: 0.630278\n",
      "[28500]\ttraining's l1: 0.0955922\tvalid_1's l1: 0.630063\n",
      "[29000]\ttraining's l1: 0.0930512\tvalid_1's l1: 0.629779\n",
      "[29500]\ttraining's l1: 0.090588\tvalid_1's l1: 0.62955\n",
      "[30000]\ttraining's l1: 0.0881971\tvalid_1's l1: 0.629354\n",
      "[30500]\ttraining's l1: 0.0858915\tvalid_1's l1: 0.62916\n",
      "[31000]\ttraining's l1: 0.083642\tvalid_1's l1: 0.628965\n",
      "[31500]\ttraining's l1: 0.0814489\tvalid_1's l1: 0.628818\n",
      "[32000]\ttraining's l1: 0.0793171\tvalid_1's l1: 0.62863\n",
      "[32500]\ttraining's l1: 0.0772619\tvalid_1's l1: 0.628451\n",
      "[33000]\ttraining's l1: 0.0752637\tvalid_1's l1: 0.628268\n",
      "[33500]\ttraining's l1: 0.0733246\tvalid_1's l1: 0.628097\n",
      "[34000]\ttraining's l1: 0.0714482\tvalid_1's l1: 0.627939\n",
      "[34500]\ttraining's l1: 0.0695947\tvalid_1's l1: 0.62777\n",
      "[35000]\ttraining's l1: 0.067813\tvalid_1's l1: 0.62763\n",
      "[35500]\ttraining's l1: 0.0660893\tvalid_1's l1: 0.627491\n",
      "[36000]\ttraining's l1: 0.0644244\tvalid_1's l1: 0.62732\n",
      "[36500]\ttraining's l1: 0.0628145\tvalid_1's l1: 0.627226\n",
      "[37000]\ttraining's l1: 0.0612338\tvalid_1's l1: 0.627124\n",
      "[37500]\ttraining's l1: 0.0597001\tvalid_1's l1: 0.627018\n",
      "[38000]\ttraining's l1: 0.0582199\tvalid_1's l1: 0.626891\n",
      "[38500]\ttraining's l1: 0.0567787\tvalid_1's l1: 0.626797\n",
      "[39000]\ttraining's l1: 0.0553702\tvalid_1's l1: 0.626672\n",
      "[39500]\ttraining's l1: 0.05401\tvalid_1's l1: 0.626567\n",
      "[40000]\ttraining's l1: 0.0526805\tvalid_1's l1: 0.626456\n",
      "[40500]\ttraining's l1: 0.0513904\tvalid_1's l1: 0.626353\n",
      "[41000]\ttraining's l1: 0.0501239\tvalid_1's l1: 0.626256\n",
      "[41500]\ttraining's l1: 0.048901\tvalid_1's l1: 0.626169\n",
      "[42000]\ttraining's l1: 0.0477162\tvalid_1's l1: 0.626091\n",
      "[42500]\ttraining's l1: 0.0465672\tvalid_1's l1: 0.626013\n",
      "[43000]\ttraining's l1: 0.0454304\tvalid_1's l1: 0.625934\n",
      "[43500]\ttraining's l1: 0.0443404\tvalid_1's l1: 0.625863\n",
      "[44000]\ttraining's l1: 0.0432777\tvalid_1's l1: 0.625793\n",
      "[44500]\ttraining's l1: 0.0422268\tvalid_1's l1: 0.625719\n",
      "[45000]\ttraining's l1: 0.0412188\tvalid_1's l1: 0.62564\n",
      "[45500]\ttraining's l1: 0.0402366\tvalid_1's l1: 0.625597\n",
      "[46000]\ttraining's l1: 0.0392668\tvalid_1's l1: 0.62554\n",
      "[46500]\ttraining's l1: 0.0383402\tvalid_1's l1: 0.625486\n",
      "[47000]\ttraining's l1: 0.0374407\tvalid_1's l1: 0.625435\n",
      "[47500]\ttraining's l1: 0.0365628\tvalid_1's l1: 0.625381\n",
      "[48000]\ttraining's l1: 0.0357009\tvalid_1's l1: 0.625331\n",
      "[48500]\ttraining's l1: 0.0348614\tvalid_1's l1: 0.625268\n",
      "[49000]\ttraining's l1: 0.0340535\tvalid_1's l1: 0.625234\n",
      "[49500]\ttraining's l1: 0.0332604\tvalid_1's l1: 0.625175\n",
      "[50000]\ttraining's l1: 0.0324834\tvalid_1's l1: 0.625127\n",
      "[50500]\ttraining's l1: 0.0317308\tvalid_1's l1: 0.625079\n",
      "[51000]\ttraining's l1: 0.0309992\tvalid_1's l1: 0.625021\n",
      "[51500]\ttraining's l1: 0.0302827\tvalid_1's l1: 0.624978\n",
      "[52000]\ttraining's l1: 0.0295959\tvalid_1's l1: 0.624933\n",
      "[52500]\ttraining's l1: 0.0289148\tvalid_1's l1: 0.624894\n",
      "[53000]\ttraining's l1: 0.0282535\tvalid_1's l1: 0.624853\n",
      "[53500]\ttraining's l1: 0.0276034\tvalid_1's l1: 0.624818\n",
      "[54000]\ttraining's l1: 0.0269804\tvalid_1's l1: 0.624788\n",
      "[54500]\ttraining's l1: 0.0263612\tvalid_1's l1: 0.624752\n",
      "[55000]\ttraining's l1: 0.0257633\tvalid_1's l1: 0.624708\n",
      "[55500]\ttraining's l1: 0.0251786\tvalid_1's l1: 0.624672\n",
      "[56000]\ttraining's l1: 0.0246146\tvalid_1's l1: 0.624634\n",
      "[56500]\ttraining's l1: 0.0240603\tvalid_1's l1: 0.624595\n",
      "[57000]\ttraining's l1: 0.0235254\tvalid_1's l1: 0.624563\n",
      "[57500]\ttraining's l1: 0.0230108\tvalid_1's l1: 0.624526\n",
      "[58000]\ttraining's l1: 0.0224933\tvalid_1's l1: 0.624494\n",
      "[58500]\ttraining's l1: 0.0219939\tvalid_1's l1: 0.624468\n",
      "[59000]\ttraining's l1: 0.0215075\tvalid_1's l1: 0.624441\n",
      "[59500]\ttraining's l1: 0.0210286\tvalid_1's l1: 0.624412\n",
      "[60000]\ttraining's l1: 0.0205694\tvalid_1's l1: 0.624381\n",
      "[60500]\ttraining's l1: 0.0201248\tvalid_1's l1: 0.62435\n",
      "[61000]\ttraining's l1: 0.0196849\tvalid_1's l1: 0.624319\n",
      "[61500]\ttraining's l1: 0.0192536\tvalid_1's l1: 0.624306\n",
      "[62000]\ttraining's l1: 0.0188307\tvalid_1's l1: 0.624286\n",
      "[62500]\ttraining's l1: 0.0184221\tvalid_1's l1: 0.624257\n",
      "[63000]\ttraining's l1: 0.0180288\tvalid_1's l1: 0.624239\n",
      "[63500]\ttraining's l1: 0.0176444\tvalid_1's l1: 0.624218\n",
      "[64000]\ttraining's l1: 0.0172685\tvalid_1's l1: 0.624196\n",
      "[64500]\ttraining's l1: 0.0169\tvalid_1's l1: 0.624176\n",
      "[65000]\ttraining's l1: 0.0165406\tvalid_1's l1: 0.624158\n",
      "[65500]\ttraining's l1: 0.0161883\tvalid_1's l1: 0.624138\n",
      "[66000]\ttraining's l1: 0.0158477\tvalid_1's l1: 0.624119\n",
      "[66500]\ttraining's l1: 0.0155095\tvalid_1's l1: 0.6241\n",
      "[67000]\ttraining's l1: 0.0151802\tvalid_1's l1: 0.624079\n",
      "[67500]\ttraining's l1: 0.0148556\tvalid_1's l1: 0.624057\n",
      "[68000]\ttraining's l1: 0.0145461\tvalid_1's l1: 0.62404\n",
      "[68500]\ttraining's l1: 0.0142403\tvalid_1's l1: 0.624026\n",
      "[69000]\ttraining's l1: 0.0139448\tvalid_1's l1: 0.624014\n",
      "[69500]\ttraining's l1: 0.0136547\tvalid_1's l1: 0.624004\n",
      "[70000]\ttraining's l1: 0.0133744\tvalid_1's l1: 0.623994\n",
      "[70500]\ttraining's l1: 0.0131003\tvalid_1's l1: 0.623978\n",
      "[71000]\ttraining's l1: 0.0128342\tvalid_1's l1: 0.623965\n",
      "[71500]\ttraining's l1: 0.0125713\tvalid_1's l1: 0.623954\n",
      "[72000]\ttraining's l1: 0.0123125\tvalid_1's l1: 0.623942\n",
      "[72500]\ttraining's l1: 0.01206\tvalid_1's l1: 0.623928\n",
      "[73000]\ttraining's l1: 0.0118136\tvalid_1's l1: 0.623917\n",
      "[73500]\ttraining's l1: 0.0115747\tvalid_1's l1: 0.623906\n",
      "[74000]\ttraining's l1: 0.0113381\tvalid_1's l1: 0.623894\n",
      "[74500]\ttraining's l1: 0.0111076\tvalid_1's l1: 0.623885\n",
      "[75000]\ttraining's l1: 0.0108833\tvalid_1's l1: 0.623879\n",
      "[75500]\ttraining's l1: 0.0106647\tvalid_1's l1: 0.623867\n",
      "[76000]\ttraining's l1: 0.0104537\tvalid_1's l1: 0.623857\n",
      "[76500]\ttraining's l1: 0.010248\tvalid_1's l1: 0.623847\n",
      "[77000]\ttraining's l1: 0.0100457\tvalid_1's l1: 0.623839\n",
      "[77500]\ttraining's l1: 0.00984521\tvalid_1's l1: 0.62383\n",
      "[78000]\ttraining's l1: 0.00965245\tvalid_1's l1: 0.623822\n",
      "[78500]\ttraining's l1: 0.00946429\tvalid_1's l1: 0.623813\n",
      "[79000]\ttraining's l1: 0.00927884\tvalid_1's l1: 0.623806\n",
      "[79500]\ttraining's l1: 0.009101\tvalid_1's l1: 0.623798\n",
      "[80000]\ttraining's l1: 0.0089236\tvalid_1's l1: 0.623791\n",
      "[80500]\ttraining's l1: 0.00875282\tvalid_1's l1: 0.623784\n",
      "[81000]\ttraining's l1: 0.00858625\tvalid_1's l1: 0.623776\n",
      "[81500]\ttraining's l1: 0.00842254\tvalid_1's l1: 0.623767\n",
      "[82000]\ttraining's l1: 0.0082644\tvalid_1's l1: 0.623761\n",
      "[82500]\ttraining's l1: 0.0081063\tvalid_1's l1: 0.623753\n",
      "[83000]\ttraining's l1: 0.00795436\tvalid_1's l1: 0.623744\n",
      "[83500]\ttraining's l1: 0.00780569\tvalid_1's l1: 0.623737\n",
      "[84000]\ttraining's l1: 0.00766169\tvalid_1's l1: 0.623734\n",
      "[84500]\ttraining's l1: 0.0075201\tvalid_1's l1: 0.623727\n",
      "[85000]\ttraining's l1: 0.00738104\tvalid_1's l1: 0.623724\n",
      "[85500]\ttraining's l1: 0.00724409\tvalid_1's l1: 0.623717\n",
      "[86000]\ttraining's l1: 0.00711221\tvalid_1's l1: 0.62371\n",
      "[86500]\ttraining's l1: 0.00698207\tvalid_1's l1: 0.623704\n",
      "[87000]\ttraining's l1: 0.00685567\tvalid_1's l1: 0.623698\n",
      "Early stopping, best iteration is:\n",
      "[87240]\ttraining's l1: 0.00679596\tvalid_1's l1: 0.623696\n",
      "Fold 2 started at Mon Jun 24 16:42:17 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.846876\tvalid_1's l1: 0.896811\n",
      "[1000]\ttraining's l1: 0.737763\tvalid_1's l1: 0.822235\n",
      "[1500]\ttraining's l1: 0.669015\tvalid_1's l1: 0.781911\n",
      "[2000]\ttraining's l1: 0.618545\tvalid_1's l1: 0.75704\n",
      "[2500]\ttraining's l1: 0.577133\tvalid_1's l1: 0.738188\n",
      "[3000]\ttraining's l1: 0.543022\tvalid_1's l1: 0.725068\n",
      "[3500]\ttraining's l1: 0.513731\tvalid_1's l1: 0.714606\n",
      "[4000]\ttraining's l1: 0.486758\tvalid_1's l1: 0.705252\n",
      "[4500]\ttraining's l1: 0.462883\tvalid_1's l1: 0.697903\n",
      "[5000]\ttraining's l1: 0.441651\tvalid_1's l1: 0.691724\n",
      "[5500]\ttraining's l1: 0.421797\tvalid_1's l1: 0.68624\n",
      "[6000]\ttraining's l1: 0.403581\tvalid_1's l1: 0.681545\n",
      "[6500]\ttraining's l1: 0.386902\tvalid_1's l1: 0.677636\n",
      "[7000]\ttraining's l1: 0.370967\tvalid_1's l1: 0.673966\n",
      "[7500]\ttraining's l1: 0.356081\tvalid_1's l1: 0.670891\n",
      "[8000]\ttraining's l1: 0.342353\tvalid_1's l1: 0.668519\n",
      "[8500]\ttraining's l1: 0.329334\tvalid_1's l1: 0.665967\n",
      "[9000]\ttraining's l1: 0.317098\tvalid_1's l1: 0.663354\n",
      "[9500]\ttraining's l1: 0.305458\tvalid_1's l1: 0.661354\n",
      "[10000]\ttraining's l1: 0.294437\tvalid_1's l1: 0.659386\n",
      "[10500]\ttraining's l1: 0.284006\tvalid_1's l1: 0.657556\n",
      "[11000]\ttraining's l1: 0.274052\tvalid_1's l1: 0.655826\n",
      "[11500]\ttraining's l1: 0.264553\tvalid_1's l1: 0.654148\n",
      "[12000]\ttraining's l1: 0.255618\tvalid_1's l1: 0.652556\n",
      "[12500]\ttraining's l1: 0.247088\tvalid_1's l1: 0.651342\n",
      "[13000]\ttraining's l1: 0.238944\tvalid_1's l1: 0.650288\n",
      "[13500]\ttraining's l1: 0.231201\tvalid_1's l1: 0.649192\n",
      "[14000]\ttraining's l1: 0.223626\tvalid_1's l1: 0.648088\n",
      "[14500]\ttraining's l1: 0.216411\tvalid_1's l1: 0.647098\n",
      "[15000]\ttraining's l1: 0.209571\tvalid_1's l1: 0.646022\n",
      "[15500]\ttraining's l1: 0.203029\tvalid_1's l1: 0.645164\n",
      "[16000]\ttraining's l1: 0.19678\tvalid_1's l1: 0.644346\n",
      "[16500]\ttraining's l1: 0.190707\tvalid_1's l1: 0.643683\n",
      "[17000]\ttraining's l1: 0.184886\tvalid_1's l1: 0.642895\n",
      "[17500]\ttraining's l1: 0.179233\tvalid_1's l1: 0.642274\n",
      "[18000]\ttraining's l1: 0.173772\tvalid_1's l1: 0.641519\n",
      "[18500]\ttraining's l1: 0.168576\tvalid_1's l1: 0.640799\n",
      "[19000]\ttraining's l1: 0.163622\tvalid_1's l1: 0.640298\n",
      "[19500]\ttraining's l1: 0.158858\tvalid_1's l1: 0.639844\n",
      "[20000]\ttraining's l1: 0.154275\tvalid_1's l1: 0.639334\n",
      "[20500]\ttraining's l1: 0.149831\tvalid_1's l1: 0.638891\n",
      "[21000]\ttraining's l1: 0.145494\tvalid_1's l1: 0.638459\n",
      "[21500]\ttraining's l1: 0.141308\tvalid_1's l1: 0.638031\n",
      "[22000]\ttraining's l1: 0.13727\tvalid_1's l1: 0.637584\n",
      "[22500]\ttraining's l1: 0.133352\tvalid_1's l1: 0.637212\n",
      "[23000]\ttraining's l1: 0.129634\tvalid_1's l1: 0.636873\n",
      "[23500]\ttraining's l1: 0.125964\tvalid_1's l1: 0.636559\n",
      "[24000]\ttraining's l1: 0.122452\tvalid_1's l1: 0.636221\n",
      "[24500]\ttraining's l1: 0.119033\tvalid_1's l1: 0.635903\n",
      "[25000]\ttraining's l1: 0.115765\tvalid_1's l1: 0.635595\n",
      "[25500]\ttraining's l1: 0.112593\tvalid_1's l1: 0.635253\n",
      "[26000]\ttraining's l1: 0.109501\tvalid_1's l1: 0.634961\n",
      "[26500]\ttraining's l1: 0.10654\tvalid_1's l1: 0.634705\n",
      "[27000]\ttraining's l1: 0.103644\tvalid_1's l1: 0.634431\n",
      "[27500]\ttraining's l1: 0.10083\tvalid_1's l1: 0.634203\n",
      "[28000]\ttraining's l1: 0.0981319\tvalid_1's l1: 0.633934\n",
      "[28500]\ttraining's l1: 0.095496\tvalid_1's l1: 0.633691\n",
      "[29000]\ttraining's l1: 0.0929695\tvalid_1's l1: 0.63343\n",
      "[29500]\ttraining's l1: 0.0905025\tvalid_1's l1: 0.633218\n",
      "[30000]\ttraining's l1: 0.0881197\tvalid_1's l1: 0.633005\n",
      "[30500]\ttraining's l1: 0.0858349\tvalid_1's l1: 0.632806\n",
      "[31000]\ttraining's l1: 0.0836112\tvalid_1's l1: 0.632661\n",
      "[31500]\ttraining's l1: 0.0814258\tvalid_1's l1: 0.632481\n",
      "[32000]\ttraining's l1: 0.0793166\tvalid_1's l1: 0.632296\n",
      "[32500]\ttraining's l1: 0.0772483\tvalid_1's l1: 0.632121\n",
      "[33000]\ttraining's l1: 0.0752449\tvalid_1's l1: 0.631947\n",
      "[33500]\ttraining's l1: 0.0733326\tvalid_1's l1: 0.631784\n",
      "[34000]\ttraining's l1: 0.0714637\tvalid_1's l1: 0.631621\n",
      "[34500]\ttraining's l1: 0.0696425\tvalid_1's l1: 0.631498\n",
      "[35000]\ttraining's l1: 0.0678552\tvalid_1's l1: 0.631353\n",
      "[35500]\ttraining's l1: 0.0661468\tvalid_1's l1: 0.631198\n",
      "[36000]\ttraining's l1: 0.0644744\tvalid_1's l1: 0.63105\n",
      "[36500]\ttraining's l1: 0.0628357\tvalid_1's l1: 0.630935\n",
      "[37000]\ttraining's l1: 0.061241\tvalid_1's l1: 0.630799\n",
      "[37500]\ttraining's l1: 0.0597283\tvalid_1's l1: 0.630665\n",
      "[38000]\ttraining's l1: 0.0582466\tvalid_1's l1: 0.630554\n",
      "[38500]\ttraining's l1: 0.0568023\tvalid_1's l1: 0.63044\n",
      "[39000]\ttraining's l1: 0.0553932\tvalid_1's l1: 0.630343\n",
      "[39500]\ttraining's l1: 0.0540108\tvalid_1's l1: 0.630271\n",
      "[40000]\ttraining's l1: 0.0526951\tvalid_1's l1: 0.630183\n",
      "[40500]\ttraining's l1: 0.0514123\tvalid_1's l1: 0.630084\n",
      "[41000]\ttraining's l1: 0.0501548\tvalid_1's l1: 0.630001\n",
      "[41500]\ttraining's l1: 0.0489318\tvalid_1's l1: 0.62992\n",
      "[42000]\ttraining's l1: 0.0477338\tvalid_1's l1: 0.629838\n",
      "[42500]\ttraining's l1: 0.0465736\tvalid_1's l1: 0.629758\n",
      "[43000]\ttraining's l1: 0.0454412\tvalid_1's l1: 0.629679\n",
      "[43500]\ttraining's l1: 0.044328\tvalid_1's l1: 0.629601\n",
      "[44000]\ttraining's l1: 0.0432589\tvalid_1's l1: 0.629515\n",
      "[44500]\ttraining's l1: 0.0422281\tvalid_1's l1: 0.629438\n",
      "[45000]\ttraining's l1: 0.0412102\tvalid_1's l1: 0.629355\n",
      "[45500]\ttraining's l1: 0.0402444\tvalid_1's l1: 0.629294\n",
      "[46000]\ttraining's l1: 0.0392959\tvalid_1's l1: 0.629232\n",
      "[46500]\ttraining's l1: 0.038358\tvalid_1's l1: 0.629162\n",
      "[47000]\ttraining's l1: 0.0374633\tvalid_1's l1: 0.629114\n",
      "[47500]\ttraining's l1: 0.0365886\tvalid_1's l1: 0.629059\n",
      "[48000]\ttraining's l1: 0.0357294\tvalid_1's l1: 0.629009\n",
      "[48500]\ttraining's l1: 0.0348908\tvalid_1's l1: 0.628951\n",
      "[49000]\ttraining's l1: 0.034081\tvalid_1's l1: 0.628901\n",
      "[49500]\ttraining's l1: 0.0332843\tvalid_1's l1: 0.628852\n",
      "[50000]\ttraining's l1: 0.0325037\tvalid_1's l1: 0.628805\n",
      "[50500]\ttraining's l1: 0.0317493\tvalid_1's l1: 0.628755\n",
      "[51000]\ttraining's l1: 0.0310197\tvalid_1's l1: 0.628711\n",
      "[51500]\ttraining's l1: 0.0302964\tvalid_1's l1: 0.628669\n",
      "[52000]\ttraining's l1: 0.0296054\tvalid_1's l1: 0.628632\n",
      "[52500]\ttraining's l1: 0.0289275\tvalid_1's l1: 0.628592\n",
      "[53000]\ttraining's l1: 0.0282635\tvalid_1's l1: 0.628558\n",
      "[53500]\ttraining's l1: 0.0276103\tvalid_1's l1: 0.628518\n",
      "[54000]\ttraining's l1: 0.0269875\tvalid_1's l1: 0.628484\n",
      "[54500]\ttraining's l1: 0.0263789\tvalid_1's l1: 0.628445\n",
      "[55000]\ttraining's l1: 0.0257898\tvalid_1's l1: 0.628409\n",
      "[55500]\ttraining's l1: 0.0252052\tvalid_1's l1: 0.628382\n",
      "[56000]\ttraining's l1: 0.0246409\tvalid_1's l1: 0.628354\n",
      "[56500]\ttraining's l1: 0.0240868\tvalid_1's l1: 0.628315\n",
      "[57000]\ttraining's l1: 0.0235456\tvalid_1's l1: 0.628278\n",
      "[57500]\ttraining's l1: 0.0230154\tvalid_1's l1: 0.628247\n",
      "[58000]\ttraining's l1: 0.0225052\tvalid_1's l1: 0.628216\n",
      "[58500]\ttraining's l1: 0.0220016\tvalid_1's l1: 0.628185\n",
      "[59000]\ttraining's l1: 0.0215177\tvalid_1's l1: 0.628159\n",
      "[59500]\ttraining's l1: 0.0210421\tvalid_1's l1: 0.62813\n",
      "[60000]\ttraining's l1: 0.0205833\tvalid_1's l1: 0.628106\n",
      "[60500]\ttraining's l1: 0.0201305\tvalid_1's l1: 0.628085\n",
      "[61000]\ttraining's l1: 0.0196888\tvalid_1's l1: 0.628057\n",
      "[61500]\ttraining's l1: 0.0192583\tvalid_1's l1: 0.628035\n",
      "[62000]\ttraining's l1: 0.0188362\tvalid_1's l1: 0.62801\n",
      "[62500]\ttraining's l1: 0.0184268\tvalid_1's l1: 0.627986\n",
      "[63000]\ttraining's l1: 0.0180321\tvalid_1's l1: 0.627967\n",
      "[63500]\ttraining's l1: 0.0176404\tvalid_1's l1: 0.627946\n",
      "[64000]\ttraining's l1: 0.0172566\tvalid_1's l1: 0.627926\n",
      "[64500]\ttraining's l1: 0.016887\tvalid_1's l1: 0.627907\n",
      "[65000]\ttraining's l1: 0.0165244\tvalid_1's l1: 0.627892\n",
      "[65500]\ttraining's l1: 0.0161717\tvalid_1's l1: 0.627871\n",
      "[66000]\ttraining's l1: 0.0158266\tvalid_1's l1: 0.627853\n",
      "[66500]\ttraining's l1: 0.0154934\tvalid_1's l1: 0.627837\n",
      "[67000]\ttraining's l1: 0.0151642\tvalid_1's l1: 0.62782\n",
      "[67500]\ttraining's l1: 0.0148417\tvalid_1's l1: 0.627802\n",
      "[68000]\ttraining's l1: 0.0145232\tvalid_1's l1: 0.627782\n",
      "[68500]\ttraining's l1: 0.0142226\tvalid_1's l1: 0.627769\n",
      "[69000]\ttraining's l1: 0.0139214\tvalid_1's l1: 0.627751\n",
      "[69500]\ttraining's l1: 0.0136315\tvalid_1's l1: 0.627739\n",
      "[70000]\ttraining's l1: 0.0133505\tvalid_1's l1: 0.627723\n",
      "[70500]\ttraining's l1: 0.013078\tvalid_1's l1: 0.627716\n",
      "[71000]\ttraining's l1: 0.0128111\tvalid_1's l1: 0.627703\n",
      "[71500]\ttraining's l1: 0.0125463\tvalid_1's l1: 0.627692\n",
      "[72000]\ttraining's l1: 0.0122921\tvalid_1's l1: 0.62768\n",
      "[72500]\ttraining's l1: 0.0120385\tvalid_1's l1: 0.62767\n",
      "[73000]\ttraining's l1: 0.0117965\tvalid_1's l1: 0.627657\n",
      "[73500]\ttraining's l1: 0.0115606\tvalid_1's l1: 0.627644\n",
      "[74000]\ttraining's l1: 0.0113291\tvalid_1's l1: 0.627632\n",
      "[74500]\ttraining's l1: 0.0111043\tvalid_1's l1: 0.627622\n",
      "[75000]\ttraining's l1: 0.0108831\tvalid_1's l1: 0.627613\n",
      "[75500]\ttraining's l1: 0.0106663\tvalid_1's l1: 0.627604\n",
      "[76000]\ttraining's l1: 0.0104542\tvalid_1's l1: 0.627593\n",
      "[76500]\ttraining's l1: 0.0102467\tvalid_1's l1: 0.627581\n",
      "[77000]\ttraining's l1: 0.0100439\tvalid_1's l1: 0.627575\n",
      "[77500]\ttraining's l1: 0.00984968\tvalid_1's l1: 0.627569\n",
      "[78000]\ttraining's l1: 0.00965885\tvalid_1's l1: 0.627562\n",
      "[78500]\ttraining's l1: 0.00947007\tvalid_1's l1: 0.627551\n",
      "[79000]\ttraining's l1: 0.00928613\tvalid_1's l1: 0.627541\n",
      "[79500]\ttraining's l1: 0.00910648\tvalid_1's l1: 0.627535\n",
      "[80000]\ttraining's l1: 0.00893022\tvalid_1's l1: 0.627528\n",
      "[80500]\ttraining's l1: 0.00875878\tvalid_1's l1: 0.627521\n",
      "[81000]\ttraining's l1: 0.00859193\tvalid_1's l1: 0.627512\n",
      "[81500]\ttraining's l1: 0.00842891\tvalid_1's l1: 0.627505\n",
      "[82000]\ttraining's l1: 0.00827019\tvalid_1's l1: 0.627499\n",
      "[82500]\ttraining's l1: 0.00811478\tvalid_1's l1: 0.62749\n",
      "[83000]\ttraining's l1: 0.00796233\tvalid_1's l1: 0.627482\n",
      "[83500]\ttraining's l1: 0.00781359\tvalid_1's l1: 0.627477\n",
      "[84000]\ttraining's l1: 0.00766961\tvalid_1's l1: 0.627471\n",
      "[84500]\ttraining's l1: 0.0075276\tvalid_1's l1: 0.627466\n",
      "[85000]\ttraining's l1: 0.00738834\tvalid_1's l1: 0.627458\n",
      "[85500]\ttraining's l1: 0.00725119\tvalid_1's l1: 0.627452\n",
      "[86000]\ttraining's l1: 0.00711876\tvalid_1's l1: 0.627445\n",
      "[86500]\ttraining's l1: 0.0069897\tvalid_1's l1: 0.62744\n",
      "[87000]\ttraining's l1: 0.00686378\tvalid_1's l1: 0.627435\n",
      "[87500]\ttraining's l1: 0.0067392\tvalid_1's l1: 0.627431\n",
      "[88000]\ttraining's l1: 0.0066194\tvalid_1's l1: 0.627424\n",
      "[88500]\ttraining's l1: 0.0064997\tvalid_1's l1: 0.62742\n",
      "[89000]\ttraining's l1: 0.00638463\tvalid_1's l1: 0.627414\n",
      "[89500]\ttraining's l1: 0.00627199\tvalid_1's l1: 0.627409\n",
      "[90000]\ttraining's l1: 0.00616329\tvalid_1's l1: 0.627404\n",
      "[90500]\ttraining's l1: 0.00605469\tvalid_1's l1: 0.627398\n",
      "[91000]\ttraining's l1: 0.0059486\tvalid_1's l1: 0.627393\n",
      "[91500]\ttraining's l1: 0.00584489\tvalid_1's l1: 0.627388\n",
      "[92000]\ttraining's l1: 0.0057449\tvalid_1's l1: 0.627385\n",
      "[92500]\ttraining's l1: 0.00564783\tvalid_1's l1: 0.627382\n",
      "[93000]\ttraining's l1: 0.00555238\tvalid_1's l1: 0.627376\n",
      "[93500]\ttraining's l1: 0.00545897\tvalid_1's l1: 0.627371\n",
      "[94000]\ttraining's l1: 0.0053668\tvalid_1's l1: 0.627366\n",
      "[94500]\ttraining's l1: 0.00527841\tvalid_1's l1: 0.627361\n",
      "[95000]\ttraining's l1: 0.00519022\tvalid_1's l1: 0.627357\n",
      "[95500]\ttraining's l1: 0.005105\tvalid_1's l1: 0.627354\n",
      "[96000]\ttraining's l1: 0.00502067\tvalid_1's l1: 0.62735\n",
      "[96500]\ttraining's l1: 0.004938\tvalid_1's l1: 0.627346\n",
      "[97000]\ttraining's l1: 0.0048572\tvalid_1's l1: 0.627342\n",
      "[97500]\ttraining's l1: 0.00477901\tvalid_1's l1: 0.627339\n",
      "[98000]\ttraining's l1: 0.00470194\tvalid_1's l1: 0.627335\n",
      "[98500]\ttraining's l1: 0.00462725\tvalid_1's l1: 0.627331\n",
      "[99000]\ttraining's l1: 0.00455385\tvalid_1's l1: 0.627328\n",
      "[99500]\ttraining's l1: 0.00448083\tvalid_1's l1: 0.627325\n",
      "[100000]\ttraining's l1: 0.00441061\tvalid_1's l1: 0.627323\n",
      "[100500]\ttraining's l1: 0.00434183\tvalid_1's l1: 0.627319\n",
      "[101000]\ttraining's l1: 0.00427488\tvalid_1's l1: 0.627316\n",
      "[101500]\ttraining's l1: 0.00420968\tvalid_1's l1: 0.627313\n",
      "[102000]\ttraining's l1: 0.00414534\tvalid_1's l1: 0.62731\n",
      "[102500]\ttraining's l1: 0.00408283\tvalid_1's l1: 0.627308\n",
      "[103000]\ttraining's l1: 0.00402175\tvalid_1's l1: 0.627304\n",
      "[103500]\ttraining's l1: 0.00396168\tvalid_1's l1: 0.627301\n",
      "[104000]\ttraining's l1: 0.00390195\tvalid_1's l1: 0.627297\n",
      "[104500]\ttraining's l1: 0.00384377\tvalid_1's l1: 0.627294\n",
      "[105000]\ttraining's l1: 0.00378756\tvalid_1's l1: 0.627291\n",
      "[105500]\ttraining's l1: 0.00373241\tvalid_1's l1: 0.627287\n",
      "[106000]\ttraining's l1: 0.00367842\tvalid_1's l1: 0.627284\n",
      "[106500]\ttraining's l1: 0.0036252\tvalid_1's l1: 0.627283\n",
      "[107000]\ttraining's l1: 0.00357345\tvalid_1's l1: 0.627282\n",
      "[107500]\ttraining's l1: 0.00352213\tvalid_1's l1: 0.627279\n",
      "[108000]\ttraining's l1: 0.00347231\tvalid_1's l1: 0.627277\n",
      "[108500]\ttraining's l1: 0.00342323\tvalid_1's l1: 0.627275\n",
      "Early stopping, best iteration is:\n",
      "[108440]\ttraining's l1: 0.00342898\tvalid_1's l1: 0.627275\n",
      "Fold 3 started at Mon Jun 24 17:52:22 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.852921\tvalid_1's l1: 0.904091\n",
      "[1000]\ttraining's l1: 0.738091\tvalid_1's l1: 0.823243\n",
      "[1500]\ttraining's l1: 0.669344\tvalid_1's l1: 0.782991\n",
      "[2000]\ttraining's l1: 0.618272\tvalid_1's l1: 0.756885\n",
      "[2500]\ttraining's l1: 0.57858\tvalid_1's l1: 0.739784\n",
      "[3000]\ttraining's l1: 0.543874\tvalid_1's l1: 0.72549\n",
      "[3500]\ttraining's l1: 0.513708\tvalid_1's l1: 0.714384\n",
      "[4000]\ttraining's l1: 0.487608\tvalid_1's l1: 0.70543\n",
      "[4500]\ttraining's l1: 0.464222\tvalid_1's l1: 0.698335\n",
      "[5000]\ttraining's l1: 0.442711\tvalid_1's l1: 0.691942\n",
      "[5500]\ttraining's l1: 0.422635\tvalid_1's l1: 0.68624\n",
      "[6000]\ttraining's l1: 0.404351\tvalid_1's l1: 0.681406\n",
      "[6500]\ttraining's l1: 0.387311\tvalid_1's l1: 0.677335\n",
      "[7000]\ttraining's l1: 0.371518\tvalid_1's l1: 0.673579\n",
      "[7500]\ttraining's l1: 0.35668\tvalid_1's l1: 0.670254\n",
      "[8000]\ttraining's l1: 0.34275\tvalid_1's l1: 0.667323\n",
      "[8500]\ttraining's l1: 0.329808\tvalid_1's l1: 0.664829\n",
      "[9000]\ttraining's l1: 0.317521\tvalid_1's l1: 0.662368\n",
      "[9500]\ttraining's l1: 0.305843\tvalid_1's l1: 0.659987\n",
      "[10000]\ttraining's l1: 0.29474\tvalid_1's l1: 0.657863\n",
      "[10500]\ttraining's l1: 0.284454\tvalid_1's l1: 0.656047\n",
      "[11000]\ttraining's l1: 0.274474\tvalid_1's l1: 0.654453\n",
      "[11500]\ttraining's l1: 0.265018\tvalid_1's l1: 0.652782\n",
      "[12000]\ttraining's l1: 0.256042\tvalid_1's l1: 0.651492\n",
      "[12500]\ttraining's l1: 0.247456\tvalid_1's l1: 0.650262\n",
      "[13000]\ttraining's l1: 0.239387\tvalid_1's l1: 0.649217\n",
      "[13500]\ttraining's l1: 0.23158\tvalid_1's l1: 0.648026\n",
      "[14000]\ttraining's l1: 0.224044\tvalid_1's l1: 0.647102\n",
      "[14500]\ttraining's l1: 0.21694\tvalid_1's l1: 0.646205\n",
      "[15000]\ttraining's l1: 0.210052\tvalid_1's l1: 0.645257\n",
      "[15500]\ttraining's l1: 0.203464\tvalid_1's l1: 0.644314\n",
      "[16000]\ttraining's l1: 0.197188\tvalid_1's l1: 0.643388\n",
      "[16500]\ttraining's l1: 0.191047\tvalid_1's l1: 0.642511\n",
      "[17000]\ttraining's l1: 0.185176\tvalid_1's l1: 0.64174\n",
      "[17500]\ttraining's l1: 0.179561\tvalid_1's l1: 0.641138\n",
      "[18000]\ttraining's l1: 0.174094\tvalid_1's l1: 0.640411\n",
      "[18500]\ttraining's l1: 0.168915\tvalid_1's l1: 0.639819\n",
      "[19000]\ttraining's l1: 0.163868\tvalid_1's l1: 0.639209\n",
      "[19500]\ttraining's l1: 0.159037\tvalid_1's l1: 0.638631\n",
      "[20000]\ttraining's l1: 0.154448\tvalid_1's l1: 0.638095\n",
      "[20500]\ttraining's l1: 0.149997\tvalid_1's l1: 0.637616\n",
      "[21000]\ttraining's l1: 0.145665\tvalid_1's l1: 0.637182\n",
      "[21500]\ttraining's l1: 0.141487\tvalid_1's l1: 0.636631\n",
      "[22000]\ttraining's l1: 0.137425\tvalid_1's l1: 0.636234\n",
      "[22500]\ttraining's l1: 0.133554\tvalid_1's l1: 0.635811\n",
      "[23000]\ttraining's l1: 0.129795\tvalid_1's l1: 0.635448\n",
      "[23500]\ttraining's l1: 0.12616\tvalid_1's l1: 0.63514\n",
      "[24000]\ttraining's l1: 0.122639\tvalid_1's l1: 0.634803\n",
      "[24500]\ttraining's l1: 0.119232\tvalid_1's l1: 0.634438\n",
      "[25000]\ttraining's l1: 0.115906\tvalid_1's l1: 0.634102\n",
      "[25500]\ttraining's l1: 0.112699\tvalid_1's l1: 0.633793\n",
      "[26000]\ttraining's l1: 0.109601\tvalid_1's l1: 0.633546\n",
      "[26500]\ttraining's l1: 0.106612\tvalid_1's l1: 0.633294\n",
      "[27000]\ttraining's l1: 0.103727\tvalid_1's l1: 0.63305\n",
      "[27500]\ttraining's l1: 0.100946\tvalid_1's l1: 0.632821\n",
      "[28000]\ttraining's l1: 0.0982401\tvalid_1's l1: 0.632527\n",
      "[28500]\ttraining's l1: 0.0956248\tvalid_1's l1: 0.632283\n",
      "[29000]\ttraining's l1: 0.0930757\tvalid_1's l1: 0.632041\n",
      "[29500]\ttraining's l1: 0.0905859\tvalid_1's l1: 0.631829\n",
      "[30000]\ttraining's l1: 0.0882123\tvalid_1's l1: 0.631616\n",
      "[30500]\ttraining's l1: 0.0859042\tvalid_1's l1: 0.631414\n",
      "[31000]\ttraining's l1: 0.0836415\tvalid_1's l1: 0.631206\n",
      "[31500]\ttraining's l1: 0.0814645\tvalid_1's l1: 0.630985\n",
      "[32000]\ttraining's l1: 0.0793329\tvalid_1's l1: 0.630759\n",
      "[32500]\ttraining's l1: 0.0772851\tvalid_1's l1: 0.630592\n",
      "[33000]\ttraining's l1: 0.0752711\tvalid_1's l1: 0.63042\n",
      "[33500]\ttraining's l1: 0.0733558\tvalid_1's l1: 0.630264\n",
      "[34000]\ttraining's l1: 0.0714889\tvalid_1's l1: 0.630125\n",
      "[34500]\ttraining's l1: 0.0696689\tvalid_1's l1: 0.629998\n",
      "[35000]\ttraining's l1: 0.067926\tvalid_1's l1: 0.629876\n",
      "[35500]\ttraining's l1: 0.0662036\tvalid_1's l1: 0.629754\n",
      "[36000]\ttraining's l1: 0.0645345\tvalid_1's l1: 0.629648\n",
      "[36500]\ttraining's l1: 0.0629025\tvalid_1's l1: 0.629534\n",
      "[37000]\ttraining's l1: 0.0613354\tvalid_1's l1: 0.62938\n",
      "[37500]\ttraining's l1: 0.0597999\tvalid_1's l1: 0.629259\n",
      "[38000]\ttraining's l1: 0.0583221\tvalid_1's l1: 0.629158\n",
      "[38500]\ttraining's l1: 0.056857\tvalid_1's l1: 0.629038\n",
      "[39000]\ttraining's l1: 0.0554707\tvalid_1's l1: 0.628941\n",
      "[39500]\ttraining's l1: 0.0541108\tvalid_1's l1: 0.628843\n",
      "[40000]\ttraining's l1: 0.052771\tvalid_1's l1: 0.628725\n",
      "[40500]\ttraining's l1: 0.0514786\tvalid_1's l1: 0.628629\n",
      "[41000]\ttraining's l1: 0.0502234\tvalid_1's l1: 0.628524\n",
      "[41500]\ttraining's l1: 0.0490038\tvalid_1's l1: 0.628426\n",
      "[42000]\ttraining's l1: 0.047805\tvalid_1's l1: 0.62836\n",
      "[42500]\ttraining's l1: 0.046637\tvalid_1's l1: 0.628284\n",
      "[43000]\ttraining's l1: 0.0455162\tvalid_1's l1: 0.628213\n",
      "[43500]\ttraining's l1: 0.04443\tvalid_1's l1: 0.628162\n",
      "[44000]\ttraining's l1: 0.0433602\tvalid_1's l1: 0.628099\n",
      "[44500]\ttraining's l1: 0.0423171\tvalid_1's l1: 0.628042\n",
      "[45000]\ttraining's l1: 0.0413048\tvalid_1's l1: 0.627949\n",
      "[45500]\ttraining's l1: 0.0403074\tvalid_1's l1: 0.627876\n",
      "[46000]\ttraining's l1: 0.0393445\tvalid_1's l1: 0.627816\n",
      "[46500]\ttraining's l1: 0.0384055\tvalid_1's l1: 0.62775\n",
      "[47000]\ttraining's l1: 0.0375003\tvalid_1's l1: 0.627705\n",
      "[47500]\ttraining's l1: 0.0366254\tvalid_1's l1: 0.627631\n",
      "[48000]\ttraining's l1: 0.0357643\tvalid_1's l1: 0.627571\n",
      "[48500]\ttraining's l1: 0.0349303\tvalid_1's l1: 0.627516\n",
      "[49000]\ttraining's l1: 0.0341142\tvalid_1's l1: 0.627461\n",
      "[49500]\ttraining's l1: 0.033323\tvalid_1's l1: 0.627427\n",
      "[50000]\ttraining's l1: 0.0325505\tvalid_1's l1: 0.627389\n",
      "[50500]\ttraining's l1: 0.0317882\tvalid_1's l1: 0.627336\n",
      "[51000]\ttraining's l1: 0.0310461\tvalid_1's l1: 0.627298\n",
      "[51500]\ttraining's l1: 0.030332\tvalid_1's l1: 0.62725\n",
      "[52000]\ttraining's l1: 0.0296332\tvalid_1's l1: 0.627204\n",
      "[52500]\ttraining's l1: 0.028955\tvalid_1's l1: 0.627164\n",
      "[53000]\ttraining's l1: 0.0282936\tvalid_1's l1: 0.627118\n",
      "[53500]\ttraining's l1: 0.0276422\tvalid_1's l1: 0.627088\n",
      "[54000]\ttraining's l1: 0.0270117\tvalid_1's l1: 0.627053\n",
      "[54500]\ttraining's l1: 0.0264006\tvalid_1's l1: 0.62702\n",
      "[55000]\ttraining's l1: 0.0257955\tvalid_1's l1: 0.626988\n",
      "[55500]\ttraining's l1: 0.0252093\tvalid_1's l1: 0.626954\n",
      "[56000]\ttraining's l1: 0.0246429\tvalid_1's l1: 0.626924\n",
      "[56500]\ttraining's l1: 0.0240909\tvalid_1's l1: 0.626884\n",
      "[57000]\ttraining's l1: 0.0235465\tvalid_1's l1: 0.626847\n",
      "[57500]\ttraining's l1: 0.0230119\tvalid_1's l1: 0.626814\n",
      "[58000]\ttraining's l1: 0.0224982\tvalid_1's l1: 0.626788\n",
      "[58500]\ttraining's l1: 0.0220004\tvalid_1's l1: 0.626764\n",
      "[59000]\ttraining's l1: 0.0215168\tvalid_1's l1: 0.626734\n",
      "[59500]\ttraining's l1: 0.0210414\tvalid_1's l1: 0.626706\n",
      "[60000]\ttraining's l1: 0.0205735\tvalid_1's l1: 0.626671\n",
      "[60500]\ttraining's l1: 0.0201168\tvalid_1's l1: 0.626641\n",
      "[61000]\ttraining's l1: 0.0196761\tvalid_1's l1: 0.62662\n",
      "[61500]\ttraining's l1: 0.0192459\tvalid_1's l1: 0.626602\n",
      "[62000]\ttraining's l1: 0.0188253\tvalid_1's l1: 0.626579\n",
      "[62500]\ttraining's l1: 0.0184209\tvalid_1's l1: 0.626556\n",
      "[63000]\ttraining's l1: 0.0180234\tvalid_1's l1: 0.626541\n",
      "[63500]\ttraining's l1: 0.0176271\tvalid_1's l1: 0.626521\n",
      "[64000]\ttraining's l1: 0.0172453\tvalid_1's l1: 0.6265\n",
      "[64500]\ttraining's l1: 0.0168775\tvalid_1's l1: 0.626472\n",
      "[65000]\ttraining's l1: 0.0165167\tvalid_1's l1: 0.626455\n",
      "[65500]\ttraining's l1: 0.016162\tvalid_1's l1: 0.626437\n",
      "[66000]\ttraining's l1: 0.0158163\tvalid_1's l1: 0.62642\n",
      "[66500]\ttraining's l1: 0.0154794\tvalid_1's l1: 0.626406\n",
      "[67000]\ttraining's l1: 0.0151543\tvalid_1's l1: 0.626388\n",
      "[67500]\ttraining's l1: 0.0148363\tvalid_1's l1: 0.626378\n",
      "[68000]\ttraining's l1: 0.0145237\tvalid_1's l1: 0.626363\n",
      "[68500]\ttraining's l1: 0.0142221\tvalid_1's l1: 0.626354\n",
      "[69000]\ttraining's l1: 0.0139249\tvalid_1's l1: 0.626344\n",
      "[69500]\ttraining's l1: 0.0136352\tvalid_1's l1: 0.626334\n",
      "[70000]\ttraining's l1: 0.0133534\tvalid_1's l1: 0.626319\n",
      "[70500]\ttraining's l1: 0.013078\tvalid_1's l1: 0.626309\n",
      "[71000]\ttraining's l1: 0.0128069\tvalid_1's l1: 0.626296\n",
      "[71500]\ttraining's l1: 0.012541\tvalid_1's l1: 0.626283\n",
      "[72000]\ttraining's l1: 0.0122876\tvalid_1's l1: 0.626272\n",
      "[72500]\ttraining's l1: 0.0120398\tvalid_1's l1: 0.626262\n",
      "[73000]\ttraining's l1: 0.0117935\tvalid_1's l1: 0.626251\n",
      "[73500]\ttraining's l1: 0.0115554\tvalid_1's l1: 0.626235\n",
      "[74000]\ttraining's l1: 0.0113236\tvalid_1's l1: 0.626224\n",
      "[74500]\ttraining's l1: 0.0110988\tvalid_1's l1: 0.626211\n",
      "[75000]\ttraining's l1: 0.0108795\tvalid_1's l1: 0.626195\n",
      "[75500]\ttraining's l1: 0.0106642\tvalid_1's l1: 0.626186\n",
      "[76000]\ttraining's l1: 0.0104518\tvalid_1's l1: 0.626174\n",
      "[76500]\ttraining's l1: 0.0102461\tvalid_1's l1: 0.626164\n",
      "[77000]\ttraining's l1: 0.0100437\tvalid_1's l1: 0.626154\n",
      "[77500]\ttraining's l1: 0.00984628\tvalid_1's l1: 0.626143\n",
      "[78000]\ttraining's l1: 0.00965247\tvalid_1's l1: 0.626136\n",
      "[78500]\ttraining's l1: 0.00946507\tvalid_1's l1: 0.626128\n",
      "[79000]\ttraining's l1: 0.0092811\tvalid_1's l1: 0.626119\n",
      "[79500]\ttraining's l1: 0.00910335\tvalid_1's l1: 0.626112\n",
      "[80000]\ttraining's l1: 0.00892672\tvalid_1's l1: 0.626103\n",
      "[80500]\ttraining's l1: 0.00875331\tvalid_1's l1: 0.626094\n",
      "[81000]\ttraining's l1: 0.00858561\tvalid_1's l1: 0.626086\n",
      "[81500]\ttraining's l1: 0.00842105\tvalid_1's l1: 0.626076\n",
      "[82000]\ttraining's l1: 0.00826439\tvalid_1's l1: 0.626069\n",
      "[82500]\ttraining's l1: 0.00810915\tvalid_1's l1: 0.626059\n",
      "[83000]\ttraining's l1: 0.00795818\tvalid_1's l1: 0.626054\n",
      "[83500]\ttraining's l1: 0.00781001\tvalid_1's l1: 0.626045\n",
      "[84000]\ttraining's l1: 0.00766526\tvalid_1's l1: 0.62604\n",
      "[84500]\ttraining's l1: 0.00752378\tvalid_1's l1: 0.626033\n",
      "[85000]\ttraining's l1: 0.00738505\tvalid_1's l1: 0.626024\n",
      "[85500]\ttraining's l1: 0.00724875\tvalid_1's l1: 0.626018\n",
      "[86000]\ttraining's l1: 0.00711567\tvalid_1's l1: 0.626013\n",
      "[86500]\ttraining's l1: 0.00698631\tvalid_1's l1: 0.626008\n",
      "[87000]\ttraining's l1: 0.00685844\tvalid_1's l1: 0.626005\n",
      "[87500]\ttraining's l1: 0.00673591\tvalid_1's l1: 0.625999\n",
      "[88000]\ttraining's l1: 0.00661629\tvalid_1's l1: 0.625994\n",
      "[88500]\ttraining's l1: 0.00649778\tvalid_1's l1: 0.625987\n",
      "[89000]\ttraining's l1: 0.00638203\tvalid_1's l1: 0.625983\n",
      "[89500]\ttraining's l1: 0.00627071\tvalid_1's l1: 0.625978\n",
      "[90000]\ttraining's l1: 0.00616056\tvalid_1's l1: 0.625973\n",
      "[90500]\ttraining's l1: 0.00605501\tvalid_1's l1: 0.625967\n",
      "[91000]\ttraining's l1: 0.00594962\tvalid_1's l1: 0.625962\n",
      "[91500]\ttraining's l1: 0.00584538\tvalid_1's l1: 0.625957\n",
      "[92000]\ttraining's l1: 0.00574505\tvalid_1's l1: 0.625953\n",
      "[92500]\ttraining's l1: 0.00564682\tvalid_1's l1: 0.625949\n",
      "[93000]\ttraining's l1: 0.00555259\tvalid_1's l1: 0.625944\n",
      "[93500]\ttraining's l1: 0.00545807\tvalid_1's l1: 0.625939\n",
      "[94000]\ttraining's l1: 0.00536738\tvalid_1's l1: 0.625935\n",
      "[94500]\ttraining's l1: 0.00527797\tvalid_1's l1: 0.625932\n",
      "[95000]\ttraining's l1: 0.00519048\tvalid_1's l1: 0.625928\n",
      "[95500]\ttraining's l1: 0.00510391\tvalid_1's l1: 0.625922\n",
      "[96000]\ttraining's l1: 0.00501904\tvalid_1's l1: 0.62592\n",
      "[96500]\ttraining's l1: 0.00493708\tvalid_1's l1: 0.625916\n",
      "[97000]\ttraining's l1: 0.00485851\tvalid_1's l1: 0.625913\n",
      "[97500]\ttraining's l1: 0.00478076\tvalid_1's l1: 0.625909\n",
      "[98000]\ttraining's l1: 0.00470416\tvalid_1's l1: 0.625904\n",
      "Early stopping, best iteration is:\n",
      "[98286]\ttraining's l1: 0.00466036\tvalid_1's l1: 0.625902\n",
      "Fold 4 started at Mon Jun 24 18:54:48 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.845759\tvalid_1's l1: 0.895888\n",
      "[1000]\ttraining's l1: 0.73491\tvalid_1's l1: 0.819479\n",
      "[1500]\ttraining's l1: 0.668088\tvalid_1's l1: 0.780973\n",
      "[2000]\ttraining's l1: 0.617564\tvalid_1's l1: 0.755647\n",
      "[2500]\ttraining's l1: 0.577253\tvalid_1's l1: 0.737757\n",
      "[3000]\ttraining's l1: 0.542648\tvalid_1's l1: 0.723632\n",
      "[3500]\ttraining's l1: 0.513415\tvalid_1's l1: 0.713632\n",
      "[4000]\ttraining's l1: 0.487051\tvalid_1's l1: 0.704689\n",
      "[4500]\ttraining's l1: 0.463004\tvalid_1's l1: 0.696887\n",
      "[5000]\ttraining's l1: 0.441574\tvalid_1's l1: 0.691018\n",
      "[5500]\ttraining's l1: 0.42181\tvalid_1's l1: 0.685565\n",
      "[6000]\ttraining's l1: 0.403181\tvalid_1's l1: 0.680266\n",
      "[6500]\ttraining's l1: 0.386524\tvalid_1's l1: 0.676638\n",
      "[7000]\ttraining's l1: 0.370816\tvalid_1's l1: 0.673109\n",
      "[7500]\ttraining's l1: 0.356034\tvalid_1's l1: 0.669836\n",
      "[8000]\ttraining's l1: 0.342059\tvalid_1's l1: 0.666744\n",
      "[8500]\ttraining's l1: 0.329066\tvalid_1's l1: 0.664173\n",
      "[9000]\ttraining's l1: 0.316914\tvalid_1's l1: 0.661813\n",
      "[9500]\ttraining's l1: 0.305282\tvalid_1's l1: 0.659588\n",
      "[10000]\ttraining's l1: 0.294292\tvalid_1's l1: 0.657557\n",
      "[10500]\ttraining's l1: 0.283897\tvalid_1's l1: 0.655788\n",
      "[11000]\ttraining's l1: 0.273958\tvalid_1's l1: 0.654107\n",
      "[11500]\ttraining's l1: 0.264522\tvalid_1's l1: 0.652728\n",
      "[12000]\ttraining's l1: 0.255564\tvalid_1's l1: 0.651361\n",
      "[12500]\ttraining's l1: 0.24708\tvalid_1's l1: 0.650181\n",
      "[13000]\ttraining's l1: 0.238977\tvalid_1's l1: 0.648982\n",
      "[13500]\ttraining's l1: 0.231184\tvalid_1's l1: 0.647804\n",
      "[14000]\ttraining's l1: 0.223685\tvalid_1's l1: 0.646634\n",
      "[14500]\ttraining's l1: 0.216508\tvalid_1's l1: 0.645712\n",
      "[15000]\ttraining's l1: 0.209682\tvalid_1's l1: 0.644709\n",
      "[15500]\ttraining's l1: 0.203119\tvalid_1's l1: 0.643839\n",
      "[16000]\ttraining's l1: 0.196772\tvalid_1's l1: 0.643008\n",
      "[16500]\ttraining's l1: 0.1907\tvalid_1's l1: 0.642257\n",
      "[17000]\ttraining's l1: 0.184905\tvalid_1's l1: 0.64147\n",
      "[17500]\ttraining's l1: 0.179277\tvalid_1's l1: 0.640783\n",
      "[18000]\ttraining's l1: 0.173873\tvalid_1's l1: 0.64012\n",
      "[18500]\ttraining's l1: 0.168731\tvalid_1's l1: 0.639587\n",
      "[19000]\ttraining's l1: 0.163707\tvalid_1's l1: 0.639037\n",
      "[19500]\ttraining's l1: 0.158936\tvalid_1's l1: 0.638481\n",
      "[20000]\ttraining's l1: 0.154322\tvalid_1's l1: 0.637978\n",
      "[20500]\ttraining's l1: 0.149859\tvalid_1's l1: 0.637511\n",
      "[21000]\ttraining's l1: 0.145492\tvalid_1's l1: 0.636978\n",
      "[21500]\ttraining's l1: 0.141347\tvalid_1's l1: 0.636559\n",
      "[22000]\ttraining's l1: 0.137333\tvalid_1's l1: 0.636109\n",
      "[22500]\ttraining's l1: 0.133431\tvalid_1's l1: 0.635682\n",
      "[23000]\ttraining's l1: 0.129667\tvalid_1's l1: 0.635319\n",
      "[23500]\ttraining's l1: 0.125959\tvalid_1's l1: 0.634986\n",
      "[24000]\ttraining's l1: 0.122428\tvalid_1's l1: 0.634638\n",
      "[24500]\ttraining's l1: 0.119029\tvalid_1's l1: 0.634343\n",
      "[25000]\ttraining's l1: 0.115753\tvalid_1's l1: 0.633987\n",
      "[25500]\ttraining's l1: 0.112585\tvalid_1's l1: 0.633668\n",
      "[26000]\ttraining's l1: 0.109525\tvalid_1's l1: 0.633359\n",
      "[26500]\ttraining's l1: 0.106539\tvalid_1's l1: 0.63307\n",
      "[27000]\ttraining's l1: 0.103672\tvalid_1's l1: 0.632811\n",
      "[27500]\ttraining's l1: 0.100881\tvalid_1's l1: 0.632517\n",
      "[28000]\ttraining's l1: 0.098184\tvalid_1's l1: 0.632268\n",
      "[28500]\ttraining's l1: 0.0955494\tvalid_1's l1: 0.632034\n",
      "[29000]\ttraining's l1: 0.0929941\tvalid_1's l1: 0.631807\n",
      "[29500]\ttraining's l1: 0.0905363\tvalid_1's l1: 0.631612\n",
      "[30000]\ttraining's l1: 0.0881478\tvalid_1's l1: 0.631409\n",
      "[30500]\ttraining's l1: 0.0858381\tvalid_1's l1: 0.631215\n",
      "[31000]\ttraining's l1: 0.0835888\tvalid_1's l1: 0.631025\n",
      "[31500]\ttraining's l1: 0.0814188\tvalid_1's l1: 0.630878\n",
      "[32000]\ttraining's l1: 0.0793351\tvalid_1's l1: 0.630718\n",
      "[32500]\ttraining's l1: 0.0772671\tvalid_1's l1: 0.630562\n",
      "[33000]\ttraining's l1: 0.0752744\tvalid_1's l1: 0.630408\n",
      "[33500]\ttraining's l1: 0.07336\tvalid_1's l1: 0.630279\n",
      "[34000]\ttraining's l1: 0.0714611\tvalid_1's l1: 0.630127\n",
      "[34500]\ttraining's l1: 0.0696297\tvalid_1's l1: 0.629976\n",
      "[35000]\ttraining's l1: 0.0678607\tvalid_1's l1: 0.629826\n",
      "[35500]\ttraining's l1: 0.066152\tvalid_1's l1: 0.629673\n",
      "[36000]\ttraining's l1: 0.0645005\tvalid_1's l1: 0.629549\n",
      "[36500]\ttraining's l1: 0.0628701\tvalid_1's l1: 0.629426\n",
      "[37000]\ttraining's l1: 0.0612912\tvalid_1's l1: 0.629292\n",
      "[37500]\ttraining's l1: 0.0597761\tvalid_1's l1: 0.62919\n",
      "[38000]\ttraining's l1: 0.0582793\tvalid_1's l1: 0.629064\n",
      "[38500]\ttraining's l1: 0.0568364\tvalid_1's l1: 0.628965\n",
      "[39000]\ttraining's l1: 0.0554297\tvalid_1's l1: 0.62888\n",
      "[39500]\ttraining's l1: 0.0540689\tvalid_1's l1: 0.628791\n",
      "[40000]\ttraining's l1: 0.0527433\tvalid_1's l1: 0.62869\n",
      "[40500]\ttraining's l1: 0.0514429\tvalid_1's l1: 0.628609\n",
      "[41000]\ttraining's l1: 0.0501665\tvalid_1's l1: 0.628508\n",
      "[41500]\ttraining's l1: 0.0489377\tvalid_1's l1: 0.628421\n",
      "[42000]\ttraining's l1: 0.0477373\tvalid_1's l1: 0.628342\n",
      "[42500]\ttraining's l1: 0.0465742\tvalid_1's l1: 0.628251\n",
      "[43000]\ttraining's l1: 0.0454644\tvalid_1's l1: 0.628173\n",
      "[43500]\ttraining's l1: 0.0443812\tvalid_1's l1: 0.628102\n",
      "[44000]\ttraining's l1: 0.0433073\tvalid_1's l1: 0.628017\n",
      "[44500]\ttraining's l1: 0.0422704\tvalid_1's l1: 0.62794\n",
      "[45000]\ttraining's l1: 0.041255\tvalid_1's l1: 0.627875\n",
      "[45500]\ttraining's l1: 0.0402692\tvalid_1's l1: 0.627806\n",
      "[46000]\ttraining's l1: 0.0393184\tvalid_1's l1: 0.627744\n",
      "[46500]\ttraining's l1: 0.0383869\tvalid_1's l1: 0.627694\n",
      "[47000]\ttraining's l1: 0.0374843\tvalid_1's l1: 0.627668\n",
      "[47500]\ttraining's l1: 0.0365953\tvalid_1's l1: 0.627612\n",
      "[48000]\ttraining's l1: 0.0357404\tvalid_1's l1: 0.627548\n",
      "[48500]\ttraining's l1: 0.0349069\tvalid_1's l1: 0.627481\n",
      "[49000]\ttraining's l1: 0.0340983\tvalid_1's l1: 0.62743\n",
      "[49500]\ttraining's l1: 0.0332996\tvalid_1's l1: 0.627384\n",
      "[50000]\ttraining's l1: 0.0325294\tvalid_1's l1: 0.62735\n",
      "[50500]\ttraining's l1: 0.0317887\tvalid_1's l1: 0.627307\n",
      "[51000]\ttraining's l1: 0.0310495\tvalid_1's l1: 0.627269\n",
      "[51500]\ttraining's l1: 0.0303313\tvalid_1's l1: 0.627227\n",
      "[52000]\ttraining's l1: 0.0296411\tvalid_1's l1: 0.627179\n",
      "[52500]\ttraining's l1: 0.0289631\tvalid_1's l1: 0.627148\n",
      "[53000]\ttraining's l1: 0.0283021\tvalid_1's l1: 0.627114\n",
      "[53500]\ttraining's l1: 0.0276547\tvalid_1's l1: 0.627075\n",
      "[54000]\ttraining's l1: 0.027025\tvalid_1's l1: 0.627039\n",
      "[54500]\ttraining's l1: 0.0264048\tvalid_1's l1: 0.627001\n",
      "[55000]\ttraining's l1: 0.025811\tvalid_1's l1: 0.626967\n",
      "[55500]\ttraining's l1: 0.025228\tvalid_1's l1: 0.626931\n",
      "[56000]\ttraining's l1: 0.0246614\tvalid_1's l1: 0.626885\n",
      "[56500]\ttraining's l1: 0.0241134\tvalid_1's l1: 0.626853\n",
      "[57000]\ttraining's l1: 0.02357\tvalid_1's l1: 0.626826\n",
      "[57500]\ttraining's l1: 0.0230463\tvalid_1's l1: 0.626793\n",
      "[58000]\ttraining's l1: 0.0225331\tvalid_1's l1: 0.626756\n",
      "[58500]\ttraining's l1: 0.0220306\tvalid_1's l1: 0.626725\n",
      "[59000]\ttraining's l1: 0.0215397\tvalid_1's l1: 0.626703\n",
      "[59500]\ttraining's l1: 0.021058\tvalid_1's l1: 0.626682\n",
      "[60000]\ttraining's l1: 0.0205996\tvalid_1's l1: 0.626658\n",
      "[60500]\ttraining's l1: 0.0201448\tvalid_1's l1: 0.62664\n",
      "[61000]\ttraining's l1: 0.0197105\tvalid_1's l1: 0.626617\n",
      "[61500]\ttraining's l1: 0.0192784\tvalid_1's l1: 0.626594\n",
      "[62000]\ttraining's l1: 0.0188541\tvalid_1's l1: 0.626578\n",
      "[62500]\ttraining's l1: 0.0184462\tvalid_1's l1: 0.626558\n",
      "[63000]\ttraining's l1: 0.0180487\tvalid_1's l1: 0.626539\n",
      "[63500]\ttraining's l1: 0.0176587\tvalid_1's l1: 0.626516\n",
      "[64000]\ttraining's l1: 0.0172789\tvalid_1's l1: 0.626496\n",
      "[64500]\ttraining's l1: 0.0169075\tvalid_1's l1: 0.626476\n",
      "[65000]\ttraining's l1: 0.0165456\tvalid_1's l1: 0.626458\n",
      "[65500]\ttraining's l1: 0.0161923\tvalid_1's l1: 0.626434\n",
      "[66000]\ttraining's l1: 0.0158499\tvalid_1's l1: 0.626421\n",
      "[66500]\ttraining's l1: 0.015516\tvalid_1's l1: 0.626401\n",
      "[67000]\ttraining's l1: 0.0151881\tvalid_1's l1: 0.626379\n",
      "[67500]\ttraining's l1: 0.0148682\tvalid_1's l1: 0.626366\n",
      "[68000]\ttraining's l1: 0.0145567\tvalid_1's l1: 0.626346\n",
      "[68500]\ttraining's l1: 0.0142491\tvalid_1's l1: 0.62633\n",
      "[69000]\ttraining's l1: 0.0139514\tvalid_1's l1: 0.626312\n",
      "[69500]\ttraining's l1: 0.0136639\tvalid_1's l1: 0.626301\n",
      "[70000]\ttraining's l1: 0.0133808\tvalid_1's l1: 0.626283\n",
      "[70500]\ttraining's l1: 0.0131044\tvalid_1's l1: 0.626269\n",
      "[71000]\ttraining's l1: 0.0128342\tvalid_1's l1: 0.626252\n",
      "[71500]\ttraining's l1: 0.0125718\tvalid_1's l1: 0.626243\n",
      "[72000]\ttraining's l1: 0.0123152\tvalid_1's l1: 0.626233\n",
      "[72500]\ttraining's l1: 0.0120629\tvalid_1's l1: 0.626223\n",
      "[73000]\ttraining's l1: 0.0118174\tvalid_1's l1: 0.62621\n",
      "[73500]\ttraining's l1: 0.0115749\tvalid_1's l1: 0.6262\n",
      "[74000]\ttraining's l1: 0.0113411\tvalid_1's l1: 0.626188\n",
      "[74500]\ttraining's l1: 0.0111114\tvalid_1's l1: 0.626175\n",
      "[75000]\ttraining's l1: 0.0108886\tvalid_1's l1: 0.626165\n",
      "[75500]\ttraining's l1: 0.0106735\tvalid_1's l1: 0.626152\n",
      "[76000]\ttraining's l1: 0.0104629\tvalid_1's l1: 0.626142\n",
      "[76500]\ttraining's l1: 0.0102574\tvalid_1's l1: 0.626129\n",
      "[77000]\ttraining's l1: 0.0100535\tvalid_1's l1: 0.626121\n",
      "[77500]\ttraining's l1: 0.00985589\tvalid_1's l1: 0.626112\n",
      "[78000]\ttraining's l1: 0.00966059\tvalid_1's l1: 0.626103\n",
      "[78500]\ttraining's l1: 0.0094718\tvalid_1's l1: 0.626091\n",
      "[79000]\ttraining's l1: 0.00928852\tvalid_1's l1: 0.62608\n",
      "[79500]\ttraining's l1: 0.00910826\tvalid_1's l1: 0.626075\n",
      "[80000]\ttraining's l1: 0.00893359\tvalid_1's l1: 0.626066\n",
      "[80500]\ttraining's l1: 0.00875931\tvalid_1's l1: 0.626058\n",
      "[81000]\ttraining's l1: 0.008593\tvalid_1's l1: 0.626049\n",
      "[81500]\ttraining's l1: 0.00843024\tvalid_1's l1: 0.626042\n",
      "[82000]\ttraining's l1: 0.00826951\tvalid_1's l1: 0.626034\n",
      "[82500]\ttraining's l1: 0.00811402\tvalid_1's l1: 0.626025\n",
      "[83000]\ttraining's l1: 0.00796179\tvalid_1's l1: 0.626018\n",
      "[83500]\ttraining's l1: 0.00781379\tvalid_1's l1: 0.626011\n",
      "[84000]\ttraining's l1: 0.00766844\tvalid_1's l1: 0.626005\n",
      "[84500]\ttraining's l1: 0.00752666\tvalid_1's l1: 0.626001\n",
      "[85000]\ttraining's l1: 0.00738803\tvalid_1's l1: 0.625991\n",
      "[85500]\ttraining's l1: 0.00725209\tvalid_1's l1: 0.625984\n",
      "[86000]\ttraining's l1: 0.00711845\tvalid_1's l1: 0.62598\n",
      "[86500]\ttraining's l1: 0.0069896\tvalid_1's l1: 0.625973\n",
      "[87000]\ttraining's l1: 0.0068635\tvalid_1's l1: 0.625968\n",
      "[87500]\ttraining's l1: 0.00673864\tvalid_1's l1: 0.625964\n",
      "[88000]\ttraining's l1: 0.00661774\tvalid_1's l1: 0.625958\n",
      "[88500]\ttraining's l1: 0.00649929\tvalid_1's l1: 0.625955\n",
      "[89000]\ttraining's l1: 0.00638457\tvalid_1's l1: 0.625952\n",
      "[89500]\ttraining's l1: 0.00627177\tvalid_1's l1: 0.625945\n",
      "[90000]\ttraining's l1: 0.00616223\tvalid_1's l1: 0.625943\n",
      "[90500]\ttraining's l1: 0.00605404\tvalid_1's l1: 0.625936\n",
      "[91000]\ttraining's l1: 0.00595091\tvalid_1's l1: 0.62593\n",
      "[91500]\ttraining's l1: 0.00584735\tvalid_1's l1: 0.625925\n",
      "[92000]\ttraining's l1: 0.00574671\tvalid_1's l1: 0.625922\n",
      "[92500]\ttraining's l1: 0.00564769\tvalid_1's l1: 0.625916\n",
      "[93000]\ttraining's l1: 0.00554992\tvalid_1's l1: 0.625913\n",
      "[93500]\ttraining's l1: 0.00545683\tvalid_1's l1: 0.625909\n",
      "[94000]\ttraining's l1: 0.00536481\tvalid_1's l1: 0.625903\n",
      "[94500]\ttraining's l1: 0.00527573\tvalid_1's l1: 0.625898\n",
      "[95000]\ttraining's l1: 0.00518738\tvalid_1's l1: 0.625895\n",
      "[95500]\ttraining's l1: 0.00510201\tvalid_1's l1: 0.625891\n",
      "[96000]\ttraining's l1: 0.00501722\tvalid_1's l1: 0.625889\n",
      "[96500]\ttraining's l1: 0.00493586\tvalid_1's l1: 0.625885\n",
      "[97000]\ttraining's l1: 0.00485593\tvalid_1's l1: 0.625882\n",
      "[97500]\ttraining's l1: 0.00477711\tvalid_1's l1: 0.625879\n",
      "[98000]\ttraining's l1: 0.00470095\tvalid_1's l1: 0.625876\n",
      "[98500]\ttraining's l1: 0.00462518\tvalid_1's l1: 0.625872\n",
      "Early stopping, best iteration is:\n",
      "[98766]\ttraining's l1: 0.00458529\tvalid_1's l1: 0.62587\n",
      "Fold 5 started at Mon Jun 24 19:59:25 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.84662\tvalid_1's l1: 0.897565\n",
      "[1000]\ttraining's l1: 0.73638\tvalid_1's l1: 0.822007\n",
      "[1500]\ttraining's l1: 0.668432\tvalid_1's l1: 0.782155\n",
      "[2000]\ttraining's l1: 0.617354\tvalid_1's l1: 0.756043\n",
      "[2500]\ttraining's l1: 0.576961\tvalid_1's l1: 0.738615\n",
      "[3000]\ttraining's l1: 0.54319\tvalid_1's l1: 0.725474\n",
      "[3500]\ttraining's l1: 0.513425\tvalid_1's l1: 0.715109\n",
      "[4000]\ttraining's l1: 0.486569\tvalid_1's l1: 0.705676\n",
      "[4500]\ttraining's l1: 0.462836\tvalid_1's l1: 0.69809\n",
      "[5000]\ttraining's l1: 0.441383\tvalid_1's l1: 0.692275\n",
      "[5500]\ttraining's l1: 0.421562\tvalid_1's l1: 0.687065\n",
      "[6000]\ttraining's l1: 0.403534\tvalid_1's l1: 0.682523\n",
      "[6500]\ttraining's l1: 0.386549\tvalid_1's l1: 0.678568\n",
      "[7000]\ttraining's l1: 0.370703\tvalid_1's l1: 0.674544\n",
      "[7500]\ttraining's l1: 0.356011\tvalid_1's l1: 0.671635\n",
      "[8000]\ttraining's l1: 0.342282\tvalid_1's l1: 0.668822\n",
      "[8500]\ttraining's l1: 0.329253\tvalid_1's l1: 0.666255\n",
      "[9000]\ttraining's l1: 0.316956\tvalid_1's l1: 0.663663\n",
      "[9500]\ttraining's l1: 0.305432\tvalid_1's l1: 0.661649\n",
      "[10000]\ttraining's l1: 0.294386\tvalid_1's l1: 0.659674\n",
      "[10500]\ttraining's l1: 0.283935\tvalid_1's l1: 0.657858\n",
      "[11000]\ttraining's l1: 0.274137\tvalid_1's l1: 0.656241\n",
      "[11500]\ttraining's l1: 0.264767\tvalid_1's l1: 0.654627\n",
      "[12000]\ttraining's l1: 0.255823\tvalid_1's l1: 0.653156\n",
      "[12500]\ttraining's l1: 0.247169\tvalid_1's l1: 0.651772\n",
      "[13000]\ttraining's l1: 0.23881\tvalid_1's l1: 0.650387\n",
      "[13500]\ttraining's l1: 0.231016\tvalid_1's l1: 0.649289\n",
      "[14000]\ttraining's l1: 0.223625\tvalid_1's l1: 0.648112\n",
      "[14500]\ttraining's l1: 0.216407\tvalid_1's l1: 0.647015\n",
      "[15000]\ttraining's l1: 0.20953\tvalid_1's l1: 0.646038\n",
      "[15500]\ttraining's l1: 0.20294\tvalid_1's l1: 0.64514\n",
      "[16000]\ttraining's l1: 0.196624\tvalid_1's l1: 0.644195\n",
      "[16500]\ttraining's l1: 0.190602\tvalid_1's l1: 0.643526\n",
      "[17000]\ttraining's l1: 0.184796\tvalid_1's l1: 0.642825\n",
      "[17500]\ttraining's l1: 0.1792\tvalid_1's l1: 0.642071\n",
      "[18000]\ttraining's l1: 0.173835\tvalid_1's l1: 0.641455\n",
      "[18500]\ttraining's l1: 0.168607\tvalid_1's l1: 0.640869\n",
      "[19000]\ttraining's l1: 0.163604\tvalid_1's l1: 0.640366\n",
      "[19500]\ttraining's l1: 0.158772\tvalid_1's l1: 0.639807\n",
      "[20000]\ttraining's l1: 0.154173\tvalid_1's l1: 0.639229\n",
      "[20500]\ttraining's l1: 0.149682\tvalid_1's l1: 0.638654\n",
      "[21000]\ttraining's l1: 0.145274\tvalid_1's l1: 0.638154\n",
      "[21500]\ttraining's l1: 0.141066\tvalid_1's l1: 0.63773\n",
      "[22000]\ttraining's l1: 0.137065\tvalid_1's l1: 0.637273\n",
      "[22500]\ttraining's l1: 0.133153\tvalid_1's l1: 0.636898\n",
      "[23000]\ttraining's l1: 0.12939\tvalid_1's l1: 0.636525\n",
      "[23500]\ttraining's l1: 0.125785\tvalid_1's l1: 0.636195\n",
      "[24000]\ttraining's l1: 0.122255\tvalid_1's l1: 0.635848\n",
      "[24500]\ttraining's l1: 0.118865\tvalid_1's l1: 0.635527\n",
      "[25000]\ttraining's l1: 0.115611\tvalid_1's l1: 0.63521\n",
      "[25500]\ttraining's l1: 0.112454\tvalid_1's l1: 0.634907\n",
      "[26000]\ttraining's l1: 0.109364\tvalid_1's l1: 0.634612\n",
      "[26500]\ttraining's l1: 0.106368\tvalid_1's l1: 0.634362\n",
      "[27000]\ttraining's l1: 0.103466\tvalid_1's l1: 0.63411\n",
      "[27500]\ttraining's l1: 0.10067\tvalid_1's l1: 0.633857\n",
      "[28000]\ttraining's l1: 0.0979757\tvalid_1's l1: 0.633642\n",
      "[28500]\ttraining's l1: 0.0953442\tvalid_1's l1: 0.633416\n",
      "[29000]\ttraining's l1: 0.0928092\tvalid_1's l1: 0.633164\n",
      "[29500]\ttraining's l1: 0.0903511\tvalid_1's l1: 0.632943\n",
      "[30000]\ttraining's l1: 0.0879861\tvalid_1's l1: 0.632767\n",
      "[30500]\ttraining's l1: 0.0856438\tvalid_1's l1: 0.63258\n",
      "[31000]\ttraining's l1: 0.0834383\tvalid_1's l1: 0.632354\n",
      "[31500]\ttraining's l1: 0.0812822\tvalid_1's l1: 0.632174\n",
      "[32000]\ttraining's l1: 0.0791784\tvalid_1's l1: 0.631999\n",
      "[32500]\ttraining's l1: 0.0771026\tvalid_1's l1: 0.63182\n",
      "[33000]\ttraining's l1: 0.075084\tvalid_1's l1: 0.631672\n",
      "[33500]\ttraining's l1: 0.07313\tvalid_1's l1: 0.631497\n",
      "[34000]\ttraining's l1: 0.071279\tvalid_1's l1: 0.631356\n",
      "[34500]\ttraining's l1: 0.0694847\tvalid_1's l1: 0.631243\n",
      "[35000]\ttraining's l1: 0.0677102\tvalid_1's l1: 0.631093\n",
      "[35500]\ttraining's l1: 0.066001\tvalid_1's l1: 0.630959\n",
      "[36000]\ttraining's l1: 0.0643169\tvalid_1's l1: 0.630832\n",
      "[36500]\ttraining's l1: 0.0627038\tvalid_1's l1: 0.630717\n",
      "[37000]\ttraining's l1: 0.0611296\tvalid_1's l1: 0.630603\n",
      "[37500]\ttraining's l1: 0.0595874\tvalid_1's l1: 0.630509\n",
      "[38000]\ttraining's l1: 0.0580952\tvalid_1's l1: 0.630399\n",
      "[38500]\ttraining's l1: 0.056639\tvalid_1's l1: 0.630292\n",
      "[39000]\ttraining's l1: 0.0552172\tvalid_1's l1: 0.630179\n",
      "[39500]\ttraining's l1: 0.0538527\tvalid_1's l1: 0.630082\n",
      "[40000]\ttraining's l1: 0.0525376\tvalid_1's l1: 0.62998\n",
      "[40500]\ttraining's l1: 0.0512567\tvalid_1's l1: 0.629878\n",
      "[41000]\ttraining's l1: 0.0500117\tvalid_1's l1: 0.629793\n",
      "[41500]\ttraining's l1: 0.0487908\tvalid_1's l1: 0.629707\n",
      "[42000]\ttraining's l1: 0.04761\tvalid_1's l1: 0.629639\n",
      "[42500]\ttraining's l1: 0.046454\tvalid_1's l1: 0.62956\n",
      "[43000]\ttraining's l1: 0.0453263\tvalid_1's l1: 0.62949\n",
      "[43500]\ttraining's l1: 0.0442446\tvalid_1's l1: 0.629413\n",
      "[44000]\ttraining's l1: 0.0431814\tvalid_1's l1: 0.629338\n",
      "[44500]\ttraining's l1: 0.0421558\tvalid_1's l1: 0.629269\n",
      "[45000]\ttraining's l1: 0.0411534\tvalid_1's l1: 0.629203\n",
      "[45500]\ttraining's l1: 0.0401723\tvalid_1's l1: 0.629142\n",
      "[46000]\ttraining's l1: 0.0392106\tvalid_1's l1: 0.629079\n",
      "[46500]\ttraining's l1: 0.0382865\tvalid_1's l1: 0.629009\n",
      "[47000]\ttraining's l1: 0.0373823\tvalid_1's l1: 0.628954\n",
      "[47500]\ttraining's l1: 0.0364952\tvalid_1's l1: 0.628898\n",
      "[48000]\ttraining's l1: 0.0356374\tvalid_1's l1: 0.628847\n",
      "[48500]\ttraining's l1: 0.0348122\tvalid_1's l1: 0.628801\n",
      "[49000]\ttraining's l1: 0.0339998\tvalid_1's l1: 0.628753\n",
      "[49500]\ttraining's l1: 0.0332018\tvalid_1's l1: 0.628697\n",
      "[50000]\ttraining's l1: 0.0324227\tvalid_1's l1: 0.628643\n",
      "[50500]\ttraining's l1: 0.0316669\tvalid_1's l1: 0.628604\n",
      "[51000]\ttraining's l1: 0.0309438\tvalid_1's l1: 0.628567\n",
      "[51500]\ttraining's l1: 0.0302362\tvalid_1's l1: 0.628534\n",
      "[52000]\ttraining's l1: 0.0295362\tvalid_1's l1: 0.628501\n",
      "[52500]\ttraining's l1: 0.0288591\tvalid_1's l1: 0.628458\n",
      "[53000]\ttraining's l1: 0.0281994\tvalid_1's l1: 0.628419\n",
      "[53500]\ttraining's l1: 0.0275459\tvalid_1's l1: 0.628366\n",
      "[54000]\ttraining's l1: 0.0269166\tvalid_1's l1: 0.628328\n",
      "[54500]\ttraining's l1: 0.0263048\tvalid_1's l1: 0.62829\n",
      "[55000]\ttraining's l1: 0.0257104\tvalid_1's l1: 0.628259\n",
      "[55500]\ttraining's l1: 0.0251331\tvalid_1's l1: 0.62823\n",
      "[56000]\ttraining's l1: 0.0245688\tvalid_1's l1: 0.628201\n",
      "[56500]\ttraining's l1: 0.0240284\tvalid_1's l1: 0.628176\n",
      "[57000]\ttraining's l1: 0.0234908\tvalid_1's l1: 0.628154\n",
      "[57500]\ttraining's l1: 0.0229644\tvalid_1's l1: 0.628128\n",
      "[58000]\ttraining's l1: 0.0224577\tvalid_1's l1: 0.628094\n",
      "[58500]\ttraining's l1: 0.021959\tvalid_1's l1: 0.628065\n",
      "[59000]\ttraining's l1: 0.021478\tvalid_1's l1: 0.62804\n",
      "[59500]\ttraining's l1: 0.0210063\tvalid_1's l1: 0.62802\n",
      "[60000]\ttraining's l1: 0.0205447\tvalid_1's l1: 0.627994\n",
      "[60500]\ttraining's l1: 0.0200871\tvalid_1's l1: 0.627968\n",
      "[61000]\ttraining's l1: 0.0196484\tvalid_1's l1: 0.627949\n",
      "[61500]\ttraining's l1: 0.0192149\tvalid_1's l1: 0.627929\n",
      "[62000]\ttraining's l1: 0.0187986\tvalid_1's l1: 0.627907\n",
      "[62500]\ttraining's l1: 0.0183915\tvalid_1's l1: 0.627885\n",
      "[63000]\ttraining's l1: 0.0179981\tvalid_1's l1: 0.627859\n",
      "[63500]\ttraining's l1: 0.0176047\tvalid_1's l1: 0.627845\n",
      "[64000]\ttraining's l1: 0.0172236\tvalid_1's l1: 0.627825\n",
      "[64500]\ttraining's l1: 0.0168543\tvalid_1's l1: 0.627805\n",
      "[65000]\ttraining's l1: 0.016489\tvalid_1's l1: 0.62779\n",
      "[65500]\ttraining's l1: 0.0161408\tvalid_1's l1: 0.627775\n",
      "[66000]\ttraining's l1: 0.0157949\tvalid_1's l1: 0.627757\n",
      "[66500]\ttraining's l1: 0.015461\tvalid_1's l1: 0.627738\n",
      "[67000]\ttraining's l1: 0.0151348\tvalid_1's l1: 0.627721\n",
      "[67500]\ttraining's l1: 0.0148177\tvalid_1's l1: 0.627706\n",
      "[68000]\ttraining's l1: 0.0145089\tvalid_1's l1: 0.627689\n",
      "[68500]\ttraining's l1: 0.0142036\tvalid_1's l1: 0.627675\n",
      "[69000]\ttraining's l1: 0.0139061\tvalid_1's l1: 0.627662\n",
      "[69500]\ttraining's l1: 0.0136173\tvalid_1's l1: 0.627647\n",
      "[70000]\ttraining's l1: 0.0133333\tvalid_1's l1: 0.627631\n",
      "[70500]\ttraining's l1: 0.0130556\tvalid_1's l1: 0.627616\n",
      "[71000]\ttraining's l1: 0.0127894\tvalid_1's l1: 0.627604\n",
      "[71500]\ttraining's l1: 0.0125291\tvalid_1's l1: 0.62759\n",
      "[72000]\ttraining's l1: 0.0122723\tvalid_1's l1: 0.627573\n",
      "[72500]\ttraining's l1: 0.0120206\tvalid_1's l1: 0.627558\n",
      "[73000]\ttraining's l1: 0.0117758\tvalid_1's l1: 0.627547\n",
      "[73500]\ttraining's l1: 0.0115375\tvalid_1's l1: 0.627531\n",
      "[74000]\ttraining's l1: 0.0113048\tvalid_1's l1: 0.627517\n",
      "[74500]\ttraining's l1: 0.0110761\tvalid_1's l1: 0.627504\n",
      "[75000]\ttraining's l1: 0.0108564\tvalid_1's l1: 0.627494\n",
      "[75500]\ttraining's l1: 0.0106418\tvalid_1's l1: 0.627485\n",
      "[76000]\ttraining's l1: 0.0104289\tvalid_1's l1: 0.627474\n",
      "[76500]\ttraining's l1: 0.0102208\tvalid_1's l1: 0.627465\n",
      "[77000]\ttraining's l1: 0.0100199\tvalid_1's l1: 0.627458\n",
      "[77500]\ttraining's l1: 0.0098251\tvalid_1's l1: 0.627447\n",
      "[78000]\ttraining's l1: 0.00962935\tvalid_1's l1: 0.627439\n",
      "[78500]\ttraining's l1: 0.00944323\tvalid_1's l1: 0.627431\n",
      "[79000]\ttraining's l1: 0.00926085\tvalid_1's l1: 0.627424\n",
      "[79500]\ttraining's l1: 0.00907909\tvalid_1's l1: 0.627417\n",
      "[80000]\ttraining's l1: 0.00890432\tvalid_1's l1: 0.627412\n",
      "[80500]\ttraining's l1: 0.00873537\tvalid_1's l1: 0.627405\n",
      "[81000]\ttraining's l1: 0.00856896\tvalid_1's l1: 0.627397\n",
      "[81500]\ttraining's l1: 0.00840629\tvalid_1's l1: 0.62739\n",
      "[82000]\ttraining's l1: 0.00824786\tvalid_1's l1: 0.627384\n",
      "[82500]\ttraining's l1: 0.00809119\tvalid_1's l1: 0.627373\n",
      "[83000]\ttraining's l1: 0.00793915\tvalid_1's l1: 0.627366\n",
      "[83500]\ttraining's l1: 0.00779149\tvalid_1's l1: 0.62736\n",
      "[84000]\ttraining's l1: 0.00764524\tvalid_1's l1: 0.627354\n",
      "[84500]\ttraining's l1: 0.00750413\tvalid_1's l1: 0.627347\n",
      "[85000]\ttraining's l1: 0.00736728\tvalid_1's l1: 0.62734\n",
      "[85500]\ttraining's l1: 0.00723178\tvalid_1's l1: 0.627334\n",
      "[86000]\ttraining's l1: 0.00710122\tvalid_1's l1: 0.627328\n",
      "[86500]\ttraining's l1: 0.00697159\tvalid_1's l1: 0.627324\n",
      "[87000]\ttraining's l1: 0.00684509\tvalid_1's l1: 0.627318\n",
      "[87500]\ttraining's l1: 0.00672326\tvalid_1's l1: 0.627312\n",
      "[88000]\ttraining's l1: 0.00660139\tvalid_1's l1: 0.627309\n",
      "[88500]\ttraining's l1: 0.00648437\tvalid_1's l1: 0.627303\n",
      "[89000]\ttraining's l1: 0.00636912\tvalid_1's l1: 0.627298\n",
      "[89500]\ttraining's l1: 0.00625716\tvalid_1's l1: 0.627293\n",
      "[90000]\ttraining's l1: 0.00614819\tvalid_1's l1: 0.627288\n",
      "[90500]\ttraining's l1: 0.00604145\tvalid_1's l1: 0.627283\n",
      "[91000]\ttraining's l1: 0.00593728\tvalid_1's l1: 0.627278\n",
      "[91500]\ttraining's l1: 0.00583433\tvalid_1's l1: 0.627274\n",
      "[92000]\ttraining's l1: 0.00573355\tvalid_1's l1: 0.627269\n",
      "[92500]\ttraining's l1: 0.00563595\tvalid_1's l1: 0.627264\n",
      "[93000]\ttraining's l1: 0.00554041\tvalid_1's l1: 0.627261\n",
      "[93500]\ttraining's l1: 0.00544663\tvalid_1's l1: 0.627259\n",
      "[94000]\ttraining's l1: 0.00535547\tvalid_1's l1: 0.627254\n",
      "[94500]\ttraining's l1: 0.00526609\tvalid_1's l1: 0.627249\n",
      "[95000]\ttraining's l1: 0.00517828\tvalid_1's l1: 0.627245\n",
      "[95500]\ttraining's l1: 0.00509316\tvalid_1's l1: 0.627241\n",
      "[96000]\ttraining's l1: 0.00500928\tvalid_1's l1: 0.627237\n",
      "[96500]\ttraining's l1: 0.00492802\tvalid_1's l1: 0.627233\n",
      "[97000]\ttraining's l1: 0.00484816\tvalid_1's l1: 0.627229\n",
      "[97500]\ttraining's l1: 0.00477011\tvalid_1's l1: 0.627225\n",
      "[98000]\ttraining's l1: 0.00469387\tvalid_1's l1: 0.627221\n",
      "[98500]\ttraining's l1: 0.00462008\tvalid_1's l1: 0.627218\n",
      "[99000]\ttraining's l1: 0.00454631\tvalid_1's l1: 0.627214\n",
      "[99500]\ttraining's l1: 0.00447416\tvalid_1's l1: 0.627211\n",
      "[100000]\ttraining's l1: 0.00440475\tvalid_1's l1: 0.627207\n",
      "[100500]\ttraining's l1: 0.00433594\tvalid_1's l1: 0.627204\n",
      "[101000]\ttraining's l1: 0.00426847\tvalid_1's l1: 0.627201\n",
      "[101500]\ttraining's l1: 0.00420181\tvalid_1's l1: 0.627198\n",
      "[102000]\ttraining's l1: 0.00413678\tvalid_1's l1: 0.627195\n",
      "Early stopping, best iteration is:\n",
      "[101857]\ttraining's l1: 0.00415467\tvalid_1's l1: 0.627195\n",
      "CV mean score: -0.4684, std: 0.0021.\n",
      "Training of type 6\n",
      "Fold 1 started at Mon Jun 24 21:07:07 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.445731\tvalid_1's l1: 0.492519\n",
      "[1000]\ttraining's l1: 0.374045\tvalid_1's l1: 0.450227\n",
      "[1500]\ttraining's l1: 0.32999\tvalid_1's l1: 0.430318\n",
      "[2000]\ttraining's l1: 0.297034\tvalid_1's l1: 0.418036\n",
      "[2500]\ttraining's l1: 0.270313\tvalid_1's l1: 0.409063\n",
      "[3000]\ttraining's l1: 0.248074\tvalid_1's l1: 0.402853\n",
      "[3500]\ttraining's l1: 0.229063\tvalid_1's l1: 0.397913\n",
      "[4000]\ttraining's l1: 0.212541\tvalid_1's l1: 0.394276\n",
      "[4500]\ttraining's l1: 0.19781\tvalid_1's l1: 0.390829\n",
      "[5000]\ttraining's l1: 0.184548\tvalid_1's l1: 0.38815\n",
      "[5500]\ttraining's l1: 0.172679\tvalid_1's l1: 0.385994\n",
      "[6000]\ttraining's l1: 0.161947\tvalid_1's l1: 0.384155\n",
      "[6500]\ttraining's l1: 0.152239\tvalid_1's l1: 0.382452\n",
      "[7000]\ttraining's l1: 0.143322\tvalid_1's l1: 0.38105\n",
      "[7500]\ttraining's l1: 0.135063\tvalid_1's l1: 0.379659\n",
      "[8000]\ttraining's l1: 0.12745\tvalid_1's l1: 0.378504\n",
      "[8500]\ttraining's l1: 0.120454\tvalid_1's l1: 0.377604\n",
      "[9000]\ttraining's l1: 0.113948\tvalid_1's l1: 0.376768\n",
      "[9500]\ttraining's l1: 0.107819\tvalid_1's l1: 0.375914\n",
      "[10000]\ttraining's l1: 0.102107\tvalid_1's l1: 0.375137\n",
      "[10500]\ttraining's l1: 0.096785\tvalid_1's l1: 0.374458\n",
      "[11000]\ttraining's l1: 0.0918218\tvalid_1's l1: 0.373895\n",
      "[11500]\ttraining's l1: 0.0871677\tvalid_1's l1: 0.373325\n",
      "[12000]\ttraining's l1: 0.0827745\tvalid_1's l1: 0.37277\n",
      "[12500]\ttraining's l1: 0.0786989\tvalid_1's l1: 0.372277\n",
      "[13000]\ttraining's l1: 0.0748669\tvalid_1's l1: 0.371844\n",
      "[13500]\ttraining's l1: 0.0712883\tvalid_1's l1: 0.371469\n",
      "[14000]\ttraining's l1: 0.0678982\tvalid_1's l1: 0.371075\n",
      "[14500]\ttraining's l1: 0.0647008\tvalid_1's l1: 0.370821\n",
      "[15000]\ttraining's l1: 0.0616842\tvalid_1's l1: 0.370529\n",
      "[15500]\ttraining's l1: 0.0588244\tvalid_1's l1: 0.370253\n",
      "[16000]\ttraining's l1: 0.0561206\tvalid_1's l1: 0.37001\n",
      "[16500]\ttraining's l1: 0.0535524\tvalid_1's l1: 0.369807\n",
      "[17000]\ttraining's l1: 0.0511295\tvalid_1's l1: 0.36962\n",
      "[17500]\ttraining's l1: 0.0488419\tvalid_1's l1: 0.369409\n",
      "[18000]\ttraining's l1: 0.0466814\tvalid_1's l1: 0.369224\n",
      "[18500]\ttraining's l1: 0.0446341\tvalid_1's l1: 0.369065\n",
      "[19000]\ttraining's l1: 0.0426868\tvalid_1's l1: 0.368905\n",
      "[19500]\ttraining's l1: 0.040818\tvalid_1's l1: 0.368781\n",
      "[20000]\ttraining's l1: 0.0390695\tvalid_1's l1: 0.368642\n",
      "[20500]\ttraining's l1: 0.0374091\tvalid_1's l1: 0.368487\n",
      "[21000]\ttraining's l1: 0.0358066\tvalid_1's l1: 0.368381\n",
      "[21500]\ttraining's l1: 0.0342889\tvalid_1's l1: 0.368291\n",
      "[22000]\ttraining's l1: 0.0328608\tvalid_1's l1: 0.368218\n",
      "[22500]\ttraining's l1: 0.0314941\tvalid_1's l1: 0.368094\n",
      "[23000]\ttraining's l1: 0.0301948\tvalid_1's l1: 0.368006\n",
      "[23500]\ttraining's l1: 0.0289449\tvalid_1's l1: 0.36792\n",
      "[24000]\ttraining's l1: 0.0277582\tvalid_1's l1: 0.367848\n",
      "[24500]\ttraining's l1: 0.0266277\tvalid_1's l1: 0.367774\n",
      "[25000]\ttraining's l1: 0.0255581\tvalid_1's l1: 0.367708\n",
      "[25500]\ttraining's l1: 0.0245371\tvalid_1's l1: 0.367638\n",
      "[26000]\ttraining's l1: 0.023565\tvalid_1's l1: 0.367578\n",
      "[26500]\ttraining's l1: 0.0226273\tvalid_1's l1: 0.36752\n",
      "[27000]\ttraining's l1: 0.0217303\tvalid_1's l1: 0.367461\n",
      "[27500]\ttraining's l1: 0.0208744\tvalid_1's l1: 0.367402\n",
      "[28000]\ttraining's l1: 0.0200645\tvalid_1's l1: 0.367351\n",
      "[28500]\ttraining's l1: 0.0192978\tvalid_1's l1: 0.367302\n",
      "[29000]\ttraining's l1: 0.0185668\tvalid_1's l1: 0.367261\n",
      "[29500]\ttraining's l1: 0.0178552\tvalid_1's l1: 0.367223\n",
      "[30000]\ttraining's l1: 0.0171668\tvalid_1's l1: 0.367182\n",
      "[30500]\ttraining's l1: 0.0165188\tvalid_1's l1: 0.367142\n",
      "[31000]\ttraining's l1: 0.015905\tvalid_1's l1: 0.367118\n",
      "[31500]\ttraining's l1: 0.0153179\tvalid_1's l1: 0.367087\n",
      "[32000]\ttraining's l1: 0.0147576\tvalid_1's l1: 0.367052\n",
      "[32500]\ttraining's l1: 0.0142181\tvalid_1's l1: 0.367029\n",
      "[33000]\ttraining's l1: 0.013702\tvalid_1's l1: 0.367003\n",
      "[33500]\ttraining's l1: 0.0132082\tvalid_1's l1: 0.366978\n",
      "[34000]\ttraining's l1: 0.0127368\tvalid_1's l1: 0.366953\n",
      "[34500]\ttraining's l1: 0.0122854\tvalid_1's l1: 0.366929\n",
      "[35000]\ttraining's l1: 0.0118541\tvalid_1's l1: 0.366909\n",
      "[35500]\ttraining's l1: 0.0114347\tvalid_1's l1: 0.36689\n",
      "[36000]\ttraining's l1: 0.0110332\tvalid_1's l1: 0.366865\n",
      "[36500]\ttraining's l1: 0.0106518\tvalid_1's l1: 0.366844\n",
      "[37000]\ttraining's l1: 0.0102828\tvalid_1's l1: 0.366823\n",
      "[37500]\ttraining's l1: 0.00993144\tvalid_1's l1: 0.366807\n",
      "[38000]\ttraining's l1: 0.00959646\tvalid_1's l1: 0.366787\n",
      "[38500]\ttraining's l1: 0.00926907\tvalid_1's l1: 0.366769\n",
      "[39000]\ttraining's l1: 0.00896002\tvalid_1's l1: 0.366758\n",
      "[39500]\ttraining's l1: 0.00866889\tvalid_1's l1: 0.366747\n",
      "[40000]\ttraining's l1: 0.00838767\tvalid_1's l1: 0.366732\n",
      "[40500]\ttraining's l1: 0.00811498\tvalid_1's l1: 0.366719\n",
      "[41000]\ttraining's l1: 0.00785928\tvalid_1's l1: 0.366705\n",
      "[41500]\ttraining's l1: 0.00760931\tvalid_1's l1: 0.366694\n",
      "[42000]\ttraining's l1: 0.00737102\tvalid_1's l1: 0.366684\n",
      "[42500]\ttraining's l1: 0.00714144\tvalid_1's l1: 0.366674\n",
      "[43000]\ttraining's l1: 0.00692401\tvalid_1's l1: 0.366659\n",
      "[43500]\ttraining's l1: 0.00671354\tvalid_1's l1: 0.366652\n",
      "[44000]\ttraining's l1: 0.00651182\tvalid_1's l1: 0.366641\n",
      "[44500]\ttraining's l1: 0.00631486\tvalid_1's l1: 0.366637\n",
      "[45000]\ttraining's l1: 0.00612823\tvalid_1's l1: 0.366629\n",
      "[45500]\ttraining's l1: 0.00594918\tvalid_1's l1: 0.366624\n",
      "[46000]\ttraining's l1: 0.00577582\tvalid_1's l1: 0.366612\n",
      "[46500]\ttraining's l1: 0.00560971\tvalid_1's l1: 0.366604\n",
      "[47000]\ttraining's l1: 0.00545114\tvalid_1's l1: 0.366594\n",
      "[47500]\ttraining's l1: 0.00529737\tvalid_1's l1: 0.366586\n",
      "[48000]\ttraining's l1: 0.0051506\tvalid_1's l1: 0.366584\n",
      "[48500]\ttraining's l1: 0.00501061\tvalid_1's l1: 0.366576\n",
      "[49000]\ttraining's l1: 0.00487535\tvalid_1's l1: 0.366568\n",
      "[49500]\ttraining's l1: 0.00474455\tvalid_1's l1: 0.366566\n",
      "Early stopping, best iteration is:\n",
      "[49371]\ttraining's l1: 0.00477755\tvalid_1's l1: 0.366565\n",
      "Fold 2 started at Mon Jun 24 21:28:30 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.445334\tvalid_1's l1: 0.491061\n",
      "[1000]\ttraining's l1: 0.373752\tvalid_1's l1: 0.449156\n",
      "[1500]\ttraining's l1: 0.330227\tvalid_1's l1: 0.429914\n",
      "[2000]\ttraining's l1: 0.297194\tvalid_1's l1: 0.41745\n",
      "[2500]\ttraining's l1: 0.270653\tvalid_1's l1: 0.408417\n",
      "[3000]\ttraining's l1: 0.2486\tvalid_1's l1: 0.402117\n",
      "[3500]\ttraining's l1: 0.229625\tvalid_1's l1: 0.397015\n",
      "[4000]\ttraining's l1: 0.212927\tvalid_1's l1: 0.393067\n",
      "[4500]\ttraining's l1: 0.198081\tvalid_1's l1: 0.389883\n",
      "[5000]\ttraining's l1: 0.184845\tvalid_1's l1: 0.387279\n",
      "[5500]\ttraining's l1: 0.172968\tvalid_1's l1: 0.385135\n",
      "[6000]\ttraining's l1: 0.162188\tvalid_1's l1: 0.383274\n",
      "[6500]\ttraining's l1: 0.152428\tvalid_1's l1: 0.381658\n",
      "[7000]\ttraining's l1: 0.143351\tvalid_1's l1: 0.380224\n",
      "[7500]\ttraining's l1: 0.135021\tvalid_1's l1: 0.378911\n",
      "[8000]\ttraining's l1: 0.12734\tvalid_1's l1: 0.377801\n",
      "[8500]\ttraining's l1: 0.12025\tvalid_1's l1: 0.376856\n",
      "[9000]\ttraining's l1: 0.113667\tvalid_1's l1: 0.375931\n",
      "[9500]\ttraining's l1: 0.107632\tvalid_1's l1: 0.37518\n",
      "[10000]\ttraining's l1: 0.102018\tvalid_1's l1: 0.374399\n",
      "[10500]\ttraining's l1: 0.0967661\tvalid_1's l1: 0.373813\n",
      "[11000]\ttraining's l1: 0.091755\tvalid_1's l1: 0.373179\n",
      "[11500]\ttraining's l1: 0.0870924\tvalid_1's l1: 0.372742\n",
      "[12000]\ttraining's l1: 0.0827472\tvalid_1's l1: 0.372237\n",
      "[12500]\ttraining's l1: 0.0786727\tvalid_1's l1: 0.37184\n",
      "[13000]\ttraining's l1: 0.0748203\tvalid_1's l1: 0.371431\n",
      "[13500]\ttraining's l1: 0.0712007\tvalid_1's l1: 0.370993\n",
      "[14000]\ttraining's l1: 0.0678177\tvalid_1's l1: 0.370632\n",
      "[14500]\ttraining's l1: 0.0646174\tvalid_1's l1: 0.37032\n",
      "[15000]\ttraining's l1: 0.0616188\tvalid_1's l1: 0.370002\n",
      "[15500]\ttraining's l1: 0.0587992\tvalid_1's l1: 0.369757\n",
      "[16000]\ttraining's l1: 0.0561081\tvalid_1's l1: 0.369512\n",
      "[16500]\ttraining's l1: 0.053574\tvalid_1's l1: 0.369323\n",
      "[17000]\ttraining's l1: 0.0511512\tvalid_1's l1: 0.369111\n",
      "[17500]\ttraining's l1: 0.048875\tvalid_1's l1: 0.368936\n",
      "[18000]\ttraining's l1: 0.0467138\tvalid_1's l1: 0.368706\n",
      "[18500]\ttraining's l1: 0.0446352\tvalid_1's l1: 0.368546\n",
      "[19000]\ttraining's l1: 0.0426863\tvalid_1's l1: 0.368447\n",
      "[19500]\ttraining's l1: 0.0408282\tvalid_1's l1: 0.368279\n",
      "[20000]\ttraining's l1: 0.0390727\tvalid_1's l1: 0.368143\n",
      "[20500]\ttraining's l1: 0.0374088\tvalid_1's l1: 0.368012\n",
      "[21000]\ttraining's l1: 0.0358007\tvalid_1's l1: 0.367897\n",
      "[21500]\ttraining's l1: 0.0342892\tvalid_1's l1: 0.367768\n",
      "[22000]\ttraining's l1: 0.0328384\tvalid_1's l1: 0.367672\n",
      "[22500]\ttraining's l1: 0.031477\tvalid_1's l1: 0.367565\n",
      "[23000]\ttraining's l1: 0.0301569\tvalid_1's l1: 0.367467\n",
      "[23500]\ttraining's l1: 0.0289116\tvalid_1's l1: 0.367383\n",
      "[24000]\ttraining's l1: 0.0277405\tvalid_1's l1: 0.367294\n",
      "[24500]\ttraining's l1: 0.0266236\tvalid_1's l1: 0.367232\n",
      "[25000]\ttraining's l1: 0.0255461\tvalid_1's l1: 0.367155\n",
      "[25500]\ttraining's l1: 0.0245352\tvalid_1's l1: 0.367075\n",
      "[26000]\ttraining's l1: 0.0235481\tvalid_1's l1: 0.367\n",
      "[26500]\ttraining's l1: 0.0226162\tvalid_1's l1: 0.366927\n",
      "[27000]\ttraining's l1: 0.0217387\tvalid_1's l1: 0.366872\n",
      "[27500]\ttraining's l1: 0.0208944\tvalid_1's l1: 0.366824\n",
      "[28000]\ttraining's l1: 0.0200857\tvalid_1's l1: 0.366771\n",
      "[28500]\ttraining's l1: 0.0193163\tvalid_1's l1: 0.366729\n",
      "[29000]\ttraining's l1: 0.0185744\tvalid_1's l1: 0.366685\n",
      "[29500]\ttraining's l1: 0.0178706\tvalid_1's l1: 0.366637\n",
      "[30000]\ttraining's l1: 0.0171995\tvalid_1's l1: 0.366596\n",
      "[30500]\ttraining's l1: 0.0165474\tvalid_1's l1: 0.366559\n",
      "[31000]\ttraining's l1: 0.0159306\tvalid_1's l1: 0.36652\n",
      "[31500]\ttraining's l1: 0.0153409\tvalid_1's l1: 0.366488\n",
      "[32000]\ttraining's l1: 0.0147777\tvalid_1's l1: 0.366453\n",
      "[32500]\ttraining's l1: 0.0142391\tvalid_1's l1: 0.366419\n",
      "[33000]\ttraining's l1: 0.013722\tvalid_1's l1: 0.366392\n",
      "[33500]\ttraining's l1: 0.0132262\tvalid_1's l1: 0.36636\n",
      "[34000]\ttraining's l1: 0.0127562\tvalid_1's l1: 0.366327\n",
      "[34500]\ttraining's l1: 0.0123018\tvalid_1's l1: 0.366309\n",
      "[35000]\ttraining's l1: 0.0118637\tvalid_1's l1: 0.366293\n",
      "[35500]\ttraining's l1: 0.011453\tvalid_1's l1: 0.366264\n",
      "[36000]\ttraining's l1: 0.0110551\tvalid_1's l1: 0.366235\n",
      "[36500]\ttraining's l1: 0.0106755\tvalid_1's l1: 0.366212\n",
      "[37000]\ttraining's l1: 0.0103084\tvalid_1's l1: 0.366196\n",
      "[37500]\ttraining's l1: 0.00996257\tvalid_1's l1: 0.366177\n",
      "[38000]\ttraining's l1: 0.00962701\tvalid_1's l1: 0.366162\n",
      "[38500]\ttraining's l1: 0.00930368\tvalid_1's l1: 0.366151\n",
      "[39000]\ttraining's l1: 0.00899402\tvalid_1's l1: 0.366137\n",
      "[39500]\ttraining's l1: 0.00869552\tvalid_1's l1: 0.366123\n",
      "[40000]\ttraining's l1: 0.00841623\tvalid_1's l1: 0.366108\n",
      "[40500]\ttraining's l1: 0.00814533\tvalid_1's l1: 0.366094\n",
      "[41000]\ttraining's l1: 0.00788723\tvalid_1's l1: 0.366081\n",
      "[41500]\ttraining's l1: 0.00764088\tvalid_1's l1: 0.366074\n",
      "[42000]\ttraining's l1: 0.00740269\tvalid_1's l1: 0.36606\n",
      "[42500]\ttraining's l1: 0.00717038\tvalid_1's l1: 0.366048\n",
      "[43000]\ttraining's l1: 0.00694975\tvalid_1's l1: 0.366039\n",
      "[43500]\ttraining's l1: 0.00673984\tvalid_1's l1: 0.36603\n",
      "[44000]\ttraining's l1: 0.0065366\tvalid_1's l1: 0.366018\n",
      "[44500]\ttraining's l1: 0.0063407\tvalid_1's l1: 0.366013\n",
      "[45000]\ttraining's l1: 0.00615215\tvalid_1's l1: 0.366002\n",
      "[45500]\ttraining's l1: 0.00597266\tvalid_1's l1: 0.365997\n",
      "[46000]\ttraining's l1: 0.00580113\tvalid_1's l1: 0.365987\n",
      "[46500]\ttraining's l1: 0.00563442\tvalid_1's l1: 0.36598\n",
      "[47000]\ttraining's l1: 0.00547516\tvalid_1's l1: 0.365973\n",
      "[47500]\ttraining's l1: 0.00532341\tvalid_1's l1: 0.365966\n",
      "[48000]\ttraining's l1: 0.00517483\tvalid_1's l1: 0.365959\n",
      "[48500]\ttraining's l1: 0.00503321\tvalid_1's l1: 0.365952\n",
      "[49000]\ttraining's l1: 0.00489707\tvalid_1's l1: 0.365948\n",
      "[49500]\ttraining's l1: 0.00476554\tvalid_1's l1: 0.365943\n",
      "[50000]\ttraining's l1: 0.00464015\tvalid_1's l1: 0.365937\n",
      "[50500]\ttraining's l1: 0.00451967\tvalid_1's l1: 0.365931\n",
      "Early stopping, best iteration is:\n",
      "[50740]\ttraining's l1: 0.00446326\tvalid_1's l1: 0.365929\n",
      "Fold 3 started at Mon Jun 24 21:56:38 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.44743\tvalid_1's l1: 0.490043\n",
      "[1000]\ttraining's l1: 0.375348\tvalid_1's l1: 0.448021\n",
      "[1500]\ttraining's l1: 0.330722\tvalid_1's l1: 0.427959\n",
      "[2000]\ttraining's l1: 0.297988\tvalid_1's l1: 0.416112\n",
      "[2500]\ttraining's l1: 0.271159\tvalid_1's l1: 0.407781\n",
      "[3000]\ttraining's l1: 0.248961\tvalid_1's l1: 0.401708\n",
      "[3500]\ttraining's l1: 0.23021\tvalid_1's l1: 0.397014\n",
      "[4000]\ttraining's l1: 0.213597\tvalid_1's l1: 0.392889\n",
      "[4500]\ttraining's l1: 0.198731\tvalid_1's l1: 0.389521\n",
      "[5000]\ttraining's l1: 0.185509\tvalid_1's l1: 0.387197\n",
      "[5500]\ttraining's l1: 0.173593\tvalid_1's l1: 0.384941\n",
      "[6000]\ttraining's l1: 0.162872\tvalid_1's l1: 0.38295\n",
      "[6500]\ttraining's l1: 0.153023\tvalid_1's l1: 0.381263\n",
      "[7000]\ttraining's l1: 0.14395\tvalid_1's l1: 0.379702\n",
      "[7500]\ttraining's l1: 0.135703\tvalid_1's l1: 0.378581\n",
      "[8000]\ttraining's l1: 0.127974\tvalid_1's l1: 0.377508\n",
      "[8500]\ttraining's l1: 0.120876\tvalid_1's l1: 0.376507\n",
      "[9000]\ttraining's l1: 0.114316\tvalid_1's l1: 0.375664\n",
      "[9500]\ttraining's l1: 0.108225\tvalid_1's l1: 0.374842\n",
      "[10000]\ttraining's l1: 0.102536\tvalid_1's l1: 0.374193\n",
      "[10500]\ttraining's l1: 0.0972646\tvalid_1's l1: 0.373454\n",
      "[11000]\ttraining's l1: 0.0922911\tvalid_1's l1: 0.372931\n",
      "[11500]\ttraining's l1: 0.0876998\tvalid_1's l1: 0.372412\n",
      "[12000]\ttraining's l1: 0.0833493\tvalid_1's l1: 0.371817\n",
      "[12500]\ttraining's l1: 0.0792054\tvalid_1's l1: 0.371362\n",
      "[13000]\ttraining's l1: 0.0753617\tvalid_1's l1: 0.370923\n",
      "[13500]\ttraining's l1: 0.071732\tvalid_1's l1: 0.370559\n",
      "[14000]\ttraining's l1: 0.0683619\tvalid_1's l1: 0.370217\n",
      "[14500]\ttraining's l1: 0.0651209\tvalid_1's l1: 0.369885\n",
      "[15000]\ttraining's l1: 0.062062\tvalid_1's l1: 0.369597\n",
      "[15500]\ttraining's l1: 0.0591893\tvalid_1's l1: 0.369296\n",
      "[16000]\ttraining's l1: 0.0564538\tvalid_1's l1: 0.369073\n",
      "[16500]\ttraining's l1: 0.0538795\tvalid_1's l1: 0.368814\n",
      "[17000]\ttraining's l1: 0.0514734\tvalid_1's l1: 0.368666\n",
      "[17500]\ttraining's l1: 0.0491626\tvalid_1's l1: 0.368467\n",
      "[18000]\ttraining's l1: 0.0469702\tvalid_1's l1: 0.368296\n",
      "[18500]\ttraining's l1: 0.0449065\tvalid_1's l1: 0.368113\n",
      "[19000]\ttraining's l1: 0.0429418\tvalid_1's l1: 0.367982\n",
      "[19500]\ttraining's l1: 0.0410941\tvalid_1's l1: 0.367825\n",
      "[20000]\ttraining's l1: 0.0393004\tvalid_1's l1: 0.367671\n",
      "[20500]\ttraining's l1: 0.0376349\tvalid_1's l1: 0.367557\n",
      "[21000]\ttraining's l1: 0.0360401\tvalid_1's l1: 0.367464\n",
      "[21500]\ttraining's l1: 0.0345115\tvalid_1's l1: 0.367351\n",
      "[22000]\ttraining's l1: 0.0330588\tvalid_1's l1: 0.367244\n",
      "[22500]\ttraining's l1: 0.0316666\tvalid_1's l1: 0.367142\n",
      "[23000]\ttraining's l1: 0.0303583\tvalid_1's l1: 0.367052\n",
      "[23500]\ttraining's l1: 0.0290981\tvalid_1's l1: 0.366954\n",
      "[24000]\ttraining's l1: 0.0279285\tvalid_1's l1: 0.366874\n",
      "[24500]\ttraining's l1: 0.0267984\tvalid_1's l1: 0.366795\n",
      "[25000]\ttraining's l1: 0.0257241\tvalid_1's l1: 0.36672\n",
      "[25500]\ttraining's l1: 0.0246863\tvalid_1's l1: 0.366652\n",
      "[26000]\ttraining's l1: 0.0237131\tvalid_1's l1: 0.366589\n",
      "[26500]\ttraining's l1: 0.0227852\tvalid_1's l1: 0.366518\n",
      "[27000]\ttraining's l1: 0.0218978\tvalid_1's l1: 0.366469\n",
      "[27500]\ttraining's l1: 0.021047\tvalid_1's l1: 0.36641\n",
      "[28000]\ttraining's l1: 0.0202181\tvalid_1's l1: 0.366363\n",
      "[28500]\ttraining's l1: 0.019438\tvalid_1's l1: 0.366315\n",
      "[29000]\ttraining's l1: 0.0186992\tvalid_1's l1: 0.366262\n",
      "[29500]\ttraining's l1: 0.0179843\tvalid_1's l1: 0.366234\n",
      "[30000]\ttraining's l1: 0.017307\tvalid_1's l1: 0.366185\n",
      "[30500]\ttraining's l1: 0.0166599\tvalid_1's l1: 0.366133\n",
      "[31000]\ttraining's l1: 0.016043\tvalid_1's l1: 0.366107\n",
      "[31500]\ttraining's l1: 0.0154536\tvalid_1's l1: 0.366073\n",
      "[32000]\ttraining's l1: 0.0148883\tvalid_1's l1: 0.366049\n",
      "[32500]\ttraining's l1: 0.0143436\tvalid_1's l1: 0.366018\n",
      "[33000]\ttraining's l1: 0.0138204\tvalid_1's l1: 0.36599\n",
      "[33500]\ttraining's l1: 0.0133275\tvalid_1's l1: 0.365957\n",
      "[34000]\ttraining's l1: 0.0128509\tvalid_1's l1: 0.365942\n",
      "[34500]\ttraining's l1: 0.0123869\tvalid_1's l1: 0.365918\n",
      "[35000]\ttraining's l1: 0.0119489\tvalid_1's l1: 0.365892\n",
      "[35500]\ttraining's l1: 0.0115301\tvalid_1's l1: 0.365874\n",
      "[36000]\ttraining's l1: 0.0111347\tvalid_1's l1: 0.365857\n",
      "[36500]\ttraining's l1: 0.0107487\tvalid_1's l1: 0.36584\n",
      "[37000]\ttraining's l1: 0.0103784\tvalid_1's l1: 0.365822\n",
      "[37500]\ttraining's l1: 0.0100279\tvalid_1's l1: 0.3658\n",
      "[38000]\ttraining's l1: 0.00968836\tvalid_1's l1: 0.365777\n",
      "[38500]\ttraining's l1: 0.0093648\tvalid_1's l1: 0.365756\n",
      "[39000]\ttraining's l1: 0.0090568\tvalid_1's l1: 0.365744\n",
      "[39500]\ttraining's l1: 0.00875761\tvalid_1's l1: 0.365725\n",
      "[40000]\ttraining's l1: 0.00846791\tvalid_1's l1: 0.365714\n",
      "[40500]\ttraining's l1: 0.00819839\tvalid_1's l1: 0.365704\n",
      "[41000]\ttraining's l1: 0.00793795\tvalid_1's l1: 0.365688\n",
      "[41500]\ttraining's l1: 0.0076819\tvalid_1's l1: 0.365674\n",
      "[42000]\ttraining's l1: 0.00744215\tvalid_1's l1: 0.36566\n",
      "[42500]\ttraining's l1: 0.00721169\tvalid_1's l1: 0.365644\n",
      "[43000]\ttraining's l1: 0.00698892\tvalid_1's l1: 0.365634\n",
      "[43500]\ttraining's l1: 0.00677535\tvalid_1's l1: 0.365625\n",
      "[44000]\ttraining's l1: 0.00656721\tvalid_1's l1: 0.365618\n",
      "[44500]\ttraining's l1: 0.00637044\tvalid_1's l1: 0.365606\n",
      "[45000]\ttraining's l1: 0.00618183\tvalid_1's l1: 0.365599\n",
      "[45500]\ttraining's l1: 0.00600181\tvalid_1's l1: 0.365588\n",
      "[46000]\ttraining's l1: 0.00582848\tvalid_1's l1: 0.365581\n",
      "[46500]\ttraining's l1: 0.00566273\tvalid_1's l1: 0.365576\n",
      "[47000]\ttraining's l1: 0.00550365\tvalid_1's l1: 0.365571\n",
      "[47500]\ttraining's l1: 0.00534937\tvalid_1's l1: 0.365565\n",
      "[48000]\ttraining's l1: 0.00520183\tvalid_1's l1: 0.365559\n",
      "[48500]\ttraining's l1: 0.00505821\tvalid_1's l1: 0.365548\n",
      "[49000]\ttraining's l1: 0.00492162\tvalid_1's l1: 0.365539\n",
      "[49500]\ttraining's l1: 0.00478876\tvalid_1's l1: 0.365533\n",
      "[50000]\ttraining's l1: 0.00466089\tvalid_1's l1: 0.365528\n",
      "[50500]\ttraining's l1: 0.0045384\tvalid_1's l1: 0.365523\n",
      "[51000]\ttraining's l1: 0.00442124\tvalid_1's l1: 0.365517\n",
      "[51500]\ttraining's l1: 0.00430836\tvalid_1's l1: 0.365512\n",
      "[52000]\ttraining's l1: 0.00419957\tvalid_1's l1: 0.36551\n",
      "[52500]\ttraining's l1: 0.0040941\tvalid_1's l1: 0.365504\n",
      "[53000]\ttraining's l1: 0.00399424\tvalid_1's l1: 0.365499\n",
      "[53500]\ttraining's l1: 0.00389795\tvalid_1's l1: 0.365494\n",
      "[54000]\ttraining's l1: 0.0038049\tvalid_1's l1: 0.36549\n",
      "[54500]\ttraining's l1: 0.00371526\tvalid_1's l1: 0.365484\n",
      "[55000]\ttraining's l1: 0.00362909\tvalid_1's l1: 0.365481\n",
      "[55500]\ttraining's l1: 0.0035453\tvalid_1's l1: 0.365476\n",
      "[56000]\ttraining's l1: 0.00346456\tvalid_1's l1: 0.365471\n",
      "[56500]\ttraining's l1: 0.00338705\tvalid_1's l1: 0.365468\n",
      "[57000]\ttraining's l1: 0.00331266\tvalid_1's l1: 0.365463\n",
      "[57500]\ttraining's l1: 0.00324025\tvalid_1's l1: 0.36546\n",
      "Early stopping, best iteration is:\n",
      "[57708]\ttraining's l1: 0.0032112\tvalid_1's l1: 0.365458\n",
      "Fold 4 started at Mon Jun 24 22:25:44 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.447067\tvalid_1's l1: 0.491274\n",
      "[1000]\ttraining's l1: 0.374162\tvalid_1's l1: 0.448604\n",
      "[1500]\ttraining's l1: 0.329905\tvalid_1's l1: 0.428115\n",
      "[2000]\ttraining's l1: 0.296588\tvalid_1's l1: 0.415409\n",
      "[2500]\ttraining's l1: 0.270346\tvalid_1's l1: 0.407219\n",
      "[3000]\ttraining's l1: 0.248189\tvalid_1's l1: 0.400746\n",
      "[3500]\ttraining's l1: 0.22903\tvalid_1's l1: 0.395895\n",
      "[4000]\ttraining's l1: 0.212354\tvalid_1's l1: 0.391991\n",
      "[4500]\ttraining's l1: 0.197657\tvalid_1's l1: 0.388484\n",
      "[5000]\ttraining's l1: 0.184425\tvalid_1's l1: 0.385767\n",
      "[5500]\ttraining's l1: 0.172657\tvalid_1's l1: 0.383432\n",
      "[6000]\ttraining's l1: 0.161871\tvalid_1's l1: 0.381578\n",
      "[6500]\ttraining's l1: 0.152007\tvalid_1's l1: 0.379899\n",
      "[7000]\ttraining's l1: 0.143067\tvalid_1's l1: 0.37844\n",
      "[7500]\ttraining's l1: 0.134881\tvalid_1's l1: 0.377103\n",
      "[8000]\ttraining's l1: 0.127114\tvalid_1's l1: 0.376054\n",
      "[8500]\ttraining's l1: 0.12002\tvalid_1's l1: 0.374984\n",
      "[9000]\ttraining's l1: 0.113523\tvalid_1's l1: 0.374125\n",
      "[9500]\ttraining's l1: 0.107442\tvalid_1's l1: 0.373294\n",
      "[10000]\ttraining's l1: 0.101794\tvalid_1's l1: 0.372586\n",
      "[10500]\ttraining's l1: 0.0965397\tvalid_1's l1: 0.371976\n",
      "[11000]\ttraining's l1: 0.0916007\tvalid_1's l1: 0.371402\n",
      "[11500]\ttraining's l1: 0.0869977\tvalid_1's l1: 0.370901\n",
      "[12000]\ttraining's l1: 0.0826353\tvalid_1's l1: 0.370448\n",
      "[12500]\ttraining's l1: 0.0786038\tvalid_1's l1: 0.369965\n",
      "[13000]\ttraining's l1: 0.0747974\tvalid_1's l1: 0.369588\n",
      "[13500]\ttraining's l1: 0.0712297\tvalid_1's l1: 0.369193\n",
      "[14000]\ttraining's l1: 0.0678389\tvalid_1's l1: 0.368901\n",
      "[14500]\ttraining's l1: 0.0646291\tvalid_1's l1: 0.368585\n",
      "[15000]\ttraining's l1: 0.0616263\tvalid_1's l1: 0.368386\n",
      "[15500]\ttraining's l1: 0.0587737\tvalid_1's l1: 0.368116\n",
      "[16000]\ttraining's l1: 0.0560667\tvalid_1's l1: 0.367836\n",
      "[16500]\ttraining's l1: 0.0535168\tvalid_1's l1: 0.367618\n",
      "[17000]\ttraining's l1: 0.0510716\tvalid_1's l1: 0.36734\n",
      "[17500]\ttraining's l1: 0.0487938\tvalid_1's l1: 0.367142\n",
      "[18000]\ttraining's l1: 0.0466259\tvalid_1's l1: 0.367003\n",
      "[18500]\ttraining's l1: 0.0445649\tvalid_1's l1: 0.366852\n",
      "[19000]\ttraining's l1: 0.0426361\tvalid_1's l1: 0.366683\n",
      "[19500]\ttraining's l1: 0.0407802\tvalid_1's l1: 0.366532\n",
      "[20000]\ttraining's l1: 0.039039\tvalid_1's l1: 0.366392\n",
      "[20500]\ttraining's l1: 0.0373658\tvalid_1's l1: 0.366295\n",
      "[21000]\ttraining's l1: 0.0357782\tvalid_1's l1: 0.366188\n",
      "[21500]\ttraining's l1: 0.034285\tvalid_1's l1: 0.36609\n",
      "[22000]\ttraining's l1: 0.0328265\tvalid_1's l1: 0.365979\n",
      "[22500]\ttraining's l1: 0.0314615\tvalid_1's l1: 0.365885\n",
      "[23000]\ttraining's l1: 0.0301583\tvalid_1's l1: 0.3658\n",
      "[23500]\ttraining's l1: 0.0289175\tvalid_1's l1: 0.365718\n",
      "[24000]\ttraining's l1: 0.0277423\tvalid_1's l1: 0.365625\n",
      "[24500]\ttraining's l1: 0.0266238\tvalid_1's l1: 0.365557\n",
      "[25000]\ttraining's l1: 0.0255483\tvalid_1's l1: 0.365475\n",
      "[25500]\ttraining's l1: 0.0245247\tvalid_1's l1: 0.365407\n",
      "[26000]\ttraining's l1: 0.0235542\tvalid_1's l1: 0.365358\n",
      "[26500]\ttraining's l1: 0.022629\tvalid_1's l1: 0.365309\n",
      "[27000]\ttraining's l1: 0.0217349\tvalid_1's l1: 0.365253\n",
      "[27500]\ttraining's l1: 0.0208897\tvalid_1's l1: 0.365192\n",
      "[28000]\ttraining's l1: 0.0200786\tvalid_1's l1: 0.365141\n",
      "[28500]\ttraining's l1: 0.0193141\tvalid_1's l1: 0.365091\n",
      "[29000]\ttraining's l1: 0.0185767\tvalid_1's l1: 0.365051\n",
      "[29500]\ttraining's l1: 0.0178711\tvalid_1's l1: 0.365024\n",
      "[30000]\ttraining's l1: 0.0171951\tvalid_1's l1: 0.364987\n",
      "[30500]\ttraining's l1: 0.0165555\tvalid_1's l1: 0.364945\n",
      "[31000]\ttraining's l1: 0.0159444\tvalid_1's l1: 0.364923\n",
      "[31500]\ttraining's l1: 0.0153554\tvalid_1's l1: 0.3649\n",
      "[32000]\ttraining's l1: 0.0147868\tvalid_1's l1: 0.364871\n",
      "[32500]\ttraining's l1: 0.0142477\tvalid_1's l1: 0.364843\n",
      "[33000]\ttraining's l1: 0.0137333\tvalid_1's l1: 0.364812\n",
      "[33500]\ttraining's l1: 0.0132294\tvalid_1's l1: 0.364796\n",
      "[34000]\ttraining's l1: 0.0127476\tvalid_1's l1: 0.364772\n",
      "[34500]\ttraining's l1: 0.0122941\tvalid_1's l1: 0.364749\n",
      "[35000]\ttraining's l1: 0.011859\tvalid_1's l1: 0.364736\n",
      "[35500]\ttraining's l1: 0.0114466\tvalid_1's l1: 0.364717\n",
      "[36000]\ttraining's l1: 0.0110501\tvalid_1's l1: 0.3647\n",
      "[36500]\ttraining's l1: 0.0106689\tvalid_1's l1: 0.364681\n",
      "[37000]\ttraining's l1: 0.0103043\tvalid_1's l1: 0.364671\n",
      "[37500]\ttraining's l1: 0.0099575\tvalid_1's l1: 0.364649\n",
      "[38000]\ttraining's l1: 0.0096204\tvalid_1's l1: 0.364646\n",
      "[38500]\ttraining's l1: 0.00929687\tvalid_1's l1: 0.364632\n",
      "[39000]\ttraining's l1: 0.00898793\tvalid_1's l1: 0.364614\n",
      "[39500]\ttraining's l1: 0.00869189\tvalid_1's l1: 0.3646\n",
      "[40000]\ttraining's l1: 0.00840768\tvalid_1's l1: 0.364584\n",
      "[40500]\ttraining's l1: 0.00813416\tvalid_1's l1: 0.364573\n",
      "[41000]\ttraining's l1: 0.00787286\tvalid_1's l1: 0.364565\n",
      "[41500]\ttraining's l1: 0.00762111\tvalid_1's l1: 0.364553\n",
      "[42000]\ttraining's l1: 0.007384\tvalid_1's l1: 0.364542\n",
      "[42500]\ttraining's l1: 0.00715571\tvalid_1's l1: 0.364529\n",
      "[43000]\ttraining's l1: 0.00693655\tvalid_1's l1: 0.364518\n",
      "[43500]\ttraining's l1: 0.00672525\tvalid_1's l1: 0.364506\n",
      "[44000]\ttraining's l1: 0.00652022\tvalid_1's l1: 0.364496\n",
      "[44500]\ttraining's l1: 0.00632469\tvalid_1's l1: 0.364483\n",
      "[45000]\ttraining's l1: 0.00614076\tvalid_1's l1: 0.364477\n",
      "[45500]\ttraining's l1: 0.00596243\tvalid_1's l1: 0.36447\n",
      "[46000]\ttraining's l1: 0.0057895\tvalid_1's l1: 0.364464\n",
      "[46500]\ttraining's l1: 0.00562294\tvalid_1's l1: 0.364454\n",
      "[47000]\ttraining's l1: 0.00546402\tvalid_1's l1: 0.364445\n",
      "[47500]\ttraining's l1: 0.00530907\tvalid_1's l1: 0.364439\n",
      "[48000]\ttraining's l1: 0.00516204\tvalid_1's l1: 0.364433\n",
      "[48500]\ttraining's l1: 0.00502096\tvalid_1's l1: 0.364429\n",
      "[49000]\ttraining's l1: 0.00488637\tvalid_1's l1: 0.364424\n",
      "[49500]\ttraining's l1: 0.00475728\tvalid_1's l1: 0.364418\n",
      "[50000]\ttraining's l1: 0.00463185\tvalid_1's l1: 0.364414\n",
      "[50500]\ttraining's l1: 0.00451025\tvalid_1's l1: 0.36441\n",
      "[51000]\ttraining's l1: 0.0043948\tvalid_1's l1: 0.364404\n",
      "[51500]\ttraining's l1: 0.00428373\tvalid_1's l1: 0.3644\n",
      "[52000]\ttraining's l1: 0.00417742\tvalid_1's l1: 0.364394\n",
      "[52500]\ttraining's l1: 0.00407467\tvalid_1's l1: 0.364388\n",
      "Early stopping, best iteration is:\n",
      "[52716]\ttraining's l1: 0.00403113\tvalid_1's l1: 0.364387\n",
      "Fold 5 started at Mon Jun 24 22:52:26 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.447553\tvalid_1's l1: 0.494264\n",
      "[1000]\ttraining's l1: 0.374186\tvalid_1's l1: 0.450823\n",
      "[1500]\ttraining's l1: 0.329033\tvalid_1's l1: 0.430127\n",
      "[2000]\ttraining's l1: 0.296217\tvalid_1's l1: 0.417365\n",
      "[2500]\ttraining's l1: 0.269345\tvalid_1's l1: 0.408173\n",
      "[3000]\ttraining's l1: 0.247306\tvalid_1's l1: 0.40183\n",
      "[3500]\ttraining's l1: 0.228643\tvalid_1's l1: 0.396875\n",
      "[4000]\ttraining's l1: 0.212132\tvalid_1's l1: 0.393009\n",
      "[4500]\ttraining's l1: 0.197493\tvalid_1's l1: 0.389758\n",
      "[5000]\ttraining's l1: 0.184311\tvalid_1's l1: 0.38714\n",
      "[5500]\ttraining's l1: 0.172413\tvalid_1's l1: 0.384841\n",
      "[6000]\ttraining's l1: 0.161615\tvalid_1's l1: 0.382771\n",
      "[6500]\ttraining's l1: 0.15176\tvalid_1's l1: 0.381252\n",
      "[7000]\ttraining's l1: 0.142729\tvalid_1's l1: 0.379759\n",
      "[7500]\ttraining's l1: 0.13468\tvalid_1's l1: 0.37864\n",
      "[8000]\ttraining's l1: 0.127105\tvalid_1's l1: 0.377468\n",
      "[8500]\ttraining's l1: 0.120024\tvalid_1's l1: 0.376365\n",
      "[9000]\ttraining's l1: 0.113479\tvalid_1's l1: 0.375711\n",
      "[9500]\ttraining's l1: 0.107388\tvalid_1's l1: 0.374906\n",
      "[10000]\ttraining's l1: 0.101744\tvalid_1's l1: 0.374072\n",
      "[10500]\ttraining's l1: 0.0964467\tvalid_1's l1: 0.373463\n",
      "[11000]\ttraining's l1: 0.091443\tvalid_1's l1: 0.372899\n",
      "[11500]\ttraining's l1: 0.0868386\tvalid_1's l1: 0.372338\n",
      "[12000]\ttraining's l1: 0.0825164\tvalid_1's l1: 0.371808\n",
      "[12500]\ttraining's l1: 0.0784166\tvalid_1's l1: 0.371352\n",
      "[13000]\ttraining's l1: 0.0746103\tvalid_1's l1: 0.370943\n",
      "[13500]\ttraining's l1: 0.0710195\tvalid_1's l1: 0.370587\n",
      "[14000]\ttraining's l1: 0.0676094\tvalid_1's l1: 0.370274\n",
      "[14500]\ttraining's l1: 0.0644085\tvalid_1's l1: 0.369956\n",
      "[15000]\ttraining's l1: 0.0614182\tvalid_1's l1: 0.36966\n",
      "[15500]\ttraining's l1: 0.0585639\tvalid_1's l1: 0.369389\n",
      "[16000]\ttraining's l1: 0.0558602\tvalid_1's l1: 0.369176\n",
      "[16500]\ttraining's l1: 0.0533005\tvalid_1's l1: 0.368962\n",
      "[17000]\ttraining's l1: 0.050907\tvalid_1's l1: 0.368723\n",
      "[17500]\ttraining's l1: 0.0486392\tvalid_1's l1: 0.368531\n",
      "[18000]\ttraining's l1: 0.0464742\tvalid_1's l1: 0.368349\n",
      "[18500]\ttraining's l1: 0.0444183\tvalid_1's l1: 0.368188\n",
      "[19000]\ttraining's l1: 0.0424723\tvalid_1's l1: 0.368048\n",
      "[19500]\ttraining's l1: 0.0406242\tvalid_1's l1: 0.367899\n",
      "[20000]\ttraining's l1: 0.0388817\tvalid_1's l1: 0.367749\n",
      "[20500]\ttraining's l1: 0.0372294\tvalid_1's l1: 0.367596\n",
      "[21000]\ttraining's l1: 0.0356436\tvalid_1's l1: 0.367469\n",
      "[21500]\ttraining's l1: 0.0341452\tvalid_1's l1: 0.367346\n",
      "[22000]\ttraining's l1: 0.0327069\tvalid_1's l1: 0.367243\n",
      "[22500]\ttraining's l1: 0.0313395\tvalid_1's l1: 0.367154\n",
      "[23000]\ttraining's l1: 0.0300405\tvalid_1's l1: 0.367054\n",
      "[23500]\ttraining's l1: 0.0287909\tvalid_1's l1: 0.366962\n",
      "[24000]\ttraining's l1: 0.0276162\tvalid_1's l1: 0.366882\n",
      "[24500]\ttraining's l1: 0.0264951\tvalid_1's l1: 0.3668\n",
      "[25000]\ttraining's l1: 0.025428\tvalid_1's l1: 0.366716\n",
      "[25500]\ttraining's l1: 0.0244075\tvalid_1's l1: 0.366646\n",
      "[26000]\ttraining's l1: 0.0234339\tvalid_1's l1: 0.36659\n",
      "[26500]\ttraining's l1: 0.0225073\tvalid_1's l1: 0.366535\n",
      "[27000]\ttraining's l1: 0.0216159\tvalid_1's l1: 0.366493\n",
      "[27500]\ttraining's l1: 0.0207753\tvalid_1's l1: 0.36644\n",
      "[28000]\ttraining's l1: 0.0199718\tvalid_1's l1: 0.366394\n",
      "[28500]\ttraining's l1: 0.0192118\tvalid_1's l1: 0.36635\n",
      "[29000]\ttraining's l1: 0.0184781\tvalid_1's l1: 0.366302\n",
      "[29500]\ttraining's l1: 0.0177766\tvalid_1's l1: 0.366257\n",
      "[30000]\ttraining's l1: 0.0170995\tvalid_1's l1: 0.366204\n",
      "[30500]\ttraining's l1: 0.0164562\tvalid_1's l1: 0.366167\n",
      "[31000]\ttraining's l1: 0.0158402\tvalid_1's l1: 0.366153\n",
      "[31500]\ttraining's l1: 0.0152549\tvalid_1's l1: 0.36613\n",
      "[32000]\ttraining's l1: 0.0146952\tvalid_1's l1: 0.366093\n",
      "[32500]\ttraining's l1: 0.0141594\tvalid_1's l1: 0.366065\n",
      "[33000]\ttraining's l1: 0.0136419\tvalid_1's l1: 0.366034\n",
      "[33500]\ttraining's l1: 0.0131477\tvalid_1's l1: 0.366001\n",
      "[34000]\ttraining's l1: 0.0126736\tvalid_1's l1: 0.365982\n",
      "[34500]\ttraining's l1: 0.0122231\tvalid_1's l1: 0.365968\n",
      "[35000]\ttraining's l1: 0.0117977\tvalid_1's l1: 0.365949\n",
      "[35500]\ttraining's l1: 0.01139\tvalid_1's l1: 0.365935\n",
      "[36000]\ttraining's l1: 0.0109935\tvalid_1's l1: 0.365908\n",
      "[36500]\ttraining's l1: 0.0106157\tvalid_1's l1: 0.365893\n",
      "[37000]\ttraining's l1: 0.0102508\tvalid_1's l1: 0.365871\n",
      "[37500]\ttraining's l1: 0.00990274\tvalid_1's l1: 0.365858\n",
      "[38000]\ttraining's l1: 0.00957178\tvalid_1's l1: 0.365841\n",
      "[38500]\ttraining's l1: 0.00925114\tvalid_1's l1: 0.365817\n",
      "[39000]\ttraining's l1: 0.0089456\tvalid_1's l1: 0.365798\n",
      "[39500]\ttraining's l1: 0.00865327\tvalid_1's l1: 0.365784\n",
      "[40000]\ttraining's l1: 0.00837291\tvalid_1's l1: 0.365773\n",
      "[40500]\ttraining's l1: 0.0081029\tvalid_1's l1: 0.365759\n",
      "[41000]\ttraining's l1: 0.00784373\tvalid_1's l1: 0.365743\n",
      "[41500]\ttraining's l1: 0.00759596\tvalid_1's l1: 0.365735\n",
      "[42000]\ttraining's l1: 0.00735762\tvalid_1's l1: 0.365722\n",
      "[42500]\ttraining's l1: 0.00712838\tvalid_1's l1: 0.365713\n",
      "[43000]\ttraining's l1: 0.00690843\tvalid_1's l1: 0.365702\n",
      "[43500]\ttraining's l1: 0.00670083\tvalid_1's l1: 0.365698\n",
      "[44000]\ttraining's l1: 0.00649741\tvalid_1's l1: 0.365687\n",
      "[44500]\ttraining's l1: 0.00630452\tvalid_1's l1: 0.365679\n",
      "[45000]\ttraining's l1: 0.00611707\tvalid_1's l1: 0.365672\n",
      "[45500]\ttraining's l1: 0.00593817\tvalid_1's l1: 0.365662\n",
      "[46000]\ttraining's l1: 0.00576771\tvalid_1's l1: 0.365652\n",
      "[46500]\ttraining's l1: 0.00560224\tvalid_1's l1: 0.365642\n",
      "[47000]\ttraining's l1: 0.00544371\tvalid_1's l1: 0.365637\n",
      "[47500]\ttraining's l1: 0.00528993\tvalid_1's l1: 0.365628\n",
      "[48000]\ttraining's l1: 0.00514591\tvalid_1's l1: 0.365621\n",
      "[48500]\ttraining's l1: 0.00500324\tvalid_1's l1: 0.365614\n",
      "[49000]\ttraining's l1: 0.00486758\tvalid_1's l1: 0.365608\n",
      "[49500]\ttraining's l1: 0.00473605\tvalid_1's l1: 0.365605\n",
      "[50000]\ttraining's l1: 0.00461143\tvalid_1's l1: 0.365596\n",
      "[50500]\ttraining's l1: 0.00449208\tvalid_1's l1: 0.365588\n",
      "[51000]\ttraining's l1: 0.00437643\tvalid_1's l1: 0.365581\n",
      "[51500]\ttraining's l1: 0.00426593\tvalid_1's l1: 0.365572\n",
      "[52000]\ttraining's l1: 0.00415904\tvalid_1's l1: 0.365567\n",
      "[52500]\ttraining's l1: 0.00405557\tvalid_1's l1: 0.365563\n",
      "[53000]\ttraining's l1: 0.00395457\tvalid_1's l1: 0.365559\n",
      "[53500]\ttraining's l1: 0.00385872\tvalid_1's l1: 0.365556\n",
      "[54000]\ttraining's l1: 0.00376687\tvalid_1's l1: 0.365551\n",
      "[54500]\ttraining's l1: 0.00367691\tvalid_1's l1: 0.365546\n",
      "[55000]\ttraining's l1: 0.00359086\tvalid_1's l1: 0.365543\n",
      "[55500]\ttraining's l1: 0.0035082\tvalid_1's l1: 0.365537\n",
      "[56000]\ttraining's l1: 0.00342959\tvalid_1's l1: 0.365532\n",
      "[56500]\ttraining's l1: 0.00335315\tvalid_1's l1: 0.365531\n",
      "[57000]\ttraining's l1: 0.00327865\tvalid_1's l1: 0.365527\n",
      "[57500]\ttraining's l1: 0.00320812\tvalid_1's l1: 0.365524\n",
      "[58000]\ttraining's l1: 0.00313844\tvalid_1's l1: 0.365521\n",
      "[58500]\ttraining's l1: 0.00307173\tvalid_1's l1: 0.365517\n",
      "[59000]\ttraining's l1: 0.00300709\tvalid_1's l1: 0.365515\n",
      "[59500]\ttraining's l1: 0.00294544\tvalid_1's l1: 0.36551\n",
      "[60000]\ttraining's l1: 0.00288566\tvalid_1's l1: 0.365509\n",
      "[60500]\ttraining's l1: 0.00282699\tvalid_1's l1: 0.365505\n",
      "[61000]\ttraining's l1: 0.00277086\tvalid_1's l1: 0.365503\n",
      "[61500]\ttraining's l1: 0.00271655\tvalid_1's l1: 0.3655\n",
      "[62000]\ttraining's l1: 0.00266428\tvalid_1's l1: 0.365499\n",
      "Early stopping, best iteration is:\n",
      "[61861]\ttraining's l1: 0.0026785\tvalid_1's l1: 0.365498\n",
      "CV mean score: -1.0063, std: 0.0019.\n",
      "Training of type 5\n",
      "Fold 1 started at Mon Jun 24 23:29:15 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.942559\tvalid_1's l1: 0.987302\n",
      "[1000]\ttraining's l1: 0.824269\tvalid_1's l1: 0.896412\n",
      "[1500]\ttraining's l1: 0.752787\tvalid_1's l1: 0.848127\n",
      "[2000]\ttraining's l1: 0.700097\tvalid_1's l1: 0.816695\n",
      "[2500]\ttraining's l1: 0.657968\tvalid_1's l1: 0.793653\n",
      "[3000]\ttraining's l1: 0.623378\tvalid_1's l1: 0.77719\n",
      "[3500]\ttraining's l1: 0.593611\tvalid_1's l1: 0.764191\n",
      "[4000]\ttraining's l1: 0.567069\tvalid_1's l1: 0.753533\n",
      "[4500]\ttraining's l1: 0.54309\tvalid_1's l1: 0.744396\n",
      "[5000]\ttraining's l1: 0.521181\tvalid_1's l1: 0.736179\n",
      "[5500]\ttraining's l1: 0.501208\tvalid_1's l1: 0.729266\n",
      "[6000]\ttraining's l1: 0.482664\tvalid_1's l1: 0.722856\n",
      "[6500]\ttraining's l1: 0.465357\tvalid_1's l1: 0.717273\n",
      "[7000]\ttraining's l1: 0.449381\tvalid_1's l1: 0.712655\n",
      "[7500]\ttraining's l1: 0.43425\tvalid_1's l1: 0.708099\n",
      "[8000]\ttraining's l1: 0.420127\tvalid_1's l1: 0.704182\n",
      "[8500]\ttraining's l1: 0.406682\tvalid_1's l1: 0.700675\n",
      "[9000]\ttraining's l1: 0.393994\tvalid_1's l1: 0.697323\n",
      "[9500]\ttraining's l1: 0.382012\tvalid_1's l1: 0.694548\n",
      "[10000]\ttraining's l1: 0.37033\tvalid_1's l1: 0.691445\n",
      "[10500]\ttraining's l1: 0.359285\tvalid_1's l1: 0.688699\n",
      "[11000]\ttraining's l1: 0.34885\tvalid_1's l1: 0.686535\n",
      "[11500]\ttraining's l1: 0.338754\tvalid_1's l1: 0.68416\n",
      "[12000]\ttraining's l1: 0.329294\tvalid_1's l1: 0.682275\n",
      "[12500]\ttraining's l1: 0.320118\tvalid_1's l1: 0.680356\n",
      "[13000]\ttraining's l1: 0.311412\tvalid_1's l1: 0.678527\n",
      "[13500]\ttraining's l1: 0.303017\tvalid_1's l1: 0.676878\n",
      "[14000]\ttraining's l1: 0.294854\tvalid_1's l1: 0.675336\n",
      "[14500]\ttraining's l1: 0.286977\tvalid_1's l1: 0.673864\n",
      "[15000]\ttraining's l1: 0.279399\tvalid_1's l1: 0.672466\n",
      "[15500]\ttraining's l1: 0.272199\tvalid_1's l1: 0.671148\n",
      "[16000]\ttraining's l1: 0.265155\tvalid_1's l1: 0.669902\n",
      "[16500]\ttraining's l1: 0.258366\tvalid_1's l1: 0.668689\n",
      "[17000]\ttraining's l1: 0.251873\tvalid_1's l1: 0.667564\n",
      "[17500]\ttraining's l1: 0.245556\tvalid_1's l1: 0.666412\n",
      "[18000]\ttraining's l1: 0.239458\tvalid_1's l1: 0.665362\n",
      "[18500]\ttraining's l1: 0.233445\tvalid_1's l1: 0.664376\n",
      "[19000]\ttraining's l1: 0.227757\tvalid_1's l1: 0.663396\n",
      "[19500]\ttraining's l1: 0.222209\tvalid_1's l1: 0.662506\n",
      "[20000]\ttraining's l1: 0.216866\tvalid_1's l1: 0.661655\n",
      "[20500]\ttraining's l1: 0.211717\tvalid_1's l1: 0.660833\n",
      "[21000]\ttraining's l1: 0.206627\tvalid_1's l1: 0.660086\n",
      "[21500]\ttraining's l1: 0.201828\tvalid_1's l1: 0.659371\n",
      "[22000]\ttraining's l1: 0.197165\tvalid_1's l1: 0.658692\n",
      "[22500]\ttraining's l1: 0.192596\tvalid_1's l1: 0.657974\n",
      "[23000]\ttraining's l1: 0.18819\tvalid_1's l1: 0.65737\n",
      "[23500]\ttraining's l1: 0.183849\tvalid_1's l1: 0.656682\n",
      "[24000]\ttraining's l1: 0.179669\tvalid_1's l1: 0.656113\n",
      "[24500]\ttraining's l1: 0.175626\tvalid_1's l1: 0.655623\n",
      "[25000]\ttraining's l1: 0.171632\tvalid_1's l1: 0.655098\n",
      "[25500]\ttraining's l1: 0.167789\tvalid_1's l1: 0.65462\n",
      "[26000]\ttraining's l1: 0.16404\tvalid_1's l1: 0.654145\n",
      "[26500]\ttraining's l1: 0.160392\tvalid_1's l1: 0.653624\n",
      "[27000]\ttraining's l1: 0.156825\tvalid_1's l1: 0.653125\n",
      "[27500]\ttraining's l1: 0.15339\tvalid_1's l1: 0.652707\n",
      "[28000]\ttraining's l1: 0.150056\tvalid_1's l1: 0.652298\n",
      "[28500]\ttraining's l1: 0.146808\tvalid_1's l1: 0.651845\n",
      "[29000]\ttraining's l1: 0.143617\tvalid_1's l1: 0.651476\n",
      "[29500]\ttraining's l1: 0.140523\tvalid_1's l1: 0.65116\n",
      "[30000]\ttraining's l1: 0.137522\tvalid_1's l1: 0.650794\n",
      "[30500]\ttraining's l1: 0.134565\tvalid_1's l1: 0.650474\n",
      "[31000]\ttraining's l1: 0.131695\tvalid_1's l1: 0.650099\n",
      "[31500]\ttraining's l1: 0.1289\tvalid_1's l1: 0.64975\n",
      "[32000]\ttraining's l1: 0.126164\tvalid_1's l1: 0.649484\n",
      "[32500]\ttraining's l1: 0.123519\tvalid_1's l1: 0.649197\n",
      "[33000]\ttraining's l1: 0.120922\tvalid_1's l1: 0.648931\n",
      "[33500]\ttraining's l1: 0.118402\tvalid_1's l1: 0.648625\n",
      "[34000]\ttraining's l1: 0.115971\tvalid_1's l1: 0.648354\n",
      "[34500]\ttraining's l1: 0.113558\tvalid_1's l1: 0.648109\n",
      "[35000]\ttraining's l1: 0.111213\tvalid_1's l1: 0.647852\n",
      "[35500]\ttraining's l1: 0.108938\tvalid_1's l1: 0.647618\n",
      "[36000]\ttraining's l1: 0.106693\tvalid_1's l1: 0.647364\n",
      "[36500]\ttraining's l1: 0.104502\tvalid_1's l1: 0.64712\n",
      "[37000]\ttraining's l1: 0.102371\tvalid_1's l1: 0.646893\n",
      "[37500]\ttraining's l1: 0.100286\tvalid_1's l1: 0.646693\n",
      "[38000]\ttraining's l1: 0.0982427\tvalid_1's l1: 0.646429\n",
      "[38500]\ttraining's l1: 0.0962437\tvalid_1's l1: 0.646215\n",
      "[39000]\ttraining's l1: 0.0942914\tvalid_1's l1: 0.646019\n",
      "[39500]\ttraining's l1: 0.0924122\tvalid_1's l1: 0.645822\n",
      "[40000]\ttraining's l1: 0.0905531\tvalid_1's l1: 0.645638\n",
      "[40500]\ttraining's l1: 0.0887567\tvalid_1's l1: 0.645469\n",
      "[41000]\ttraining's l1: 0.0869913\tvalid_1's l1: 0.645282\n",
      "[41500]\ttraining's l1: 0.0852821\tvalid_1's l1: 0.64514\n",
      "[42000]\ttraining's l1: 0.0835985\tvalid_1's l1: 0.645003\n",
      "[42500]\ttraining's l1: 0.0819322\tvalid_1's l1: 0.644827\n",
      "[43000]\ttraining's l1: 0.080333\tvalid_1's l1: 0.644665\n",
      "[43500]\ttraining's l1: 0.0787351\tvalid_1's l1: 0.644508\n",
      "[44000]\ttraining's l1: 0.0771873\tvalid_1's l1: 0.644369\n",
      "[44500]\ttraining's l1: 0.0756889\tvalid_1's l1: 0.64419\n",
      "[45000]\ttraining's l1: 0.0742257\tvalid_1's l1: 0.644072\n",
      "[45500]\ttraining's l1: 0.0727954\tvalid_1's l1: 0.643949\n",
      "[46000]\ttraining's l1: 0.0713781\tvalid_1's l1: 0.64384\n",
      "[46500]\ttraining's l1: 0.0700006\tvalid_1's l1: 0.64373\n",
      "[47000]\ttraining's l1: 0.0686517\tvalid_1's l1: 0.643609\n",
      "[47500]\ttraining's l1: 0.0673476\tvalid_1's l1: 0.643503\n",
      "[48000]\ttraining's l1: 0.0660387\tvalid_1's l1: 0.643388\n",
      "[48500]\ttraining's l1: 0.0647764\tvalid_1's l1: 0.643282\n",
      "[49000]\ttraining's l1: 0.0635351\tvalid_1's l1: 0.643171\n",
      "[49500]\ttraining's l1: 0.0623295\tvalid_1's l1: 0.643085\n",
      "[50000]\ttraining's l1: 0.0611514\tvalid_1's l1: 0.642979\n",
      "[50500]\ttraining's l1: 0.0600012\tvalid_1's l1: 0.642891\n",
      "[51000]\ttraining's l1: 0.0588659\tvalid_1's l1: 0.642791\n",
      "[51500]\ttraining's l1: 0.0577469\tvalid_1's l1: 0.642686\n",
      "[52000]\ttraining's l1: 0.0566588\tvalid_1's l1: 0.642589\n",
      "[52500]\ttraining's l1: 0.0555981\tvalid_1's l1: 0.642493\n",
      "[53000]\ttraining's l1: 0.054569\tvalid_1's l1: 0.642411\n",
      "[53500]\ttraining's l1: 0.0535424\tvalid_1's l1: 0.642333\n",
      "[54000]\ttraining's l1: 0.0525277\tvalid_1's l1: 0.64226\n",
      "[54500]\ttraining's l1: 0.0515471\tvalid_1's l1: 0.642176\n",
      "[55000]\ttraining's l1: 0.0505959\tvalid_1's l1: 0.642104\n",
      "[55500]\ttraining's l1: 0.0496634\tvalid_1's l1: 0.642024\n",
      "[56000]\ttraining's l1: 0.0487394\tvalid_1's l1: 0.641955\n",
      "[56500]\ttraining's l1: 0.0478498\tvalid_1's l1: 0.641886\n",
      "[57000]\ttraining's l1: 0.0469781\tvalid_1's l1: 0.641824\n",
      "[57500]\ttraining's l1: 0.0461114\tvalid_1's l1: 0.641751\n",
      "[58000]\ttraining's l1: 0.0452695\tvalid_1's l1: 0.641705\n",
      "[58500]\ttraining's l1: 0.0444535\tvalid_1's l1: 0.641649\n",
      "[59000]\ttraining's l1: 0.0436437\tvalid_1's l1: 0.641599\n",
      "[59500]\ttraining's l1: 0.0428566\tvalid_1's l1: 0.641546\n",
      "[60000]\ttraining's l1: 0.0420865\tvalid_1's l1: 0.641487\n",
      "[60500]\ttraining's l1: 0.0413212\tvalid_1's l1: 0.641425\n",
      "[61000]\ttraining's l1: 0.040571\tvalid_1's l1: 0.641385\n",
      "[61500]\ttraining's l1: 0.0398414\tvalid_1's l1: 0.641331\n",
      "[62000]\ttraining's l1: 0.0391238\tvalid_1's l1: 0.641282\n",
      "[62500]\ttraining's l1: 0.0384222\tvalid_1's l1: 0.641232\n",
      "[63000]\ttraining's l1: 0.0377295\tvalid_1's l1: 0.641182\n",
      "[63500]\ttraining's l1: 0.0370548\tvalid_1's l1: 0.641129\n",
      "[64000]\ttraining's l1: 0.0363938\tvalid_1's l1: 0.64108\n",
      "[64500]\ttraining's l1: 0.0357442\tvalid_1's l1: 0.641036\n",
      "[65000]\ttraining's l1: 0.0351103\tvalid_1's l1: 0.640997\n",
      "[65500]\ttraining's l1: 0.0344814\tvalid_1's l1: 0.64096\n",
      "[66000]\ttraining's l1: 0.0338739\tvalid_1's l1: 0.640919\n",
      "[66500]\ttraining's l1: 0.033276\tvalid_1's l1: 0.640878\n",
      "[67000]\ttraining's l1: 0.0326908\tvalid_1's l1: 0.640835\n",
      "[67500]\ttraining's l1: 0.0321237\tvalid_1's l1: 0.640788\n",
      "[68000]\ttraining's l1: 0.0315673\tvalid_1's l1: 0.64075\n",
      "[68500]\ttraining's l1: 0.031003\tvalid_1's l1: 0.640704\n",
      "[69000]\ttraining's l1: 0.0304534\tvalid_1's l1: 0.640663\n",
      "[69500]\ttraining's l1: 0.0299246\tvalid_1's l1: 0.640632\n",
      "[70000]\ttraining's l1: 0.0294131\tvalid_1's l1: 0.640594\n",
      "[70500]\ttraining's l1: 0.0289006\tvalid_1's l1: 0.640556\n",
      "[71000]\ttraining's l1: 0.0284003\tvalid_1's l1: 0.640514\n",
      "[71500]\ttraining's l1: 0.0279099\tvalid_1's l1: 0.640485\n",
      "[72000]\ttraining's l1: 0.0274278\tvalid_1's l1: 0.640451\n",
      "[72500]\ttraining's l1: 0.0269543\tvalid_1's l1: 0.640421\n",
      "[73000]\ttraining's l1: 0.0264874\tvalid_1's l1: 0.640388\n",
      "[73500]\ttraining's l1: 0.0260343\tvalid_1's l1: 0.64036\n",
      "[74000]\ttraining's l1: 0.0255922\tvalid_1's l1: 0.640326\n",
      "[74500]\ttraining's l1: 0.0251539\tvalid_1's l1: 0.640293\n",
      "[75000]\ttraining's l1: 0.0247243\tvalid_1's l1: 0.640269\n",
      "[75500]\ttraining's l1: 0.0243064\tvalid_1's l1: 0.640248\n",
      "[76000]\ttraining's l1: 0.0238962\tvalid_1's l1: 0.640226\n",
      "[76500]\ttraining's l1: 0.0234873\tvalid_1's l1: 0.640203\n",
      "[77000]\ttraining's l1: 0.0230942\tvalid_1's l1: 0.64018\n",
      "[77500]\ttraining's l1: 0.0227069\tvalid_1's l1: 0.640155\n",
      "[78000]\ttraining's l1: 0.0223261\tvalid_1's l1: 0.640132\n",
      "[78500]\ttraining's l1: 0.0219479\tvalid_1's l1: 0.64011\n",
      "[79000]\ttraining's l1: 0.021577\tvalid_1's l1: 0.640082\n",
      "[79500]\ttraining's l1: 0.0212128\tvalid_1's l1: 0.640063\n",
      "[80000]\ttraining's l1: 0.0208635\tvalid_1's l1: 0.640036\n",
      "[80500]\ttraining's l1: 0.0205155\tvalid_1's l1: 0.640015\n",
      "[81000]\ttraining's l1: 0.0201743\tvalid_1's l1: 0.639997\n",
      "[81500]\ttraining's l1: 0.0198394\tvalid_1's l1: 0.639974\n",
      "[82000]\ttraining's l1: 0.0195066\tvalid_1's l1: 0.639947\n",
      "[82500]\ttraining's l1: 0.0191834\tvalid_1's l1: 0.639932\n",
      "[83000]\ttraining's l1: 0.0188698\tvalid_1's l1: 0.639908\n",
      "[83500]\ttraining's l1: 0.0185588\tvalid_1's l1: 0.639888\n",
      "[84000]\ttraining's l1: 0.0182509\tvalid_1's l1: 0.63987\n",
      "[84500]\ttraining's l1: 0.0179522\tvalid_1's l1: 0.639855\n",
      "[85000]\ttraining's l1: 0.0176574\tvalid_1's l1: 0.639842\n",
      "[85500]\ttraining's l1: 0.017374\tvalid_1's l1: 0.639822\n",
      "[86000]\ttraining's l1: 0.0170914\tvalid_1's l1: 0.639809\n",
      "[86500]\ttraining's l1: 0.0168147\tvalid_1's l1: 0.639791\n",
      "[87000]\ttraining's l1: 0.0165406\tvalid_1's l1: 0.63978\n",
      "[87500]\ttraining's l1: 0.0162747\tvalid_1's l1: 0.639768\n",
      "[88000]\ttraining's l1: 0.0160119\tvalid_1's l1: 0.639748\n",
      "[88500]\ttraining's l1: 0.0157533\tvalid_1's l1: 0.639732\n",
      "[89000]\ttraining's l1: 0.015502\tvalid_1's l1: 0.639716\n",
      "[89500]\ttraining's l1: 0.015255\tvalid_1's l1: 0.639704\n",
      "[90000]\ttraining's l1: 0.015012\tvalid_1's l1: 0.639686\n",
      "[90500]\ttraining's l1: 0.0147748\tvalid_1's l1: 0.639673\n",
      "[91000]\ttraining's l1: 0.0145408\tvalid_1's l1: 0.639661\n",
      "[91500]\ttraining's l1: 0.0143077\tvalid_1's l1: 0.639648\n",
      "[92000]\ttraining's l1: 0.0140815\tvalid_1's l1: 0.639635\n",
      "[92500]\ttraining's l1: 0.0138562\tvalid_1's l1: 0.639617\n",
      "[93000]\ttraining's l1: 0.0136419\tvalid_1's l1: 0.639604\n",
      "[93500]\ttraining's l1: 0.0134299\tvalid_1's l1: 0.639585\n",
      "[94000]\ttraining's l1: 0.0132188\tvalid_1's l1: 0.639575\n",
      "[94500]\ttraining's l1: 0.0130089\tvalid_1's l1: 0.639561\n",
      "[95000]\ttraining's l1: 0.0128074\tvalid_1's l1: 0.63955\n",
      "[95500]\ttraining's l1: 0.0126093\tvalid_1's l1: 0.63954\n",
      "[96000]\ttraining's l1: 0.0124154\tvalid_1's l1: 0.639526\n",
      "[96500]\ttraining's l1: 0.0122223\tvalid_1's l1: 0.639517\n",
      "[97000]\ttraining's l1: 0.0120339\tvalid_1's l1: 0.639508\n",
      "[97500]\ttraining's l1: 0.0118502\tvalid_1's l1: 0.639495\n",
      "[98000]\ttraining's l1: 0.0116698\tvalid_1's l1: 0.639485\n",
      "[98500]\ttraining's l1: 0.011492\tvalid_1's l1: 0.639474\n",
      "[99000]\ttraining's l1: 0.0113171\tvalid_1's l1: 0.639465\n",
      "[99500]\ttraining's l1: 0.0111444\tvalid_1's l1: 0.639458\n",
      "[100000]\ttraining's l1: 0.0109783\tvalid_1's l1: 0.639447\n",
      "[100500]\ttraining's l1: 0.0108137\tvalid_1's l1: 0.639438\n",
      "[101000]\ttraining's l1: 0.0106492\tvalid_1's l1: 0.639429\n",
      "[101500]\ttraining's l1: 0.0104899\tvalid_1's l1: 0.639419\n",
      "[102000]\ttraining's l1: 0.0103346\tvalid_1's l1: 0.639411\n",
      "[102500]\ttraining's l1: 0.0101803\tvalid_1's l1: 0.639403\n",
      "[103000]\ttraining's l1: 0.0100272\tvalid_1's l1: 0.639395\n",
      "[103500]\ttraining's l1: 0.00987943\tvalid_1's l1: 0.639383\n",
      "[104000]\ttraining's l1: 0.00973363\tvalid_1's l1: 0.639374\n",
      "[104500]\ttraining's l1: 0.00959099\tvalid_1's l1: 0.639367\n",
      "[105000]\ttraining's l1: 0.00944907\tvalid_1's l1: 0.639359\n",
      "[105500]\ttraining's l1: 0.00930987\tvalid_1's l1: 0.639352\n",
      "[106000]\ttraining's l1: 0.00917336\tvalid_1's l1: 0.639343\n",
      "[106500]\ttraining's l1: 0.00903641\tvalid_1's l1: 0.639336\n",
      "[107000]\ttraining's l1: 0.00890423\tvalid_1's l1: 0.639328\n",
      "[107500]\ttraining's l1: 0.00877538\tvalid_1's l1: 0.639323\n",
      "[108000]\ttraining's l1: 0.00864793\tvalid_1's l1: 0.639318\n",
      "[108500]\ttraining's l1: 0.00852435\tvalid_1's l1: 0.639311\n",
      "[109000]\ttraining's l1: 0.00840235\tvalid_1's l1: 0.639305\n",
      "[109500]\ttraining's l1: 0.00828239\tvalid_1's l1: 0.639298\n",
      "[110000]\ttraining's l1: 0.00816377\tvalid_1's l1: 0.639293\n",
      "[110500]\ttraining's l1: 0.00804729\tvalid_1's l1: 0.639287\n",
      "[111000]\ttraining's l1: 0.00793384\tvalid_1's l1: 0.639281\n",
      "[111500]\ttraining's l1: 0.00782151\tvalid_1's l1: 0.639276\n",
      "[112000]\ttraining's l1: 0.00771021\tvalid_1's l1: 0.63927\n",
      "[112500]\ttraining's l1: 0.00760161\tvalid_1's l1: 0.639264\n",
      "[113000]\ttraining's l1: 0.00749607\tvalid_1's l1: 0.639256\n",
      "[113500]\ttraining's l1: 0.00739154\tvalid_1's l1: 0.639249\n",
      "[114000]\ttraining's l1: 0.00729011\tvalid_1's l1: 0.639245\n",
      "[114500]\ttraining's l1: 0.00719047\tvalid_1's l1: 0.63924\n",
      "[115000]\ttraining's l1: 0.00709229\tvalid_1's l1: 0.639234\n",
      "[115500]\ttraining's l1: 0.00699439\tvalid_1's l1: 0.63923\n",
      "[116000]\ttraining's l1: 0.00689744\tvalid_1's l1: 0.639226\n",
      "[116500]\ttraining's l1: 0.00680407\tvalid_1's l1: 0.639222\n",
      "[117000]\ttraining's l1: 0.00671171\tvalid_1's l1: 0.639218\n",
      "[117500]\ttraining's l1: 0.0066213\tvalid_1's l1: 0.639215\n",
      "[118000]\ttraining's l1: 0.00653268\tvalid_1's l1: 0.639211\n",
      "[118500]\ttraining's l1: 0.00644435\tvalid_1's l1: 0.639207\n",
      "[119000]\ttraining's l1: 0.00635799\tvalid_1's l1: 0.639202\n",
      "[119500]\ttraining's l1: 0.00627244\tvalid_1's l1: 0.639199\n",
      "[120000]\ttraining's l1: 0.00618944\tvalid_1's l1: 0.639195\n",
      "[120500]\ttraining's l1: 0.00610856\tvalid_1's l1: 0.639191\n",
      "[121000]\ttraining's l1: 0.00602823\tvalid_1's l1: 0.639187\n",
      "[121500]\ttraining's l1: 0.00594847\tvalid_1's l1: 0.639184\n",
      "[122000]\ttraining's l1: 0.00587175\tvalid_1's l1: 0.63918\n",
      "[122500]\ttraining's l1: 0.00579539\tvalid_1's l1: 0.639177\n",
      "[123000]\ttraining's l1: 0.00572133\tvalid_1's l1: 0.639171\n",
      "[123500]\ttraining's l1: 0.00564777\tvalid_1's l1: 0.639168\n",
      "[124000]\ttraining's l1: 0.00557492\tvalid_1's l1: 0.639163\n",
      "[124500]\ttraining's l1: 0.00550268\tvalid_1's l1: 0.63916\n",
      "[125000]\ttraining's l1: 0.0054315\tvalid_1's l1: 0.639156\n",
      "[125500]\ttraining's l1: 0.0053625\tvalid_1's l1: 0.639153\n",
      "[126000]\ttraining's l1: 0.00529366\tvalid_1's l1: 0.639149\n",
      "[126500]\ttraining's l1: 0.00522648\tvalid_1's l1: 0.639146\n",
      "[127000]\ttraining's l1: 0.00516128\tvalid_1's l1: 0.639144\n",
      "[127500]\ttraining's l1: 0.00509664\tvalid_1's l1: 0.639141\n",
      "[128000]\ttraining's l1: 0.00503329\tvalid_1's l1: 0.639136\n",
      "[128500]\ttraining's l1: 0.00497159\tvalid_1's l1: 0.639133\n",
      "[129000]\ttraining's l1: 0.00490988\tvalid_1's l1: 0.639131\n",
      "[129500]\ttraining's l1: 0.00484957\tvalid_1's l1: 0.639128\n",
      "[130000]\ttraining's l1: 0.00479046\tvalid_1's l1: 0.639125\n",
      "[130500]\ttraining's l1: 0.00473235\tvalid_1's l1: 0.639122\n",
      "[131000]\ttraining's l1: 0.00467536\tvalid_1's l1: 0.639119\n",
      "[131500]\ttraining's l1: 0.00461914\tvalid_1's l1: 0.639115\n",
      "[132000]\ttraining's l1: 0.00456306\tvalid_1's l1: 0.639113\n",
      "[132500]\ttraining's l1: 0.00450926\tvalid_1's l1: 0.63911\n",
      "[133000]\ttraining's l1: 0.00445572\tvalid_1's l1: 0.639108\n",
      "[133500]\ttraining's l1: 0.0044029\tvalid_1's l1: 0.639106\n",
      "[134000]\ttraining's l1: 0.00435117\tvalid_1's l1: 0.639102\n",
      "[134500]\ttraining's l1: 0.00430011\tvalid_1's l1: 0.6391\n",
      "[135000]\ttraining's l1: 0.0042496\tvalid_1's l1: 0.639098\n",
      "[135500]\ttraining's l1: 0.00420025\tvalid_1's l1: 0.639094\n",
      "[136000]\ttraining's l1: 0.00415146\tvalid_1's l1: 0.639091\n",
      "[136500]\ttraining's l1: 0.00410369\tvalid_1's l1: 0.63909\n",
      "[137000]\ttraining's l1: 0.00405625\tvalid_1's l1: 0.639087\n",
      "[137500]\ttraining's l1: 0.00400961\tvalid_1's l1: 0.639085\n",
      "[138000]\ttraining's l1: 0.00396362\tvalid_1's l1: 0.639082\n",
      "[138500]\ttraining's l1: 0.00391859\tvalid_1's l1: 0.63908\n",
      "[139000]\ttraining's l1: 0.00387443\tvalid_1's l1: 0.639078\n",
      "[139500]\ttraining's l1: 0.00383093\tvalid_1's l1: 0.639077\n",
      "[140000]\ttraining's l1: 0.00378804\tvalid_1's l1: 0.639074\n",
      "[140500]\ttraining's l1: 0.00374617\tvalid_1's l1: 0.639072\n",
      "[141000]\ttraining's l1: 0.00370498\tvalid_1's l1: 0.63907\n",
      "[141500]\ttraining's l1: 0.00366469\tvalid_1's l1: 0.639067\n",
      "[142000]\ttraining's l1: 0.00362487\tvalid_1's l1: 0.639066\n",
      "[142500]\ttraining's l1: 0.00358559\tvalid_1's l1: 0.639064\n",
      "[143000]\ttraining's l1: 0.00354663\tvalid_1's l1: 0.639062\n",
      "[143500]\ttraining's l1: 0.00350886\tvalid_1's l1: 0.63906\n",
      "[144000]\ttraining's l1: 0.00347129\tvalid_1's l1: 0.639057\n",
      "[144500]\ttraining's l1: 0.00343456\tvalid_1's l1: 0.639056\n",
      "[145000]\ttraining's l1: 0.00339858\tvalid_1's l1: 0.639054\n",
      "Early stopping, best iteration is:\n",
      "[144828]\ttraining's l1: 0.00341093\tvalid_1's l1: 0.639054\n",
      "Fold 2 started at Tue Jun 25 01:42:29 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.942633\tvalid_1's l1: 0.983019\n",
      "[1000]\ttraining's l1: 0.825708\tvalid_1's l1: 0.895134\n",
      "[1500]\ttraining's l1: 0.753339\tvalid_1's l1: 0.846766\n",
      "[2000]\ttraining's l1: 0.701988\tvalid_1's l1: 0.817208\n",
      "[2500]\ttraining's l1: 0.659097\tvalid_1's l1: 0.793222\n",
      "[3000]\ttraining's l1: 0.623419\tvalid_1's l1: 0.776301\n",
      "[3500]\ttraining's l1: 0.592664\tvalid_1's l1: 0.762559\n",
      "[4000]\ttraining's l1: 0.565976\tvalid_1's l1: 0.751301\n",
      "[4500]\ttraining's l1: 0.542524\tvalid_1's l1: 0.742597\n",
      "[5000]\ttraining's l1: 0.520775\tvalid_1's l1: 0.734647\n",
      "[5500]\ttraining's l1: 0.500453\tvalid_1's l1: 0.727587\n",
      "[6000]\ttraining's l1: 0.481835\tvalid_1's l1: 0.721712\n",
      "[6500]\ttraining's l1: 0.464765\tvalid_1's l1: 0.716442\n",
      "[7000]\ttraining's l1: 0.44888\tvalid_1's l1: 0.711566\n",
      "[7500]\ttraining's l1: 0.433724\tvalid_1's l1: 0.707309\n",
      "[8000]\ttraining's l1: 0.419666\tvalid_1's l1: 0.703522\n",
      "[8500]\ttraining's l1: 0.406261\tvalid_1's l1: 0.700207\n",
      "[9000]\ttraining's l1: 0.393602\tvalid_1's l1: 0.69716\n",
      "[9500]\ttraining's l1: 0.381552\tvalid_1's l1: 0.694291\n",
      "[10000]\ttraining's l1: 0.370024\tvalid_1's l1: 0.691498\n",
      "[10500]\ttraining's l1: 0.359024\tvalid_1's l1: 0.68886\n",
      "[11000]\ttraining's l1: 0.348469\tvalid_1's l1: 0.686566\n",
      "[11500]\ttraining's l1: 0.33844\tvalid_1's l1: 0.684381\n",
      "[12000]\ttraining's l1: 0.328791\tvalid_1's l1: 0.682532\n",
      "[12500]\ttraining's l1: 0.319711\tvalid_1's l1: 0.680719\n",
      "[13000]\ttraining's l1: 0.310843\tvalid_1's l1: 0.678782\n",
      "[13500]\ttraining's l1: 0.302432\tvalid_1's l1: 0.677185\n",
      "[14000]\ttraining's l1: 0.294296\tvalid_1's l1: 0.675548\n",
      "[14500]\ttraining's l1: 0.286446\tvalid_1's l1: 0.674093\n",
      "[15000]\ttraining's l1: 0.27893\tvalid_1's l1: 0.672731\n",
      "[15500]\ttraining's l1: 0.271726\tvalid_1's l1: 0.671428\n",
      "[16000]\ttraining's l1: 0.264655\tvalid_1's l1: 0.670134\n",
      "[16500]\ttraining's l1: 0.257957\tvalid_1's l1: 0.668949\n",
      "[17000]\ttraining's l1: 0.251492\tvalid_1's l1: 0.667772\n",
      "[17500]\ttraining's l1: 0.245196\tvalid_1's l1: 0.666632\n",
      "[18000]\ttraining's l1: 0.239194\tvalid_1's l1: 0.665784\n",
      "[18500]\ttraining's l1: 0.233311\tvalid_1's l1: 0.664846\n",
      "[19000]\ttraining's l1: 0.227584\tvalid_1's l1: 0.663854\n",
      "[19500]\ttraining's l1: 0.22206\tvalid_1's l1: 0.662986\n",
      "[20000]\ttraining's l1: 0.216741\tvalid_1's l1: 0.662122\n",
      "[20500]\ttraining's l1: 0.211565\tvalid_1's l1: 0.661408\n",
      "[21000]\ttraining's l1: 0.20657\tvalid_1's l1: 0.660748\n",
      "[21500]\ttraining's l1: 0.201702\tvalid_1's l1: 0.660055\n",
      "[22000]\ttraining's l1: 0.197001\tvalid_1's l1: 0.65938\n",
      "[22500]\ttraining's l1: 0.192429\tvalid_1's l1: 0.65874\n",
      "[23000]\ttraining's l1: 0.188027\tvalid_1's l1: 0.658132\n",
      "[23500]\ttraining's l1: 0.18373\tvalid_1's l1: 0.657516\n",
      "[24000]\ttraining's l1: 0.179569\tvalid_1's l1: 0.656957\n",
      "[24500]\ttraining's l1: 0.175478\tvalid_1's l1: 0.656406\n",
      "[25000]\ttraining's l1: 0.171533\tvalid_1's l1: 0.655951\n",
      "[25500]\ttraining's l1: 0.167745\tvalid_1's l1: 0.655476\n",
      "[26000]\ttraining's l1: 0.164017\tvalid_1's l1: 0.654983\n",
      "[26500]\ttraining's l1: 0.160361\tvalid_1's l1: 0.654533\n",
      "[27000]\ttraining's l1: 0.156827\tvalid_1's l1: 0.654083\n",
      "[27500]\ttraining's l1: 0.153345\tvalid_1's l1: 0.653616\n",
      "[28000]\ttraining's l1: 0.150004\tvalid_1's l1: 0.653237\n",
      "[28500]\ttraining's l1: 0.146734\tvalid_1's l1: 0.65276\n",
      "[29000]\ttraining's l1: 0.143512\tvalid_1's l1: 0.652384\n",
      "[29500]\ttraining's l1: 0.140444\tvalid_1's l1: 0.651981\n",
      "[30000]\ttraining's l1: 0.137428\tvalid_1's l1: 0.651639\n",
      "[30500]\ttraining's l1: 0.134485\tvalid_1's l1: 0.651302\n",
      "[31000]\ttraining's l1: 0.131615\tvalid_1's l1: 0.650968\n",
      "[31500]\ttraining's l1: 0.128792\tvalid_1's l1: 0.650667\n",
      "[32000]\ttraining's l1: 0.126076\tvalid_1's l1: 0.65037\n",
      "[32500]\ttraining's l1: 0.123412\tvalid_1's l1: 0.650078\n",
      "[33000]\ttraining's l1: 0.120842\tvalid_1's l1: 0.649766\n",
      "[33500]\ttraining's l1: 0.118291\tvalid_1's l1: 0.649493\n",
      "[34000]\ttraining's l1: 0.115804\tvalid_1's l1: 0.649248\n",
      "[34500]\ttraining's l1: 0.11344\tvalid_1's l1: 0.648968\n",
      "[35000]\ttraining's l1: 0.111104\tvalid_1's l1: 0.648727\n",
      "[35500]\ttraining's l1: 0.108801\tvalid_1's l1: 0.648509\n",
      "[36000]\ttraining's l1: 0.10658\tvalid_1's l1: 0.648249\n",
      "[36500]\ttraining's l1: 0.104391\tvalid_1's l1: 0.648014\n",
      "[37000]\ttraining's l1: 0.102279\tvalid_1's l1: 0.647809\n",
      "[37500]\ttraining's l1: 0.10021\tvalid_1's l1: 0.647615\n",
      "[38000]\ttraining's l1: 0.0981728\tvalid_1's l1: 0.647412\n",
      "[38500]\ttraining's l1: 0.0962079\tvalid_1's l1: 0.647213\n",
      "[39000]\ttraining's l1: 0.0942737\tvalid_1's l1: 0.647054\n",
      "[39500]\ttraining's l1: 0.0923632\tvalid_1's l1: 0.646882\n",
      "[40000]\ttraining's l1: 0.0905029\tvalid_1's l1: 0.646701\n",
      "[40500]\ttraining's l1: 0.0886887\tvalid_1's l1: 0.646521\n",
      "[41000]\ttraining's l1: 0.0869236\tvalid_1's l1: 0.646349\n",
      "[41500]\ttraining's l1: 0.0851882\tvalid_1's l1: 0.646177\n",
      "[42000]\ttraining's l1: 0.083516\tvalid_1's l1: 0.645969\n",
      "[42500]\ttraining's l1: 0.0818635\tvalid_1's l1: 0.645833\n",
      "[43000]\ttraining's l1: 0.0802481\tvalid_1's l1: 0.645689\n",
      "[43500]\ttraining's l1: 0.0786821\tvalid_1's l1: 0.645554\n",
      "[44000]\ttraining's l1: 0.0771368\tvalid_1's l1: 0.645409\n",
      "[44500]\ttraining's l1: 0.0756328\tvalid_1's l1: 0.645264\n",
      "[45000]\ttraining's l1: 0.0741766\tvalid_1's l1: 0.645122\n",
      "[45500]\ttraining's l1: 0.0727464\tvalid_1's l1: 0.645\n",
      "[46000]\ttraining's l1: 0.0713273\tvalid_1's l1: 0.644868\n",
      "[46500]\ttraining's l1: 0.0699667\tvalid_1's l1: 0.644729\n",
      "[47000]\ttraining's l1: 0.0686228\tvalid_1's l1: 0.644618\n",
      "[47500]\ttraining's l1: 0.0672957\tvalid_1's l1: 0.644511\n",
      "[48000]\ttraining's l1: 0.065999\tvalid_1's l1: 0.64442\n",
      "[48500]\ttraining's l1: 0.064729\tvalid_1's l1: 0.644302\n",
      "[49000]\ttraining's l1: 0.0634939\tvalid_1's l1: 0.644195\n",
      "[49500]\ttraining's l1: 0.0623023\tvalid_1's l1: 0.644102\n",
      "[50000]\ttraining's l1: 0.0611196\tvalid_1's l1: 0.64401\n",
      "[50500]\ttraining's l1: 0.0599633\tvalid_1's l1: 0.64391\n",
      "[51000]\ttraining's l1: 0.0588236\tvalid_1's l1: 0.643829\n",
      "[51500]\ttraining's l1: 0.0577206\tvalid_1's l1: 0.643728\n",
      "[52000]\ttraining's l1: 0.0566414\tvalid_1's l1: 0.643658\n",
      "[52500]\ttraining's l1: 0.0555959\tvalid_1's l1: 0.643576\n",
      "[53000]\ttraining's l1: 0.0545654\tvalid_1's l1: 0.643494\n",
      "[53500]\ttraining's l1: 0.0535427\tvalid_1's l1: 0.643424\n",
      "[54000]\ttraining's l1: 0.0525313\tvalid_1's l1: 0.643348\n",
      "[54500]\ttraining's l1: 0.0515564\tvalid_1's l1: 0.643274\n",
      "[55000]\ttraining's l1: 0.050603\tvalid_1's l1: 0.643182\n",
      "[55500]\ttraining's l1: 0.0496674\tvalid_1's l1: 0.643113\n",
      "[56000]\ttraining's l1: 0.0487561\tvalid_1's l1: 0.643034\n",
      "[56500]\ttraining's l1: 0.0478635\tvalid_1's l1: 0.642961\n",
      "[57000]\ttraining's l1: 0.0469955\tvalid_1's l1: 0.642902\n",
      "[57500]\ttraining's l1: 0.0461293\tvalid_1's l1: 0.642846\n",
      "[58000]\ttraining's l1: 0.0452904\tvalid_1's l1: 0.642783\n",
      "[58500]\ttraining's l1: 0.0444565\tvalid_1's l1: 0.642737\n",
      "[59000]\ttraining's l1: 0.043634\tvalid_1's l1: 0.642672\n",
      "[59500]\ttraining's l1: 0.042831\tvalid_1's l1: 0.64262\n",
      "[60000]\ttraining's l1: 0.0420598\tvalid_1's l1: 0.642563\n",
      "[60500]\ttraining's l1: 0.0413064\tvalid_1's l1: 0.642514\n",
      "[61000]\ttraining's l1: 0.0405531\tvalid_1's l1: 0.642467\n",
      "[61500]\ttraining's l1: 0.0398226\tvalid_1's l1: 0.642416\n",
      "[62000]\ttraining's l1: 0.0391096\tvalid_1's l1: 0.642366\n",
      "[62500]\ttraining's l1: 0.0384139\tvalid_1's l1: 0.642301\n",
      "[63000]\ttraining's l1: 0.0377282\tvalid_1's l1: 0.642262\n",
      "[63500]\ttraining's l1: 0.0370511\tvalid_1's l1: 0.642208\n",
      "[64000]\ttraining's l1: 0.0363918\tvalid_1's l1: 0.642158\n",
      "[64500]\ttraining's l1: 0.035748\tvalid_1's l1: 0.642112\n",
      "[65000]\ttraining's l1: 0.0351105\tvalid_1's l1: 0.642063\n",
      "[65500]\ttraining's l1: 0.0344894\tvalid_1's l1: 0.642024\n",
      "[66000]\ttraining's l1: 0.0338812\tvalid_1's l1: 0.641974\n",
      "[66500]\ttraining's l1: 0.033279\tvalid_1's l1: 0.641938\n",
      "[67000]\ttraining's l1: 0.0326915\tvalid_1's l1: 0.641896\n",
      "[67500]\ttraining's l1: 0.0321214\tvalid_1's l1: 0.641862\n",
      "[68000]\ttraining's l1: 0.031557\tvalid_1's l1: 0.641828\n",
      "[68500]\ttraining's l1: 0.0310046\tvalid_1's l1: 0.641796\n",
      "[69000]\ttraining's l1: 0.0304686\tvalid_1's l1: 0.641744\n",
      "[69500]\ttraining's l1: 0.029941\tvalid_1's l1: 0.641706\n",
      "[70000]\ttraining's l1: 0.0294198\tvalid_1's l1: 0.641675\n",
      "[70500]\ttraining's l1: 0.0289094\tvalid_1's l1: 0.641644\n",
      "[71000]\ttraining's l1: 0.0284102\tvalid_1's l1: 0.641606\n",
      "[71500]\ttraining's l1: 0.0279213\tvalid_1's l1: 0.641575\n",
      "[72000]\ttraining's l1: 0.0274309\tvalid_1's l1: 0.641541\n",
      "[72500]\ttraining's l1: 0.0269602\tvalid_1's l1: 0.641506\n",
      "[73000]\ttraining's l1: 0.0264965\tvalid_1's l1: 0.641476\n",
      "[73500]\ttraining's l1: 0.0260389\tvalid_1's l1: 0.641443\n",
      "[74000]\ttraining's l1: 0.0255906\tvalid_1's l1: 0.64141\n",
      "[74500]\ttraining's l1: 0.0251554\tvalid_1's l1: 0.641381\n",
      "[75000]\ttraining's l1: 0.0247188\tvalid_1's l1: 0.641359\n",
      "[75500]\ttraining's l1: 0.0242964\tvalid_1's l1: 0.64133\n",
      "[76000]\ttraining's l1: 0.0238867\tvalid_1's l1: 0.6413\n",
      "[76500]\ttraining's l1: 0.0234887\tvalid_1's l1: 0.641267\n",
      "[77000]\ttraining's l1: 0.0230905\tvalid_1's l1: 0.641238\n",
      "[77500]\ttraining's l1: 0.0227013\tvalid_1's l1: 0.641209\n",
      "[78000]\ttraining's l1: 0.022319\tvalid_1's l1: 0.641184\n",
      "[78500]\ttraining's l1: 0.0219468\tvalid_1's l1: 0.641164\n",
      "[79000]\ttraining's l1: 0.0215806\tvalid_1's l1: 0.641144\n",
      "[79500]\ttraining's l1: 0.0212196\tvalid_1's l1: 0.641129\n",
      "[80000]\ttraining's l1: 0.020869\tvalid_1's l1: 0.641111\n",
      "[80500]\ttraining's l1: 0.0205229\tvalid_1's l1: 0.641087\n",
      "[81000]\ttraining's l1: 0.020182\tvalid_1's l1: 0.641065\n",
      "[81500]\ttraining's l1: 0.0198501\tvalid_1's l1: 0.641046\n",
      "[82000]\ttraining's l1: 0.0195246\tvalid_1's l1: 0.641024\n",
      "[82500]\ttraining's l1: 0.0192003\tvalid_1's l1: 0.641007\n",
      "[83000]\ttraining's l1: 0.0188867\tvalid_1's l1: 0.640986\n",
      "[83500]\ttraining's l1: 0.018577\tvalid_1's l1: 0.640969\n",
      "[84000]\ttraining's l1: 0.018276\tvalid_1's l1: 0.640949\n",
      "[84500]\ttraining's l1: 0.0179786\tvalid_1's l1: 0.640926\n",
      "[85000]\ttraining's l1: 0.0176858\tvalid_1's l1: 0.640909\n",
      "[85500]\ttraining's l1: 0.017397\tvalid_1's l1: 0.640889\n",
      "[86000]\ttraining's l1: 0.0171128\tvalid_1's l1: 0.64087\n",
      "[86500]\ttraining's l1: 0.0168375\tvalid_1's l1: 0.640857\n",
      "[87000]\ttraining's l1: 0.0165646\tvalid_1's l1: 0.640843\n",
      "[87500]\ttraining's l1: 0.0162996\tvalid_1's l1: 0.640826\n",
      "[88000]\ttraining's l1: 0.0160359\tvalid_1's l1: 0.640814\n",
      "[88500]\ttraining's l1: 0.0157792\tvalid_1's l1: 0.640801\n",
      "[89000]\ttraining's l1: 0.0155241\tvalid_1's l1: 0.640789\n",
      "[89500]\ttraining's l1: 0.0152786\tvalid_1's l1: 0.640773\n",
      "[90000]\ttraining's l1: 0.0150346\tvalid_1's l1: 0.640762\n",
      "[90500]\ttraining's l1: 0.0147948\tvalid_1's l1: 0.640743\n",
      "[91000]\ttraining's l1: 0.0145578\tvalid_1's l1: 0.640728\n",
      "[91500]\ttraining's l1: 0.0143247\tvalid_1's l1: 0.640715\n",
      "[92000]\ttraining's l1: 0.0140986\tvalid_1's l1: 0.6407\n",
      "[92500]\ttraining's l1: 0.0138755\tvalid_1's l1: 0.640688\n",
      "[93000]\ttraining's l1: 0.013657\tvalid_1's l1: 0.640677\n",
      "[93500]\ttraining's l1: 0.0134402\tvalid_1's l1: 0.640665\n",
      "[94000]\ttraining's l1: 0.0132301\tvalid_1's l1: 0.640654\n",
      "[94500]\ttraining's l1: 0.0130244\tvalid_1's l1: 0.640642\n",
      "[95000]\ttraining's l1: 0.012823\tvalid_1's l1: 0.640633\n",
      "[95500]\ttraining's l1: 0.0126238\tvalid_1's l1: 0.640623\n",
      "[96000]\ttraining's l1: 0.0124315\tvalid_1's l1: 0.640614\n",
      "[96500]\ttraining's l1: 0.0122416\tvalid_1's l1: 0.640602\n",
      "[97000]\ttraining's l1: 0.0120525\tvalid_1's l1: 0.640591\n",
      "[97500]\ttraining's l1: 0.0118679\tvalid_1's l1: 0.64058\n",
      "[98000]\ttraining's l1: 0.0116869\tvalid_1's l1: 0.640569\n",
      "[98500]\ttraining's l1: 0.0115111\tvalid_1's l1: 0.640559\n",
      "[99000]\ttraining's l1: 0.0113372\tvalid_1's l1: 0.640552\n",
      "[99500]\ttraining's l1: 0.0111632\tvalid_1's l1: 0.640541\n",
      "[100000]\ttraining's l1: 0.0109942\tvalid_1's l1: 0.640535\n",
      "[100500]\ttraining's l1: 0.0108289\tvalid_1's l1: 0.640522\n",
      "[101000]\ttraining's l1: 0.0106667\tvalid_1's l1: 0.640516\n",
      "[101500]\ttraining's l1: 0.0105059\tvalid_1's l1: 0.64051\n",
      "[102000]\ttraining's l1: 0.0103471\tvalid_1's l1: 0.640502\n",
      "[102500]\ttraining's l1: 0.0101938\tvalid_1's l1: 0.640492\n",
      "[103000]\ttraining's l1: 0.0100421\tvalid_1's l1: 0.640485\n",
      "[103500]\ttraining's l1: 0.00989363\tvalid_1's l1: 0.640478\n",
      "[104000]\ttraining's l1: 0.00974726\tvalid_1's l1: 0.640468\n",
      "[104500]\ttraining's l1: 0.00960382\tvalid_1's l1: 0.640462\n",
      "[105000]\ttraining's l1: 0.00946316\tvalid_1's l1: 0.640454\n",
      "[105500]\ttraining's l1: 0.00932294\tvalid_1's l1: 0.640448\n",
      "[106000]\ttraining's l1: 0.00918521\tvalid_1's l1: 0.640441\n",
      "[106500]\ttraining's l1: 0.00905174\tvalid_1's l1: 0.640432\n",
      "[107000]\ttraining's l1: 0.00892077\tvalid_1's l1: 0.640425\n",
      "[107500]\ttraining's l1: 0.00879254\tvalid_1's l1: 0.640416\n",
      "[108000]\ttraining's l1: 0.0086665\tvalid_1's l1: 0.640409\n",
      "[108500]\ttraining's l1: 0.00854243\tvalid_1's l1: 0.640401\n",
      "[109000]\ttraining's l1: 0.00841956\tvalid_1's l1: 0.640396\n",
      "[109500]\ttraining's l1: 0.00829844\tvalid_1's l1: 0.64039\n",
      "[110000]\ttraining's l1: 0.00817981\tvalid_1's l1: 0.640382\n",
      "[110500]\ttraining's l1: 0.00806304\tvalid_1's l1: 0.640376\n",
      "[111000]\ttraining's l1: 0.00794797\tvalid_1's l1: 0.640368\n",
      "[111500]\ttraining's l1: 0.00783543\tvalid_1's l1: 0.640362\n",
      "[112000]\ttraining's l1: 0.00772499\tvalid_1's l1: 0.640359\n",
      "[112500]\ttraining's l1: 0.00761831\tvalid_1's l1: 0.640354\n",
      "[113000]\ttraining's l1: 0.00751249\tvalid_1's l1: 0.640349\n",
      "[113500]\ttraining's l1: 0.00740904\tvalid_1's l1: 0.640344\n",
      "[114000]\ttraining's l1: 0.00730484\tvalid_1's l1: 0.640339\n",
      "[114500]\ttraining's l1: 0.00720298\tvalid_1's l1: 0.640336\n",
      "[115000]\ttraining's l1: 0.00710486\tvalid_1's l1: 0.640332\n",
      "[115500]\ttraining's l1: 0.0070074\tvalid_1's l1: 0.640327\n",
      "[116000]\ttraining's l1: 0.00691177\tvalid_1's l1: 0.640321\n",
      "[116500]\ttraining's l1: 0.00681655\tvalid_1's l1: 0.640315\n",
      "[117000]\ttraining's l1: 0.00672517\tvalid_1's l1: 0.640309\n",
      "[117500]\ttraining's l1: 0.00663516\tvalid_1's l1: 0.640304\n",
      "[118000]\ttraining's l1: 0.00654691\tvalid_1's l1: 0.640298\n",
      "[118500]\ttraining's l1: 0.00645855\tvalid_1's l1: 0.640293\n",
      "[119000]\ttraining's l1: 0.00637207\tvalid_1's l1: 0.640286\n",
      "[119500]\ttraining's l1: 0.00628813\tvalid_1's l1: 0.640283\n",
      "[120000]\ttraining's l1: 0.0062055\tvalid_1's l1: 0.640279\n",
      "[120500]\ttraining's l1: 0.00612287\tvalid_1's l1: 0.640275\n",
      "[121000]\ttraining's l1: 0.00604296\tvalid_1's l1: 0.64027\n",
      "[121500]\ttraining's l1: 0.00596428\tvalid_1's l1: 0.640265\n",
      "[122000]\ttraining's l1: 0.00588628\tvalid_1's l1: 0.64026\n",
      "[122500]\ttraining's l1: 0.00580923\tvalid_1's l1: 0.640258\n",
      "[123000]\ttraining's l1: 0.00573383\tvalid_1's l1: 0.640252\n",
      "[123500]\ttraining's l1: 0.00565999\tvalid_1's l1: 0.640249\n",
      "[124000]\ttraining's l1: 0.00558708\tvalid_1's l1: 0.640247\n",
      "[124500]\ttraining's l1: 0.00551583\tvalid_1's l1: 0.640244\n",
      "[125000]\ttraining's l1: 0.00544495\tvalid_1's l1: 0.64024\n",
      "[125500]\ttraining's l1: 0.00537553\tvalid_1's l1: 0.640235\n",
      "[126000]\ttraining's l1: 0.00530761\tvalid_1's l1: 0.640232\n",
      "[126500]\ttraining's l1: 0.00524\tvalid_1's l1: 0.64023\n",
      "[127000]\ttraining's l1: 0.00517528\tvalid_1's l1: 0.640227\n",
      "[127500]\ttraining's l1: 0.00511161\tvalid_1's l1: 0.640223\n",
      "[128000]\ttraining's l1: 0.00504934\tvalid_1's l1: 0.640222\n",
      "[128500]\ttraining's l1: 0.00498758\tvalid_1's l1: 0.640217\n",
      "[129000]\ttraining's l1: 0.00492578\tvalid_1's l1: 0.640215\n",
      "[129500]\ttraining's l1: 0.00486583\tvalid_1's l1: 0.640211\n",
      "[130000]\ttraining's l1: 0.00480614\tvalid_1's l1: 0.640209\n",
      "[130500]\ttraining's l1: 0.00474796\tvalid_1's l1: 0.640206\n",
      "[131000]\ttraining's l1: 0.00469094\tvalid_1's l1: 0.640204\n",
      "[131500]\ttraining's l1: 0.00463429\tvalid_1's l1: 0.640201\n",
      "[132000]\ttraining's l1: 0.00457892\tvalid_1's l1: 0.640199\n",
      "[132500]\ttraining's l1: 0.00452468\tvalid_1's l1: 0.640196\n",
      "[133000]\ttraining's l1: 0.00447166\tvalid_1's l1: 0.640193\n",
      "[133500]\ttraining's l1: 0.00441902\tvalid_1's l1: 0.640191\n",
      "[134000]\ttraining's l1: 0.00436731\tvalid_1's l1: 0.640188\n",
      "[134500]\ttraining's l1: 0.00431618\tvalid_1's l1: 0.640186\n",
      "[135000]\ttraining's l1: 0.00426593\tvalid_1's l1: 0.640183\n",
      "[135500]\ttraining's l1: 0.00421615\tvalid_1's l1: 0.640179\n",
      "[136000]\ttraining's l1: 0.00416813\tvalid_1's l1: 0.640176\n",
      "[136500]\ttraining's l1: 0.00411966\tvalid_1's l1: 0.640174\n",
      "[137000]\ttraining's l1: 0.00407253\tvalid_1's l1: 0.640172\n",
      "[137500]\ttraining's l1: 0.00402586\tvalid_1's l1: 0.640171\n",
      "[138000]\ttraining's l1: 0.00398058\tvalid_1's l1: 0.640169\n",
      "[138500]\ttraining's l1: 0.00393591\tvalid_1's l1: 0.640166\n",
      "[139000]\ttraining's l1: 0.00389168\tvalid_1's l1: 0.640164\n",
      "[139500]\ttraining's l1: 0.00384815\tvalid_1's l1: 0.640163\n",
      "[140000]\ttraining's l1: 0.0038056\tvalid_1's l1: 0.640161\n",
      "[140500]\ttraining's l1: 0.00376352\tvalid_1's l1: 0.640159\n",
      "[141000]\ttraining's l1: 0.00372295\tvalid_1's l1: 0.640157\n",
      "[141500]\ttraining's l1: 0.00368213\tvalid_1's l1: 0.640156\n",
      "[142000]\ttraining's l1: 0.00364226\tvalid_1's l1: 0.640154\n",
      "[142500]\ttraining's l1: 0.00360312\tvalid_1's l1: 0.640152\n",
      "[143000]\ttraining's l1: 0.00356475\tvalid_1's l1: 0.64015\n",
      "[143500]\ttraining's l1: 0.00352633\tvalid_1's l1: 0.640148\n",
      "[144000]\ttraining's l1: 0.00348848\tvalid_1's l1: 0.640146\n",
      "[144500]\ttraining's l1: 0.00345117\tvalid_1's l1: 0.640143\n",
      "[145000]\ttraining's l1: 0.00341467\tvalid_1's l1: 0.640142\n",
      "[145500]\ttraining's l1: 0.00337895\tvalid_1's l1: 0.64014\n",
      "[146000]\ttraining's l1: 0.00334331\tvalid_1's l1: 0.640139\n",
      "[146500]\ttraining's l1: 0.00330864\tvalid_1's l1: 0.640137\n",
      "[147000]\ttraining's l1: 0.00327422\tvalid_1's l1: 0.640134\n",
      "[147500]\ttraining's l1: 0.0032404\tvalid_1's l1: 0.640133\n",
      "[148000]\ttraining's l1: 0.00320769\tvalid_1's l1: 0.640132\n",
      "[148500]\ttraining's l1: 0.00317497\tvalid_1's l1: 0.64013\n",
      "Early stopping, best iteration is:\n",
      "[148632]\ttraining's l1: 0.00316646\tvalid_1's l1: 0.64013\n",
      "Fold 3 started at Tue Jun 25 03:56:15 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.94444\tvalid_1's l1: 0.984591\n",
      "[1000]\ttraining's l1: 0.824185\tvalid_1's l1: 0.893013\n",
      "[1500]\ttraining's l1: 0.752129\tvalid_1's l1: 0.844522\n",
      "[2000]\ttraining's l1: 0.699945\tvalid_1's l1: 0.813258\n",
      "[2500]\ttraining's l1: 0.658283\tvalid_1's l1: 0.791396\n",
      "[3000]\ttraining's l1: 0.623652\tvalid_1's l1: 0.7748\n",
      "[3500]\ttraining's l1: 0.59267\tvalid_1's l1: 0.760668\n",
      "[4000]\ttraining's l1: 0.566197\tvalid_1's l1: 0.749874\n",
      "[4500]\ttraining's l1: 0.541986\tvalid_1's l1: 0.739964\n",
      "[5000]\ttraining's l1: 0.520376\tvalid_1's l1: 0.732271\n",
      "[5500]\ttraining's l1: 0.500346\tvalid_1's l1: 0.72516\n",
      "[6000]\ttraining's l1: 0.481939\tvalid_1's l1: 0.718936\n",
      "[6500]\ttraining's l1: 0.464559\tvalid_1's l1: 0.71334\n",
      "[7000]\ttraining's l1: 0.44852\tvalid_1's l1: 0.708856\n",
      "[7500]\ttraining's l1: 0.433444\tvalid_1's l1: 0.704725\n",
      "[8000]\ttraining's l1: 0.419224\tvalid_1's l1: 0.700596\n",
      "[8500]\ttraining's l1: 0.405744\tvalid_1's l1: 0.696957\n",
      "[9000]\ttraining's l1: 0.393062\tvalid_1's l1: 0.693897\n",
      "[9500]\ttraining's l1: 0.381099\tvalid_1's l1: 0.691085\n",
      "[10000]\ttraining's l1: 0.36957\tvalid_1's l1: 0.688426\n",
      "[10500]\ttraining's l1: 0.358645\tvalid_1's l1: 0.685916\n",
      "[11000]\ttraining's l1: 0.348366\tvalid_1's l1: 0.683645\n",
      "[11500]\ttraining's l1: 0.338237\tvalid_1's l1: 0.681351\n",
      "[12000]\ttraining's l1: 0.328615\tvalid_1's l1: 0.679199\n",
      "[12500]\ttraining's l1: 0.319296\tvalid_1's l1: 0.677269\n",
      "[13000]\ttraining's l1: 0.310433\tvalid_1's l1: 0.675508\n",
      "[13500]\ttraining's l1: 0.302088\tvalid_1's l1: 0.673887\n",
      "[14000]\ttraining's l1: 0.293936\tvalid_1's l1: 0.67234\n",
      "[14500]\ttraining's l1: 0.286129\tvalid_1's l1: 0.670895\n",
      "[15000]\ttraining's l1: 0.278606\tvalid_1's l1: 0.669467\n",
      "[15500]\ttraining's l1: 0.271458\tvalid_1's l1: 0.668242\n",
      "[16000]\ttraining's l1: 0.264496\tvalid_1's l1: 0.667047\n",
      "[16500]\ttraining's l1: 0.257774\tvalid_1's l1: 0.665846\n",
      "[17000]\ttraining's l1: 0.251215\tvalid_1's l1: 0.664651\n",
      "[17500]\ttraining's l1: 0.244932\tvalid_1's l1: 0.663608\n",
      "[18000]\ttraining's l1: 0.238842\tvalid_1's l1: 0.662663\n",
      "[18500]\ttraining's l1: 0.232993\tvalid_1's l1: 0.661685\n",
      "[19000]\ttraining's l1: 0.227294\tvalid_1's l1: 0.660623\n",
      "[19500]\ttraining's l1: 0.221861\tvalid_1's l1: 0.659851\n",
      "[20000]\ttraining's l1: 0.216592\tvalid_1's l1: 0.659043\n",
      "[20500]\ttraining's l1: 0.211476\tvalid_1's l1: 0.658278\n",
      "[21000]\ttraining's l1: 0.206475\tvalid_1's l1: 0.657606\n",
      "[21500]\ttraining's l1: 0.201629\tvalid_1's l1: 0.656956\n",
      "[22000]\ttraining's l1: 0.196903\tvalid_1's l1: 0.656324\n",
      "[22500]\ttraining's l1: 0.19231\tvalid_1's l1: 0.655694\n",
      "[23000]\ttraining's l1: 0.187849\tvalid_1's l1: 0.655043\n",
      "[23500]\ttraining's l1: 0.183539\tvalid_1's l1: 0.654421\n",
      "[24000]\ttraining's l1: 0.179363\tvalid_1's l1: 0.653839\n",
      "[24500]\ttraining's l1: 0.175273\tvalid_1's l1: 0.653399\n",
      "[25000]\ttraining's l1: 0.171273\tvalid_1's l1: 0.652878\n",
      "[25500]\ttraining's l1: 0.167418\tvalid_1's l1: 0.652365\n",
      "[26000]\ttraining's l1: 0.16364\tvalid_1's l1: 0.651844\n",
      "[26500]\ttraining's l1: 0.160079\tvalid_1's l1: 0.651394\n",
      "[27000]\ttraining's l1: 0.156551\tvalid_1's l1: 0.650923\n",
      "[27500]\ttraining's l1: 0.153151\tvalid_1's l1: 0.650532\n",
      "[28000]\ttraining's l1: 0.149788\tvalid_1's l1: 0.650122\n",
      "[28500]\ttraining's l1: 0.146524\tvalid_1's l1: 0.649735\n",
      "[29000]\ttraining's l1: 0.143344\tvalid_1's l1: 0.649355\n",
      "[29500]\ttraining's l1: 0.140242\tvalid_1's l1: 0.648939\n",
      "[30000]\ttraining's l1: 0.137202\tvalid_1's l1: 0.648617\n",
      "[30500]\ttraining's l1: 0.134273\tvalid_1's l1: 0.648254\n",
      "[31000]\ttraining's l1: 0.131436\tvalid_1's l1: 0.64797\n",
      "[31500]\ttraining's l1: 0.128648\tvalid_1's l1: 0.647617\n",
      "[32000]\ttraining's l1: 0.125942\tvalid_1's l1: 0.64729\n",
      "[32500]\ttraining's l1: 0.123303\tvalid_1's l1: 0.647046\n",
      "[33000]\ttraining's l1: 0.120715\tvalid_1's l1: 0.646778\n",
      "[33500]\ttraining's l1: 0.118221\tvalid_1's l1: 0.646516\n",
      "[34000]\ttraining's l1: 0.11573\tvalid_1's l1: 0.646253\n",
      "[34500]\ttraining's l1: 0.113337\tvalid_1's l1: 0.645984\n",
      "[35000]\ttraining's l1: 0.110988\tvalid_1's l1: 0.645759\n",
      "[35500]\ttraining's l1: 0.108679\tvalid_1's l1: 0.645542\n",
      "[36000]\ttraining's l1: 0.106454\tvalid_1's l1: 0.645332\n",
      "[36500]\ttraining's l1: 0.104298\tvalid_1's l1: 0.645143\n",
      "[37000]\ttraining's l1: 0.102187\tvalid_1's l1: 0.644923\n",
      "[37500]\ttraining's l1: 0.100105\tvalid_1's l1: 0.644696\n",
      "[38000]\ttraining's l1: 0.0980569\tvalid_1's l1: 0.644505\n",
      "[38500]\ttraining's l1: 0.096061\tvalid_1's l1: 0.644335\n",
      "[39000]\ttraining's l1: 0.0941193\tvalid_1's l1: 0.644127\n",
      "[39500]\ttraining's l1: 0.0922042\tvalid_1's l1: 0.643918\n",
      "[40000]\ttraining's l1: 0.0903607\tvalid_1's l1: 0.643766\n",
      "[40500]\ttraining's l1: 0.0885532\tvalid_1's l1: 0.643597\n",
      "[41000]\ttraining's l1: 0.086783\tvalid_1's l1: 0.643425\n",
      "[41500]\ttraining's l1: 0.0850788\tvalid_1's l1: 0.643249\n",
      "[42000]\ttraining's l1: 0.0833821\tvalid_1's l1: 0.643088\n",
      "[42500]\ttraining's l1: 0.0817301\tvalid_1's l1: 0.64293\n",
      "[43000]\ttraining's l1: 0.0801397\tvalid_1's l1: 0.642803\n",
      "[43500]\ttraining's l1: 0.078559\tvalid_1's l1: 0.64266\n",
      "[44000]\ttraining's l1: 0.077008\tvalid_1's l1: 0.64255\n",
      "[44500]\ttraining's l1: 0.0755074\tvalid_1's l1: 0.642419\n",
      "[45000]\ttraining's l1: 0.0740549\tvalid_1's l1: 0.642279\n",
      "[45500]\ttraining's l1: 0.0726204\tvalid_1's l1: 0.642153\n",
      "[46000]\ttraining's l1: 0.0712123\tvalid_1's l1: 0.642028\n",
      "[46500]\ttraining's l1: 0.0698345\tvalid_1's l1: 0.64191\n",
      "[47000]\ttraining's l1: 0.0684981\tvalid_1's l1: 0.641791\n",
      "[47500]\ttraining's l1: 0.0671759\tvalid_1's l1: 0.641674\n",
      "[48000]\ttraining's l1: 0.0658955\tvalid_1's l1: 0.641553\n",
      "[48500]\ttraining's l1: 0.0646401\tvalid_1's l1: 0.641453\n",
      "[49000]\ttraining's l1: 0.0634212\tvalid_1's l1: 0.641341\n",
      "[49500]\ttraining's l1: 0.0622125\tvalid_1's l1: 0.641259\n",
      "[50000]\ttraining's l1: 0.0610278\tvalid_1's l1: 0.641146\n",
      "[50500]\ttraining's l1: 0.0598628\tvalid_1's l1: 0.641058\n",
      "[51000]\ttraining's l1: 0.0587334\tvalid_1's l1: 0.640968\n",
      "[51500]\ttraining's l1: 0.0576228\tvalid_1's l1: 0.640882\n",
      "[52000]\ttraining's l1: 0.0565523\tvalid_1's l1: 0.640799\n",
      "[52500]\ttraining's l1: 0.0554875\tvalid_1's l1: 0.640704\n",
      "[53000]\ttraining's l1: 0.0544545\tvalid_1's l1: 0.640625\n",
      "[53500]\ttraining's l1: 0.0534342\tvalid_1's l1: 0.640538\n",
      "[54000]\ttraining's l1: 0.0524407\tvalid_1's l1: 0.640475\n",
      "[54500]\ttraining's l1: 0.0514677\tvalid_1's l1: 0.640416\n",
      "[55000]\ttraining's l1: 0.050511\tvalid_1's l1: 0.640325\n",
      "[55500]\ttraining's l1: 0.0495742\tvalid_1's l1: 0.64026\n",
      "[56000]\ttraining's l1: 0.0486469\tvalid_1's l1: 0.640198\n",
      "[56500]\ttraining's l1: 0.0477517\tvalid_1's l1: 0.640128\n",
      "[57000]\ttraining's l1: 0.0468854\tvalid_1's l1: 0.64007\n",
      "[57500]\ttraining's l1: 0.0460311\tvalid_1's l1: 0.639999\n",
      "[58000]\ttraining's l1: 0.0451919\tvalid_1's l1: 0.639931\n",
      "[58500]\ttraining's l1: 0.0443659\tvalid_1's l1: 0.639886\n",
      "[59000]\ttraining's l1: 0.0435571\tvalid_1's l1: 0.639833\n",
      "[59500]\ttraining's l1: 0.0427701\tvalid_1's l1: 0.63977\n",
      "[60000]\ttraining's l1: 0.0419963\tvalid_1's l1: 0.639727\n",
      "[60500]\ttraining's l1: 0.0412368\tvalid_1's l1: 0.639677\n",
      "[61000]\ttraining's l1: 0.0405028\tvalid_1's l1: 0.639619\n",
      "[61500]\ttraining's l1: 0.0397582\tvalid_1's l1: 0.639554\n",
      "[62000]\ttraining's l1: 0.0390457\tvalid_1's l1: 0.639501\n",
      "[62500]\ttraining's l1: 0.0383492\tvalid_1's l1: 0.639453\n",
      "[63000]\ttraining's l1: 0.0376618\tvalid_1's l1: 0.639406\n",
      "[63500]\ttraining's l1: 0.0369818\tvalid_1's l1: 0.639369\n",
      "[64000]\ttraining's l1: 0.0363266\tvalid_1's l1: 0.639322\n",
      "[64500]\ttraining's l1: 0.0356781\tvalid_1's l1: 0.63928\n",
      "[65000]\ttraining's l1: 0.0350482\tvalid_1's l1: 0.639234\n",
      "[65500]\ttraining's l1: 0.0344284\tvalid_1's l1: 0.639186\n",
      "[66000]\ttraining's l1: 0.0338197\tvalid_1's l1: 0.639138\n",
      "[66500]\ttraining's l1: 0.0332218\tvalid_1's l1: 0.639108\n",
      "[67000]\ttraining's l1: 0.0326371\tvalid_1's l1: 0.639069\n",
      "[67500]\ttraining's l1: 0.0320657\tvalid_1's l1: 0.639023\n",
      "[68000]\ttraining's l1: 0.031504\tvalid_1's l1: 0.638985\n",
      "[68500]\ttraining's l1: 0.0309545\tvalid_1's l1: 0.638941\n",
      "[69000]\ttraining's l1: 0.0304038\tvalid_1's l1: 0.638907\n",
      "[69500]\ttraining's l1: 0.02988\tvalid_1's l1: 0.638883\n",
      "[70000]\ttraining's l1: 0.0293642\tvalid_1's l1: 0.638842\n",
      "[70500]\ttraining's l1: 0.0288539\tvalid_1's l1: 0.638814\n",
      "[71000]\ttraining's l1: 0.0283578\tvalid_1's l1: 0.638791\n",
      "[71500]\ttraining's l1: 0.0278709\tvalid_1's l1: 0.638752\n",
      "[72000]\ttraining's l1: 0.0273895\tvalid_1's l1: 0.638727\n",
      "[72500]\ttraining's l1: 0.0269191\tvalid_1's l1: 0.638697\n",
      "[73000]\ttraining's l1: 0.0264593\tvalid_1's l1: 0.638668\n",
      "[73500]\ttraining's l1: 0.0260051\tvalid_1's l1: 0.638641\n",
      "[74000]\ttraining's l1: 0.025555\tvalid_1's l1: 0.638618\n",
      "[74500]\ttraining's l1: 0.0251214\tvalid_1's l1: 0.63859\n",
      "[75000]\ttraining's l1: 0.0246894\tvalid_1's l1: 0.638558\n",
      "[75500]\ttraining's l1: 0.0242665\tvalid_1's l1: 0.638528\n",
      "[76000]\ttraining's l1: 0.0238594\tvalid_1's l1: 0.638502\n",
      "[76500]\ttraining's l1: 0.0234521\tvalid_1's l1: 0.638473\n",
      "[77000]\ttraining's l1: 0.0230554\tvalid_1's l1: 0.638449\n",
      "[77500]\ttraining's l1: 0.0226635\tvalid_1's l1: 0.638421\n",
      "[78000]\ttraining's l1: 0.022284\tvalid_1's l1: 0.638397\n",
      "[78500]\ttraining's l1: 0.0219098\tvalid_1's l1: 0.638377\n",
      "[79000]\ttraining's l1: 0.0215399\tvalid_1's l1: 0.638355\n",
      "[79500]\ttraining's l1: 0.0211812\tvalid_1's l1: 0.638336\n",
      "[80000]\ttraining's l1: 0.0208252\tvalid_1's l1: 0.638311\n",
      "[80500]\ttraining's l1: 0.0204791\tvalid_1's l1: 0.638287\n",
      "[81000]\ttraining's l1: 0.0201388\tvalid_1's l1: 0.638268\n",
      "[81500]\ttraining's l1: 0.019806\tvalid_1's l1: 0.638248\n",
      "[82000]\ttraining's l1: 0.0194795\tvalid_1's l1: 0.638224\n",
      "[82500]\ttraining's l1: 0.0191614\tvalid_1's l1: 0.638206\n",
      "[83000]\ttraining's l1: 0.0188464\tvalid_1's l1: 0.638188\n",
      "[83500]\ttraining's l1: 0.018532\tvalid_1's l1: 0.63817\n",
      "[84000]\ttraining's l1: 0.0182315\tvalid_1's l1: 0.638155\n",
      "[84500]\ttraining's l1: 0.0179307\tvalid_1's l1: 0.638142\n",
      "[85000]\ttraining's l1: 0.0176376\tvalid_1's l1: 0.638128\n",
      "[85500]\ttraining's l1: 0.0173528\tvalid_1's l1: 0.638106\n",
      "[86000]\ttraining's l1: 0.0170688\tvalid_1's l1: 0.638089\n",
      "[86500]\ttraining's l1: 0.0167928\tvalid_1's l1: 0.638076\n",
      "[87000]\ttraining's l1: 0.0165206\tvalid_1's l1: 0.63806\n",
      "[87500]\ttraining's l1: 0.0162506\tvalid_1's l1: 0.638049\n",
      "[88000]\ttraining's l1: 0.0159866\tvalid_1's l1: 0.638031\n",
      "[88500]\ttraining's l1: 0.0157312\tvalid_1's l1: 0.638014\n",
      "[89000]\ttraining's l1: 0.0154803\tvalid_1's l1: 0.638002\n",
      "[89500]\ttraining's l1: 0.0152326\tvalid_1's l1: 0.63799\n",
      "[90000]\ttraining's l1: 0.0149865\tvalid_1's l1: 0.637977\n",
      "[90500]\ttraining's l1: 0.0147455\tvalid_1's l1: 0.637966\n",
      "[91000]\ttraining's l1: 0.0145124\tvalid_1's l1: 0.637954\n",
      "[91500]\ttraining's l1: 0.0142808\tvalid_1's l1: 0.63794\n",
      "[92000]\ttraining's l1: 0.0140571\tvalid_1's l1: 0.637927\n",
      "[92500]\ttraining's l1: 0.0138341\tvalid_1's l1: 0.63791\n",
      "[93000]\ttraining's l1: 0.0136141\tvalid_1's l1: 0.637899\n",
      "[93500]\ttraining's l1: 0.0134035\tvalid_1's l1: 0.637886\n",
      "[94000]\ttraining's l1: 0.0131922\tvalid_1's l1: 0.637876\n",
      "[94500]\ttraining's l1: 0.0129861\tvalid_1's l1: 0.637863\n",
      "[95000]\ttraining's l1: 0.0127843\tvalid_1's l1: 0.637855\n",
      "[95500]\ttraining's l1: 0.012588\tvalid_1's l1: 0.637847\n",
      "[96000]\ttraining's l1: 0.0123918\tvalid_1's l1: 0.637833\n",
      "[96500]\ttraining's l1: 0.0121995\tvalid_1's l1: 0.637822\n",
      "[97000]\ttraining's l1: 0.0120144\tvalid_1's l1: 0.637814\n",
      "[97500]\ttraining's l1: 0.0118305\tvalid_1's l1: 0.637803\n",
      "[98000]\ttraining's l1: 0.0116495\tvalid_1's l1: 0.637794\n",
      "[98500]\ttraining's l1: 0.0114713\tvalid_1's l1: 0.637783\n",
      "[99000]\ttraining's l1: 0.0112972\tvalid_1's l1: 0.637773\n",
      "[99500]\ttraining's l1: 0.0111247\tvalid_1's l1: 0.637765\n",
      "[100000]\ttraining's l1: 0.0109555\tvalid_1's l1: 0.637753\n",
      "[100500]\ttraining's l1: 0.0107907\tvalid_1's l1: 0.637745\n",
      "[101000]\ttraining's l1: 0.0106292\tvalid_1's l1: 0.637737\n",
      "[101500]\ttraining's l1: 0.0104683\tvalid_1's l1: 0.637727\n",
      "[102000]\ttraining's l1: 0.0103127\tvalid_1's l1: 0.637715\n",
      "[102500]\ttraining's l1: 0.0101582\tvalid_1's l1: 0.637707\n",
      "[103000]\ttraining's l1: 0.0100067\tvalid_1's l1: 0.637699\n",
      "[103500]\ttraining's l1: 0.00985989\tvalid_1's l1: 0.637693\n",
      "[104000]\ttraining's l1: 0.00971539\tvalid_1's l1: 0.637687\n",
      "[104500]\ttraining's l1: 0.00957359\tvalid_1's l1: 0.637679\n",
      "[105000]\ttraining's l1: 0.00943176\tvalid_1's l1: 0.637671\n",
      "[105500]\ttraining's l1: 0.00929551\tvalid_1's l1: 0.637664\n",
      "[106000]\ttraining's l1: 0.00915932\tvalid_1's l1: 0.637655\n",
      "[106500]\ttraining's l1: 0.00902459\tvalid_1's l1: 0.63765\n",
      "[107000]\ttraining's l1: 0.00889239\tvalid_1's l1: 0.637644\n",
      "[107500]\ttraining's l1: 0.00876338\tvalid_1's l1: 0.63764\n",
      "[108000]\ttraining's l1: 0.00863656\tvalid_1's l1: 0.637633\n",
      "[108500]\ttraining's l1: 0.00851257\tvalid_1's l1: 0.637626\n",
      "[109000]\ttraining's l1: 0.00839047\tvalid_1's l1: 0.637619\n",
      "[109500]\ttraining's l1: 0.00827058\tvalid_1's l1: 0.637613\n",
      "[110000]\ttraining's l1: 0.00815322\tvalid_1's l1: 0.637608\n",
      "[110500]\ttraining's l1: 0.00803657\tvalid_1's l1: 0.637599\n",
      "[111000]\ttraining's l1: 0.00792239\tvalid_1's l1: 0.637591\n",
      "[111500]\ttraining's l1: 0.00781058\tvalid_1's l1: 0.637585\n",
      "[112000]\ttraining's l1: 0.00770152\tvalid_1's l1: 0.637578\n",
      "[112500]\ttraining's l1: 0.0075942\tvalid_1's l1: 0.637572\n",
      "[113000]\ttraining's l1: 0.00748851\tvalid_1's l1: 0.637568\n",
      "[113500]\ttraining's l1: 0.00738498\tvalid_1's l1: 0.637561\n",
      "[114000]\ttraining's l1: 0.00728345\tvalid_1's l1: 0.637556\n",
      "[114500]\ttraining's l1: 0.0071817\tvalid_1's l1: 0.637551\n",
      "[115000]\ttraining's l1: 0.00708342\tvalid_1's l1: 0.637547\n",
      "[115500]\ttraining's l1: 0.00698529\tvalid_1's l1: 0.637541\n",
      "[116000]\ttraining's l1: 0.00689102\tvalid_1's l1: 0.637536\n",
      "[116500]\ttraining's l1: 0.00679757\tvalid_1's l1: 0.637532\n",
      "[117000]\ttraining's l1: 0.00670529\tvalid_1's l1: 0.637527\n",
      "[117500]\ttraining's l1: 0.00661505\tvalid_1's l1: 0.637523\n",
      "[118000]\ttraining's l1: 0.0065255\tvalid_1's l1: 0.63752\n",
      "[118500]\ttraining's l1: 0.00643861\tvalid_1's l1: 0.637514\n",
      "[119000]\ttraining's l1: 0.00635292\tvalid_1's l1: 0.63751\n",
      "[119500]\ttraining's l1: 0.00626764\tvalid_1's l1: 0.637506\n",
      "[120000]\ttraining's l1: 0.00618442\tvalid_1's l1: 0.637499\n",
      "[120500]\ttraining's l1: 0.00610348\tvalid_1's l1: 0.637496\n",
      "[121000]\ttraining's l1: 0.00602321\tvalid_1's l1: 0.637492\n",
      "[121500]\ttraining's l1: 0.00594408\tvalid_1's l1: 0.637486\n",
      "[122000]\ttraining's l1: 0.00586585\tvalid_1's l1: 0.637482\n",
      "[122500]\ttraining's l1: 0.00579072\tvalid_1's l1: 0.637479\n",
      "[123000]\ttraining's l1: 0.00571553\tvalid_1's l1: 0.637475\n",
      "[123500]\ttraining's l1: 0.00564278\tvalid_1's l1: 0.637472\n",
      "[124000]\ttraining's l1: 0.00557005\tvalid_1's l1: 0.637468\n",
      "[124500]\ttraining's l1: 0.00549924\tvalid_1's l1: 0.637464\n",
      "[125000]\ttraining's l1: 0.00542821\tvalid_1's l1: 0.63746\n",
      "[125500]\ttraining's l1: 0.00536029\tvalid_1's l1: 0.637457\n",
      "[126000]\ttraining's l1: 0.00529327\tvalid_1's l1: 0.637452\n",
      "[126500]\ttraining's l1: 0.00522686\tvalid_1's l1: 0.637448\n",
      "[127000]\ttraining's l1: 0.00516075\tvalid_1's l1: 0.637446\n",
      "[127500]\ttraining's l1: 0.00509727\tvalid_1's l1: 0.637442\n",
      "[128000]\ttraining's l1: 0.00503392\tvalid_1's l1: 0.63744\n",
      "[128500]\ttraining's l1: 0.00497177\tvalid_1's l1: 0.637436\n",
      "[129000]\ttraining's l1: 0.00491077\tvalid_1's l1: 0.637433\n",
      "[129500]\ttraining's l1: 0.00485086\tvalid_1's l1: 0.637431\n",
      "[130000]\ttraining's l1: 0.00479178\tvalid_1's l1: 0.637428\n",
      "[130500]\ttraining's l1: 0.00473415\tvalid_1's l1: 0.637426\n",
      "[131000]\ttraining's l1: 0.00467714\tvalid_1's l1: 0.637423\n",
      "[131500]\ttraining's l1: 0.00462071\tvalid_1's l1: 0.63742\n",
      "[132000]\ttraining's l1: 0.00456522\tvalid_1's l1: 0.637418\n",
      "[132500]\ttraining's l1: 0.0045106\tvalid_1's l1: 0.637415\n",
      "[133000]\ttraining's l1: 0.0044577\tvalid_1's l1: 0.637411\n",
      "[133500]\ttraining's l1: 0.0044058\tvalid_1's l1: 0.637409\n",
      "[134000]\ttraining's l1: 0.00435424\tvalid_1's l1: 0.637406\n",
      "[134500]\ttraining's l1: 0.00430331\tvalid_1's l1: 0.637403\n",
      "[135000]\ttraining's l1: 0.00425304\tvalid_1's l1: 0.6374\n",
      "[135500]\ttraining's l1: 0.00420376\tvalid_1's l1: 0.637399\n",
      "[136000]\ttraining's l1: 0.00415586\tvalid_1's l1: 0.637397\n",
      "[136500]\ttraining's l1: 0.00410817\tvalid_1's l1: 0.637395\n",
      "[137000]\ttraining's l1: 0.00406108\tvalid_1's l1: 0.637393\n",
      "[137500]\ttraining's l1: 0.00401532\tvalid_1's l1: 0.637391\n",
      "[138000]\ttraining's l1: 0.00396954\tvalid_1's l1: 0.637389\n",
      "[138500]\ttraining's l1: 0.00392485\tvalid_1's l1: 0.637387\n",
      "[139000]\ttraining's l1: 0.00388073\tvalid_1's l1: 0.637385\n",
      "[139500]\ttraining's l1: 0.00383713\tvalid_1's l1: 0.637382\n",
      "[140000]\ttraining's l1: 0.0037946\tvalid_1's l1: 0.63738\n",
      "[140500]\ttraining's l1: 0.00375256\tvalid_1's l1: 0.637378\n",
      "[141000]\ttraining's l1: 0.00371111\tvalid_1's l1: 0.637376\n",
      "[141500]\ttraining's l1: 0.00367058\tvalid_1's l1: 0.637374\n",
      "[142000]\ttraining's l1: 0.00363047\tvalid_1's l1: 0.637372\n",
      "[142500]\ttraining's l1: 0.00359142\tvalid_1's l1: 0.63737\n",
      "[143000]\ttraining's l1: 0.00355235\tvalid_1's l1: 0.637368\n",
      "[143500]\ttraining's l1: 0.00351383\tvalid_1's l1: 0.637367\n",
      "[144000]\ttraining's l1: 0.00347665\tvalid_1's l1: 0.637365\n",
      "[144500]\ttraining's l1: 0.00343991\tvalid_1's l1: 0.637363\n",
      "[145000]\ttraining's l1: 0.00340365\tvalid_1's l1: 0.637361\n",
      "[145500]\ttraining's l1: 0.00336754\tvalid_1's l1: 0.637359\n",
      "[146000]\ttraining's l1: 0.003332\tvalid_1's l1: 0.637357\n",
      "[146500]\ttraining's l1: 0.00329742\tvalid_1's l1: 0.637355\n",
      "[147000]\ttraining's l1: 0.0032632\tvalid_1's l1: 0.637354\n",
      "[147500]\ttraining's l1: 0.00323014\tvalid_1's l1: 0.637352\n",
      "[148000]\ttraining's l1: 0.00319748\tvalid_1's l1: 0.63735\n",
      "[148500]\ttraining's l1: 0.00316461\tvalid_1's l1: 0.637348\n",
      "[149000]\ttraining's l1: 0.00313224\tvalid_1's l1: 0.637346\n",
      "[149500]\ttraining's l1: 0.00310098\tvalid_1's l1: 0.637345\n",
      "[150000]\ttraining's l1: 0.00306983\tvalid_1's l1: 0.637343\n",
      "[150500]\ttraining's l1: 0.0030392\tvalid_1's l1: 0.637342\n",
      "[151000]\ttraining's l1: 0.00300909\tvalid_1's l1: 0.63734\n",
      "[151500]\ttraining's l1: 0.00297952\tvalid_1's l1: 0.637339\n",
      "[152000]\ttraining's l1: 0.00295028\tvalid_1's l1: 0.637337\n",
      "[152500]\ttraining's l1: 0.00292131\tvalid_1's l1: 0.637336\n",
      "[153000]\ttraining's l1: 0.00289315\tvalid_1's l1: 0.637334\n",
      "[153500]\ttraining's l1: 0.00286538\tvalid_1's l1: 0.637332\n",
      "[154000]\ttraining's l1: 0.00283772\tvalid_1's l1: 0.637331\n",
      "[154500]\ttraining's l1: 0.00281081\tvalid_1's l1: 0.63733\n",
      "[155000]\ttraining's l1: 0.00278375\tvalid_1's l1: 0.637329\n",
      "[155500]\ttraining's l1: 0.00275769\tvalid_1's l1: 0.637328\n",
      "[156000]\ttraining's l1: 0.00273189\tvalid_1's l1: 0.637326\n",
      "[156500]\ttraining's l1: 0.00270627\tvalid_1's l1: 0.637325\n",
      "[157000]\ttraining's l1: 0.00268132\tvalid_1's l1: 0.637324\n",
      "[157500]\ttraining's l1: 0.00265656\tvalid_1's l1: 0.637322\n",
      "Early stopping, best iteration is:\n",
      "[157648]\ttraining's l1: 0.00264927\tvalid_1's l1: 0.637322\n",
      "Fold 4 started at Tue Jun 25 06:21:38 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.942252\tvalid_1's l1: 0.985239\n",
      "[1000]\ttraining's l1: 0.82544\tvalid_1's l1: 0.897529\n",
      "[1500]\ttraining's l1: 0.752213\tvalid_1's l1: 0.848571\n",
      "[2000]\ttraining's l1: 0.699515\tvalid_1's l1: 0.817705\n",
      "[2500]\ttraining's l1: 0.658225\tvalid_1's l1: 0.795897\n",
      "[3000]\ttraining's l1: 0.623068\tvalid_1's l1: 0.778836\n",
      "[3500]\ttraining's l1: 0.593308\tvalid_1's l1: 0.765858\n",
      "[4000]\ttraining's l1: 0.566131\tvalid_1's l1: 0.754285\n",
      "[4500]\ttraining's l1: 0.542099\tvalid_1's l1: 0.745041\n",
      "[5000]\ttraining's l1: 0.519883\tvalid_1's l1: 0.736876\n",
      "[5500]\ttraining's l1: 0.500156\tvalid_1's l1: 0.730402\n",
      "[6000]\ttraining's l1: 0.481515\tvalid_1's l1: 0.724278\n",
      "[6500]\ttraining's l1: 0.464579\tvalid_1's l1: 0.719193\n",
      "[7000]\ttraining's l1: 0.448435\tvalid_1's l1: 0.714472\n",
      "[7500]\ttraining's l1: 0.433384\tvalid_1's l1: 0.710326\n",
      "[8000]\ttraining's l1: 0.419295\tvalid_1's l1: 0.706541\n",
      "[8500]\ttraining's l1: 0.405857\tvalid_1's l1: 0.702968\n",
      "[9000]\ttraining's l1: 0.393117\tvalid_1's l1: 0.699663\n",
      "[9500]\ttraining's l1: 0.380895\tvalid_1's l1: 0.696539\n",
      "[10000]\ttraining's l1: 0.369369\tvalid_1's l1: 0.69357\n",
      "[10500]\ttraining's l1: 0.358412\tvalid_1's l1: 0.691042\n",
      "[11000]\ttraining's l1: 0.347942\tvalid_1's l1: 0.688774\n",
      "[11500]\ttraining's l1: 0.337894\tvalid_1's l1: 0.686433\n",
      "[12000]\ttraining's l1: 0.328293\tvalid_1's l1: 0.684237\n",
      "[12500]\ttraining's l1: 0.319254\tvalid_1's l1: 0.682444\n",
      "[13000]\ttraining's l1: 0.310477\tvalid_1's l1: 0.680698\n",
      "[13500]\ttraining's l1: 0.302101\tvalid_1's l1: 0.679135\n",
      "[14000]\ttraining's l1: 0.293889\tvalid_1's l1: 0.677532\n",
      "[14500]\ttraining's l1: 0.286045\tvalid_1's l1: 0.676119\n",
      "[15000]\ttraining's l1: 0.278468\tvalid_1's l1: 0.674767\n",
      "[15500]\ttraining's l1: 0.271264\tvalid_1's l1: 0.673448\n",
      "[16000]\ttraining's l1: 0.264332\tvalid_1's l1: 0.67217\n",
      "[16500]\ttraining's l1: 0.257584\tvalid_1's l1: 0.670994\n",
      "[17000]\ttraining's l1: 0.251059\tvalid_1's l1: 0.669804\n",
      "[17500]\ttraining's l1: 0.24474\tvalid_1's l1: 0.668654\n",
      "[18000]\ttraining's l1: 0.238627\tvalid_1's l1: 0.667635\n",
      "[18500]\ttraining's l1: 0.232769\tvalid_1's l1: 0.666652\n",
      "[19000]\ttraining's l1: 0.22707\tvalid_1's l1: 0.665776\n",
      "[19500]\ttraining's l1: 0.221536\tvalid_1's l1: 0.664977\n",
      "[20000]\ttraining's l1: 0.216246\tvalid_1's l1: 0.664298\n",
      "[20500]\ttraining's l1: 0.211018\tvalid_1's l1: 0.663501\n",
      "[21000]\ttraining's l1: 0.206027\tvalid_1's l1: 0.662805\n",
      "[21500]\ttraining's l1: 0.201174\tvalid_1's l1: 0.662054\n",
      "[22000]\ttraining's l1: 0.196521\tvalid_1's l1: 0.661448\n",
      "[22500]\ttraining's l1: 0.191993\tvalid_1's l1: 0.660759\n",
      "[23000]\ttraining's l1: 0.187571\tvalid_1's l1: 0.66018\n",
      "[23500]\ttraining's l1: 0.183308\tvalid_1's l1: 0.659576\n",
      "[24000]\ttraining's l1: 0.179124\tvalid_1's l1: 0.659013\n",
      "[24500]\ttraining's l1: 0.175044\tvalid_1's l1: 0.658462\n",
      "[25000]\ttraining's l1: 0.171049\tvalid_1's l1: 0.657904\n",
      "[25500]\ttraining's l1: 0.167212\tvalid_1's l1: 0.657367\n",
      "[26000]\ttraining's l1: 0.163481\tvalid_1's l1: 0.656916\n",
      "[26500]\ttraining's l1: 0.159828\tvalid_1's l1: 0.65645\n",
      "[27000]\ttraining's l1: 0.156301\tvalid_1's l1: 0.656023\n",
      "[27500]\ttraining's l1: 0.152866\tvalid_1's l1: 0.655567\n",
      "[28000]\ttraining's l1: 0.149495\tvalid_1's l1: 0.655187\n",
      "[28500]\ttraining's l1: 0.14628\tvalid_1's l1: 0.654746\n",
      "[29000]\ttraining's l1: 0.143115\tvalid_1's l1: 0.654347\n",
      "[29500]\ttraining's l1: 0.14008\tvalid_1's l1: 0.653976\n",
      "[30000]\ttraining's l1: 0.137104\tvalid_1's l1: 0.653602\n",
      "[30500]\ttraining's l1: 0.134194\tvalid_1's l1: 0.65328\n",
      "[31000]\ttraining's l1: 0.131307\tvalid_1's l1: 0.652955\n",
      "[31500]\ttraining's l1: 0.128483\tvalid_1's l1: 0.652647\n",
      "[32000]\ttraining's l1: 0.12577\tvalid_1's l1: 0.652336\n",
      "[32500]\ttraining's l1: 0.123143\tvalid_1's l1: 0.652045\n",
      "[33000]\ttraining's l1: 0.120563\tvalid_1's l1: 0.651724\n",
      "[33500]\ttraining's l1: 0.118052\tvalid_1's l1: 0.651414\n",
      "[34000]\ttraining's l1: 0.115594\tvalid_1's l1: 0.651124\n",
      "[34500]\ttraining's l1: 0.113205\tvalid_1's l1: 0.650885\n",
      "[35000]\ttraining's l1: 0.11087\tvalid_1's l1: 0.650648\n",
      "[35500]\ttraining's l1: 0.108563\tvalid_1's l1: 0.650377\n",
      "[36000]\ttraining's l1: 0.10634\tvalid_1's l1: 0.650151\n",
      "[36500]\ttraining's l1: 0.104166\tvalid_1's l1: 0.649904\n",
      "[37000]\ttraining's l1: 0.102046\tvalid_1's l1: 0.6497\n",
      "[37500]\ttraining's l1: 0.0999549\tvalid_1's l1: 0.649499\n",
      "[38000]\ttraining's l1: 0.0979472\tvalid_1's l1: 0.649298\n",
      "[38500]\ttraining's l1: 0.0959576\tvalid_1's l1: 0.649078\n",
      "[39000]\ttraining's l1: 0.0940291\tvalid_1's l1: 0.648859\n",
      "[39500]\ttraining's l1: 0.0921438\tvalid_1's l1: 0.648654\n",
      "[40000]\ttraining's l1: 0.0903131\tvalid_1's l1: 0.648497\n",
      "[40500]\ttraining's l1: 0.0885151\tvalid_1's l1: 0.648329\n",
      "[41000]\ttraining's l1: 0.0867512\tvalid_1's l1: 0.648152\n",
      "[41500]\ttraining's l1: 0.0850279\tvalid_1's l1: 0.647994\n",
      "[42000]\ttraining's l1: 0.0833636\tvalid_1's l1: 0.647838\n",
      "[42500]\ttraining's l1: 0.0816948\tvalid_1's l1: 0.647657\n",
      "[43000]\ttraining's l1: 0.0800789\tvalid_1's l1: 0.64752\n",
      "[43500]\ttraining's l1: 0.0784866\tvalid_1's l1: 0.64737\n",
      "[44000]\ttraining's l1: 0.076936\tvalid_1's l1: 0.647251\n",
      "[44500]\ttraining's l1: 0.0754035\tvalid_1's l1: 0.647098\n",
      "[45000]\ttraining's l1: 0.0739556\tvalid_1's l1: 0.647023\n",
      "[45500]\ttraining's l1: 0.0725448\tvalid_1's l1: 0.646904\n",
      "[46000]\ttraining's l1: 0.071153\tvalid_1's l1: 0.64678\n",
      "[46500]\ttraining's l1: 0.0697892\tvalid_1's l1: 0.646668\n",
      "[47000]\ttraining's l1: 0.0684362\tvalid_1's l1: 0.646583\n",
      "[47500]\ttraining's l1: 0.0671251\tvalid_1's l1: 0.646454\n",
      "[48000]\ttraining's l1: 0.0658354\tvalid_1's l1: 0.646346\n",
      "[48500]\ttraining's l1: 0.0645741\tvalid_1's l1: 0.64625\n",
      "[49000]\ttraining's l1: 0.0633375\tvalid_1's l1: 0.646147\n",
      "[49500]\ttraining's l1: 0.0621427\tvalid_1's l1: 0.646052\n",
      "[50000]\ttraining's l1: 0.0609605\tvalid_1's l1: 0.645945\n",
      "[50500]\ttraining's l1: 0.0597999\tvalid_1's l1: 0.645824\n",
      "[51000]\ttraining's l1: 0.0586649\tvalid_1's l1: 0.645722\n",
      "[51500]\ttraining's l1: 0.0575536\tvalid_1's l1: 0.645639\n",
      "[52000]\ttraining's l1: 0.0564686\tvalid_1's l1: 0.645554\n",
      "[52500]\ttraining's l1: 0.0554308\tvalid_1's l1: 0.645474\n",
      "[53000]\ttraining's l1: 0.0544053\tvalid_1's l1: 0.645383\n",
      "[53500]\ttraining's l1: 0.0533886\tvalid_1's l1: 0.645301\n",
      "[54000]\ttraining's l1: 0.0523889\tvalid_1's l1: 0.645223\n",
      "[54500]\ttraining's l1: 0.0514155\tvalid_1's l1: 0.645154\n",
      "[55000]\ttraining's l1: 0.0504665\tvalid_1's l1: 0.645073\n",
      "[55500]\ttraining's l1: 0.0495439\tvalid_1's l1: 0.644984\n",
      "[56000]\ttraining's l1: 0.0486307\tvalid_1's l1: 0.644928\n",
      "[56500]\ttraining's l1: 0.0477277\tvalid_1's l1: 0.644858\n",
      "[57000]\ttraining's l1: 0.0468259\tvalid_1's l1: 0.644794\n",
      "[57500]\ttraining's l1: 0.0459523\tvalid_1's l1: 0.644739\n",
      "[58000]\ttraining's l1: 0.0451062\tvalid_1's l1: 0.64466\n",
      "[58500]\ttraining's l1: 0.0442847\tvalid_1's l1: 0.644597\n",
      "[59000]\ttraining's l1: 0.0434782\tvalid_1's l1: 0.644531\n",
      "[59500]\ttraining's l1: 0.0426819\tvalid_1's l1: 0.644469\n",
      "[60000]\ttraining's l1: 0.0419077\tvalid_1's l1: 0.644409\n",
      "[60500]\ttraining's l1: 0.0411539\tvalid_1's l1: 0.644352\n",
      "[61000]\ttraining's l1: 0.0404067\tvalid_1's l1: 0.644298\n",
      "[61500]\ttraining's l1: 0.0396834\tvalid_1's l1: 0.644255\n",
      "[62000]\ttraining's l1: 0.0389663\tvalid_1's l1: 0.644199\n",
      "[62500]\ttraining's l1: 0.0382657\tvalid_1's l1: 0.644159\n",
      "[63000]\ttraining's l1: 0.0375838\tvalid_1's l1: 0.644113\n",
      "[63500]\ttraining's l1: 0.0369141\tvalid_1's l1: 0.644082\n",
      "[64000]\ttraining's l1: 0.036259\tvalid_1's l1: 0.644035\n",
      "[64500]\ttraining's l1: 0.035611\tvalid_1's l1: 0.643977\n",
      "[65000]\ttraining's l1: 0.0349795\tvalid_1's l1: 0.643938\n",
      "[65500]\ttraining's l1: 0.0343617\tvalid_1's l1: 0.64388\n",
      "[66000]\ttraining's l1: 0.0337519\tvalid_1's l1: 0.643838\n",
      "[66500]\ttraining's l1: 0.0331548\tvalid_1's l1: 0.643795\n",
      "[67000]\ttraining's l1: 0.032572\tvalid_1's l1: 0.643764\n",
      "[67500]\ttraining's l1: 0.0320023\tvalid_1's l1: 0.643721\n",
      "[68000]\ttraining's l1: 0.0314462\tvalid_1's l1: 0.643682\n",
      "[68500]\ttraining's l1: 0.0308959\tvalid_1's l1: 0.643637\n",
      "[69000]\ttraining's l1: 0.030352\tvalid_1's l1: 0.643597\n",
      "[69500]\ttraining's l1: 0.0298246\tvalid_1's l1: 0.643565\n",
      "[70000]\ttraining's l1: 0.0293095\tvalid_1's l1: 0.643521\n",
      "[70500]\ttraining's l1: 0.0287994\tvalid_1's l1: 0.643488\n",
      "[71000]\ttraining's l1: 0.0282979\tvalid_1's l1: 0.643455\n",
      "[71500]\ttraining's l1: 0.0278133\tvalid_1's l1: 0.643427\n",
      "[72000]\ttraining's l1: 0.0273386\tvalid_1's l1: 0.643399\n",
      "[72500]\ttraining's l1: 0.026869\tvalid_1's l1: 0.643371\n",
      "[73000]\ttraining's l1: 0.0264069\tvalid_1's l1: 0.643341\n",
      "[73500]\ttraining's l1: 0.0259575\tvalid_1's l1: 0.643325\n",
      "[74000]\ttraining's l1: 0.0255109\tvalid_1's l1: 0.643307\n",
      "[74500]\ttraining's l1: 0.0250743\tvalid_1's l1: 0.643274\n",
      "[75000]\ttraining's l1: 0.0246471\tvalid_1's l1: 0.64325\n",
      "[75500]\ttraining's l1: 0.0242227\tvalid_1's l1: 0.643223\n",
      "[76000]\ttraining's l1: 0.0238099\tvalid_1's l1: 0.643198\n",
      "[76500]\ttraining's l1: 0.0234063\tvalid_1's l1: 0.643176\n",
      "[77000]\ttraining's l1: 0.0230079\tvalid_1's l1: 0.64315\n",
      "[77500]\ttraining's l1: 0.0226199\tvalid_1's l1: 0.643124\n",
      "[78000]\ttraining's l1: 0.0222384\tvalid_1's l1: 0.643094\n",
      "[78500]\ttraining's l1: 0.0218678\tvalid_1's l1: 0.64307\n",
      "[79000]\ttraining's l1: 0.0215023\tvalid_1's l1: 0.643046\n",
      "[79500]\ttraining's l1: 0.0211398\tvalid_1's l1: 0.643024\n",
      "[80000]\ttraining's l1: 0.020788\tvalid_1's l1: 0.642997\n",
      "[80500]\ttraining's l1: 0.0204433\tvalid_1's l1: 0.642975\n",
      "[81000]\ttraining's l1: 0.0201024\tvalid_1's l1: 0.642955\n",
      "[81500]\ttraining's l1: 0.0197684\tvalid_1's l1: 0.642937\n",
      "[82000]\ttraining's l1: 0.0194421\tvalid_1's l1: 0.642909\n",
      "[82500]\ttraining's l1: 0.0191236\tvalid_1's l1: 0.642886\n",
      "[83000]\ttraining's l1: 0.0188115\tvalid_1's l1: 0.642865\n",
      "[83500]\ttraining's l1: 0.0185028\tvalid_1's l1: 0.642847\n",
      "[84000]\ttraining's l1: 0.0181999\tvalid_1's l1: 0.642827\n",
      "[84500]\ttraining's l1: 0.017903\tvalid_1's l1: 0.642812\n",
      "[85000]\ttraining's l1: 0.0176086\tvalid_1's l1: 0.642794\n",
      "[85500]\ttraining's l1: 0.0173229\tvalid_1's l1: 0.642782\n",
      "[86000]\ttraining's l1: 0.0170446\tvalid_1's l1: 0.642764\n",
      "[86500]\ttraining's l1: 0.0167671\tvalid_1's l1: 0.642745\n",
      "[87000]\ttraining's l1: 0.0164959\tvalid_1's l1: 0.64273\n",
      "[87500]\ttraining's l1: 0.0162308\tvalid_1's l1: 0.642717\n",
      "[88000]\ttraining's l1: 0.0159683\tvalid_1's l1: 0.642702\n",
      "[88500]\ttraining's l1: 0.0157121\tvalid_1's l1: 0.642691\n",
      "[89000]\ttraining's l1: 0.0154629\tvalid_1's l1: 0.642674\n",
      "[89500]\ttraining's l1: 0.0152171\tvalid_1's l1: 0.642658\n",
      "[90000]\ttraining's l1: 0.0149745\tvalid_1's l1: 0.642641\n",
      "[90500]\ttraining's l1: 0.0147329\tvalid_1's l1: 0.642625\n",
      "[91000]\ttraining's l1: 0.0144981\tvalid_1's l1: 0.642612\n",
      "[91500]\ttraining's l1: 0.0142666\tvalid_1's l1: 0.642599\n",
      "[92000]\ttraining's l1: 0.0140388\tvalid_1's l1: 0.642588\n",
      "[92500]\ttraining's l1: 0.0138185\tvalid_1's l1: 0.642569\n",
      "[93000]\ttraining's l1: 0.0136025\tvalid_1's l1: 0.642553\n",
      "[93500]\ttraining's l1: 0.013389\tvalid_1's l1: 0.642542\n",
      "[94000]\ttraining's l1: 0.0131774\tvalid_1's l1: 0.642529\n",
      "[94500]\ttraining's l1: 0.0129718\tvalid_1's l1: 0.64252\n",
      "[95000]\ttraining's l1: 0.0127697\tvalid_1's l1: 0.642511\n",
      "[95500]\ttraining's l1: 0.0125734\tvalid_1's l1: 0.642502\n",
      "[96000]\ttraining's l1: 0.0123793\tvalid_1's l1: 0.642493\n",
      "[96500]\ttraining's l1: 0.0121891\tvalid_1's l1: 0.642485\n",
      "[97000]\ttraining's l1: 0.0120026\tvalid_1's l1: 0.642472\n",
      "[97500]\ttraining's l1: 0.0118199\tvalid_1's l1: 0.642464\n",
      "[98000]\ttraining's l1: 0.0116405\tvalid_1's l1: 0.642454\n",
      "[98500]\ttraining's l1: 0.0114606\tvalid_1's l1: 0.642443\n",
      "[99000]\ttraining's l1: 0.0112867\tvalid_1's l1: 0.642434\n",
      "[99500]\ttraining's l1: 0.0111149\tvalid_1's l1: 0.642427\n",
      "[100000]\ttraining's l1: 0.0109473\tvalid_1's l1: 0.642416\n",
      "[100500]\ttraining's l1: 0.010783\tvalid_1's l1: 0.642409\n",
      "[101000]\ttraining's l1: 0.0106203\tvalid_1's l1: 0.642401\n",
      "[101500]\ttraining's l1: 0.0104604\tvalid_1's l1: 0.642389\n",
      "[102000]\ttraining's l1: 0.0103047\tvalid_1's l1: 0.642384\n",
      "[102500]\ttraining's l1: 0.0101513\tvalid_1's l1: 0.642373\n",
      "[103000]\ttraining's l1: 0.0100001\tvalid_1's l1: 0.642367\n",
      "[103500]\ttraining's l1: 0.0098507\tvalid_1's l1: 0.642359\n",
      "[104000]\ttraining's l1: 0.00970406\tvalid_1's l1: 0.642349\n",
      "[104500]\ttraining's l1: 0.00956001\tvalid_1's l1: 0.64234\n",
      "[105000]\ttraining's l1: 0.00941846\tvalid_1's l1: 0.642333\n",
      "[105500]\ttraining's l1: 0.00927961\tvalid_1's l1: 0.642328\n",
      "[106000]\ttraining's l1: 0.00914407\tvalid_1's l1: 0.642321\n",
      "[106500]\ttraining's l1: 0.00901181\tvalid_1's l1: 0.642316\n",
      "[107000]\ttraining's l1: 0.00888134\tvalid_1's l1: 0.642308\n",
      "[107500]\ttraining's l1: 0.00875237\tvalid_1's l1: 0.642301\n",
      "[108000]\ttraining's l1: 0.00862585\tvalid_1's l1: 0.642292\n",
      "[108500]\ttraining's l1: 0.00850031\tvalid_1's l1: 0.642287\n",
      "[109000]\ttraining's l1: 0.00837847\tvalid_1's l1: 0.64228\n",
      "[109500]\ttraining's l1: 0.00825878\tvalid_1's l1: 0.642276\n",
      "[110000]\ttraining's l1: 0.00813945\tvalid_1's l1: 0.642269\n",
      "[110500]\ttraining's l1: 0.00802511\tvalid_1's l1: 0.642265\n",
      "[111000]\ttraining's l1: 0.00791117\tvalid_1's l1: 0.642259\n",
      "[111500]\ttraining's l1: 0.00780113\tvalid_1's l1: 0.642254\n",
      "[112000]\ttraining's l1: 0.00769156\tvalid_1's l1: 0.642247\n",
      "[112500]\ttraining's l1: 0.00758487\tvalid_1's l1: 0.64224\n",
      "[113000]\ttraining's l1: 0.00748063\tvalid_1's l1: 0.642233\n",
      "[113500]\ttraining's l1: 0.00737664\tvalid_1's l1: 0.642227\n",
      "[114000]\ttraining's l1: 0.00727272\tvalid_1's l1: 0.642222\n",
      "[114500]\ttraining's l1: 0.00717326\tvalid_1's l1: 0.642215\n",
      "[115000]\ttraining's l1: 0.00707317\tvalid_1's l1: 0.642209\n",
      "[115500]\ttraining's l1: 0.00697597\tvalid_1's l1: 0.642205\n",
      "[116000]\ttraining's l1: 0.00688094\tvalid_1's l1: 0.6422\n",
      "[116500]\ttraining's l1: 0.0067875\tvalid_1's l1: 0.642194\n",
      "[117000]\ttraining's l1: 0.00669624\tvalid_1's l1: 0.642189\n",
      "[117500]\ttraining's l1: 0.00660599\tvalid_1's l1: 0.642186\n",
      "[118000]\ttraining's l1: 0.00651556\tvalid_1's l1: 0.642182\n",
      "[118500]\ttraining's l1: 0.00642842\tvalid_1's l1: 0.642177\n",
      "[119000]\ttraining's l1: 0.00634319\tvalid_1's l1: 0.642172\n",
      "[119500]\ttraining's l1: 0.00625882\tvalid_1's l1: 0.642167\n",
      "[120000]\ttraining's l1: 0.00617545\tvalid_1's l1: 0.642162\n",
      "[120500]\ttraining's l1: 0.00609356\tvalid_1's l1: 0.642157\n",
      "[121000]\ttraining's l1: 0.00601365\tvalid_1's l1: 0.642153\n",
      "[121500]\ttraining's l1: 0.00593443\tvalid_1's l1: 0.64215\n",
      "[122000]\ttraining's l1: 0.00585782\tvalid_1's l1: 0.642147\n",
      "[122500]\ttraining's l1: 0.0057814\tvalid_1's l1: 0.642143\n",
      "[123000]\ttraining's l1: 0.00570675\tvalid_1's l1: 0.64214\n",
      "[123500]\ttraining's l1: 0.00563357\tvalid_1's l1: 0.642135\n",
      "[124000]\ttraining's l1: 0.0055614\tvalid_1's l1: 0.642131\n",
      "[124500]\ttraining's l1: 0.00549049\tvalid_1's l1: 0.642128\n",
      "[125000]\ttraining's l1: 0.00542053\tvalid_1's l1: 0.642126\n",
      "[125500]\ttraining's l1: 0.00535214\tvalid_1's l1: 0.642122\n",
      "[126000]\ttraining's l1: 0.00528467\tvalid_1's l1: 0.642118\n",
      "[126500]\ttraining's l1: 0.00521811\tvalid_1's l1: 0.642115\n",
      "[127000]\ttraining's l1: 0.00515283\tvalid_1's l1: 0.642112\n",
      "[127500]\ttraining's l1: 0.00508807\tvalid_1's l1: 0.642108\n",
      "[128000]\ttraining's l1: 0.00502468\tvalid_1's l1: 0.642105\n",
      "[128500]\ttraining's l1: 0.00496313\tvalid_1's l1: 0.642102\n",
      "[129000]\ttraining's l1: 0.00490214\tvalid_1's l1: 0.642098\n",
      "[129500]\ttraining's l1: 0.00484257\tvalid_1's l1: 0.642095\n",
      "[130000]\ttraining's l1: 0.00478333\tvalid_1's l1: 0.642092\n",
      "[130500]\ttraining's l1: 0.00472562\tvalid_1's l1: 0.642089\n",
      "[131000]\ttraining's l1: 0.00466894\tvalid_1's l1: 0.642085\n",
      "[131500]\ttraining's l1: 0.00461292\tvalid_1's l1: 0.642083\n",
      "[132000]\ttraining's l1: 0.00455732\tvalid_1's l1: 0.64208\n",
      "[132500]\ttraining's l1: 0.00450299\tvalid_1's l1: 0.642076\n",
      "[133000]\ttraining's l1: 0.00444931\tvalid_1's l1: 0.642073\n",
      "[133500]\ttraining's l1: 0.00439683\tvalid_1's l1: 0.64207\n",
      "[134000]\ttraining's l1: 0.00434499\tvalid_1's l1: 0.642068\n",
      "[134500]\ttraining's l1: 0.00429489\tvalid_1's l1: 0.642066\n",
      "[135000]\ttraining's l1: 0.00424539\tvalid_1's l1: 0.642064\n",
      "[135500]\ttraining's l1: 0.00419627\tvalid_1's l1: 0.64206\n",
      "[136000]\ttraining's l1: 0.00414795\tvalid_1's l1: 0.642058\n",
      "[136500]\ttraining's l1: 0.00410011\tvalid_1's l1: 0.642056\n",
      "[137000]\ttraining's l1: 0.00405394\tvalid_1's l1: 0.642053\n",
      "[137500]\ttraining's l1: 0.00400783\tvalid_1's l1: 0.642051\n",
      "[138000]\ttraining's l1: 0.00396245\tvalid_1's l1: 0.642048\n",
      "[138500]\ttraining's l1: 0.00391793\tvalid_1's l1: 0.642047\n",
      "[139000]\ttraining's l1: 0.00387421\tvalid_1's l1: 0.642044\n",
      "[139500]\ttraining's l1: 0.00383137\tvalid_1's l1: 0.642043\n",
      "[140000]\ttraining's l1: 0.00378931\tvalid_1's l1: 0.642041\n",
      "[140500]\ttraining's l1: 0.00374764\tvalid_1's l1: 0.642038\n",
      "Early stopping, best iteration is:\n",
      "[140754]\ttraining's l1: 0.0037264\tvalid_1's l1: 0.642038\n",
      "Fold 5 started at Tue Jun 25 08:28:19 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.942064\tvalid_1's l1: 0.978804\n",
      "[1000]\ttraining's l1: 0.824709\tvalid_1's l1: 0.889861\n",
      "[1500]\ttraining's l1: 0.751462\tvalid_1's l1: 0.840345\n",
      "[2000]\ttraining's l1: 0.699458\tvalid_1's l1: 0.810174\n",
      "[2500]\ttraining's l1: 0.657342\tvalid_1's l1: 0.787992\n",
      "[3000]\ttraining's l1: 0.623117\tvalid_1's l1: 0.771995\n",
      "[3500]\ttraining's l1: 0.593384\tvalid_1's l1: 0.759279\n",
      "[4000]\ttraining's l1: 0.56676\tvalid_1's l1: 0.747879\n",
      "[4500]\ttraining's l1: 0.542809\tvalid_1's l1: 0.738643\n",
      "[5000]\ttraining's l1: 0.520841\tvalid_1's l1: 0.73089\n",
      "[5500]\ttraining's l1: 0.500744\tvalid_1's l1: 0.724058\n",
      "[6000]\ttraining's l1: 0.482179\tvalid_1's l1: 0.718183\n",
      "[6500]\ttraining's l1: 0.464868\tvalid_1's l1: 0.712823\n",
      "[7000]\ttraining's l1: 0.448712\tvalid_1's l1: 0.70794\n",
      "[7500]\ttraining's l1: 0.433569\tvalid_1's l1: 0.703742\n",
      "[8000]\ttraining's l1: 0.41968\tvalid_1's l1: 0.700017\n",
      "[8500]\ttraining's l1: 0.406302\tvalid_1's l1: 0.696566\n",
      "[9000]\ttraining's l1: 0.393373\tvalid_1's l1: 0.693319\n",
      "[9500]\ttraining's l1: 0.381409\tvalid_1's l1: 0.690479\n",
      "[10000]\ttraining's l1: 0.369979\tvalid_1's l1: 0.687797\n",
      "[10500]\ttraining's l1: 0.358973\tvalid_1's l1: 0.685264\n",
      "[11000]\ttraining's l1: 0.348527\tvalid_1's l1: 0.682971\n",
      "[11500]\ttraining's l1: 0.338578\tvalid_1's l1: 0.680863\n",
      "[12000]\ttraining's l1: 0.32901\tvalid_1's l1: 0.678812\n",
      "[12500]\ttraining's l1: 0.319762\tvalid_1's l1: 0.676857\n",
      "[13000]\ttraining's l1: 0.310932\tvalid_1's l1: 0.675192\n",
      "[13500]\ttraining's l1: 0.302597\tvalid_1's l1: 0.673625\n",
      "[14000]\ttraining's l1: 0.294454\tvalid_1's l1: 0.672131\n",
      "[14500]\ttraining's l1: 0.286623\tvalid_1's l1: 0.670707\n",
      "[15000]\ttraining's l1: 0.279199\tvalid_1's l1: 0.669361\n",
      "[15500]\ttraining's l1: 0.271989\tvalid_1's l1: 0.667951\n",
      "[16000]\ttraining's l1: 0.265018\tvalid_1's l1: 0.666742\n",
      "[16500]\ttraining's l1: 0.258286\tvalid_1's l1: 0.665624\n",
      "[17000]\ttraining's l1: 0.251771\tvalid_1's l1: 0.664563\n",
      "[17500]\ttraining's l1: 0.245514\tvalid_1's l1: 0.663574\n",
      "[18000]\ttraining's l1: 0.239475\tvalid_1's l1: 0.662558\n",
      "[18500]\ttraining's l1: 0.233623\tvalid_1's l1: 0.661563\n",
      "[19000]\ttraining's l1: 0.227899\tvalid_1's l1: 0.660689\n",
      "[19500]\ttraining's l1: 0.222382\tvalid_1's l1: 0.659833\n",
      "[20000]\ttraining's l1: 0.217079\tvalid_1's l1: 0.658997\n",
      "[20500]\ttraining's l1: 0.211871\tvalid_1's l1: 0.658189\n",
      "[21000]\ttraining's l1: 0.206825\tvalid_1's l1: 0.657498\n",
      "[21500]\ttraining's l1: 0.20198\tvalid_1's l1: 0.656828\n",
      "[22000]\ttraining's l1: 0.197272\tvalid_1's l1: 0.656172\n",
      "[22500]\ttraining's l1: 0.192708\tvalid_1's l1: 0.655544\n",
      "[23000]\ttraining's l1: 0.188243\tvalid_1's l1: 0.654913\n",
      "[23500]\ttraining's l1: 0.183993\tvalid_1's l1: 0.654347\n",
      "[24000]\ttraining's l1: 0.179767\tvalid_1's l1: 0.653782\n",
      "[24500]\ttraining's l1: 0.175669\tvalid_1's l1: 0.653251\n",
      "[25000]\ttraining's l1: 0.171696\tvalid_1's l1: 0.652726\n",
      "[25500]\ttraining's l1: 0.16785\tvalid_1's l1: 0.652192\n",
      "[26000]\ttraining's l1: 0.164098\tvalid_1's l1: 0.651687\n",
      "[26500]\ttraining's l1: 0.160452\tvalid_1's l1: 0.651222\n",
      "[27000]\ttraining's l1: 0.156909\tvalid_1's l1: 0.650824\n",
      "[27500]\ttraining's l1: 0.153439\tvalid_1's l1: 0.650394\n",
      "[28000]\ttraining's l1: 0.150077\tvalid_1's l1: 0.64997\n",
      "[28500]\ttraining's l1: 0.146844\tvalid_1's l1: 0.649665\n",
      "[29000]\ttraining's l1: 0.143623\tvalid_1's l1: 0.649296\n",
      "[29500]\ttraining's l1: 0.140511\tvalid_1's l1: 0.648901\n",
      "[30000]\ttraining's l1: 0.137494\tvalid_1's l1: 0.648628\n",
      "[30500]\ttraining's l1: 0.134551\tvalid_1's l1: 0.648242\n",
      "[31000]\ttraining's l1: 0.131669\tvalid_1's l1: 0.64792\n",
      "[31500]\ttraining's l1: 0.128889\tvalid_1's l1: 0.64765\n",
      "[32000]\ttraining's l1: 0.126158\tvalid_1's l1: 0.647345\n",
      "[32500]\ttraining's l1: 0.123525\tvalid_1's l1: 0.647046\n",
      "[33000]\ttraining's l1: 0.120917\tvalid_1's l1: 0.646799\n",
      "[33500]\ttraining's l1: 0.1184\tvalid_1's l1: 0.646527\n",
      "[34000]\ttraining's l1: 0.115935\tvalid_1's l1: 0.646256\n",
      "[34500]\ttraining's l1: 0.113508\tvalid_1's l1: 0.646056\n",
      "[35000]\ttraining's l1: 0.11115\tvalid_1's l1: 0.645802\n",
      "[35500]\ttraining's l1: 0.108864\tvalid_1's l1: 0.645566\n",
      "[36000]\ttraining's l1: 0.106644\tvalid_1's l1: 0.645321\n",
      "[36500]\ttraining's l1: 0.104453\tvalid_1's l1: 0.645084\n",
      "[37000]\ttraining's l1: 0.102319\tvalid_1's l1: 0.644857\n",
      "[37500]\ttraining's l1: 0.100238\tvalid_1's l1: 0.644659\n",
      "[38000]\ttraining's l1: 0.0982003\tvalid_1's l1: 0.644475\n",
      "[38500]\ttraining's l1: 0.0962153\tvalid_1's l1: 0.644309\n",
      "[39000]\ttraining's l1: 0.0942987\tvalid_1's l1: 0.644119\n",
      "[39500]\ttraining's l1: 0.0924209\tvalid_1's l1: 0.643942\n",
      "[40000]\ttraining's l1: 0.0905732\tvalid_1's l1: 0.643766\n",
      "[40500]\ttraining's l1: 0.0887797\tvalid_1's l1: 0.643615\n",
      "[41000]\ttraining's l1: 0.0870198\tvalid_1's l1: 0.643438\n",
      "[41500]\ttraining's l1: 0.0853008\tvalid_1's l1: 0.643267\n",
      "[42000]\ttraining's l1: 0.0835924\tvalid_1's l1: 0.643099\n",
      "[42500]\ttraining's l1: 0.0819301\tvalid_1's l1: 0.64295\n",
      "[43000]\ttraining's l1: 0.0803485\tvalid_1's l1: 0.642788\n",
      "[43500]\ttraining's l1: 0.0787643\tvalid_1's l1: 0.642639\n",
      "[44000]\ttraining's l1: 0.0772263\tvalid_1's l1: 0.642496\n",
      "[44500]\ttraining's l1: 0.0757281\tvalid_1's l1: 0.642368\n",
      "[45000]\ttraining's l1: 0.0742528\tvalid_1's l1: 0.642213\n",
      "[45500]\ttraining's l1: 0.0728128\tvalid_1's l1: 0.642086\n",
      "[46000]\ttraining's l1: 0.0714018\tvalid_1's l1: 0.641957\n",
      "[46500]\ttraining's l1: 0.0700099\tvalid_1's l1: 0.641827\n",
      "[47000]\ttraining's l1: 0.0686629\tvalid_1's l1: 0.641705\n",
      "[47500]\ttraining's l1: 0.0673461\tvalid_1's l1: 0.641597\n",
      "[48000]\ttraining's l1: 0.0660572\tvalid_1's l1: 0.641512\n",
      "[48500]\ttraining's l1: 0.0647862\tvalid_1's l1: 0.641414\n",
      "[49000]\ttraining's l1: 0.0635636\tvalid_1's l1: 0.641313\n",
      "[49500]\ttraining's l1: 0.0623543\tvalid_1's l1: 0.641206\n",
      "[50000]\ttraining's l1: 0.0611737\tvalid_1's l1: 0.641118\n",
      "[50500]\ttraining's l1: 0.0600137\tvalid_1's l1: 0.641026\n",
      "[51000]\ttraining's l1: 0.0588893\tvalid_1's l1: 0.64095\n",
      "[51500]\ttraining's l1: 0.0577864\tvalid_1's l1: 0.640862\n",
      "[52000]\ttraining's l1: 0.0566927\tvalid_1's l1: 0.640783\n",
      "[52500]\ttraining's l1: 0.055622\tvalid_1's l1: 0.64071\n",
      "[53000]\ttraining's l1: 0.0545851\tvalid_1's l1: 0.640625\n",
      "[53500]\ttraining's l1: 0.0535607\tvalid_1's l1: 0.640542\n",
      "[54000]\ttraining's l1: 0.0525609\tvalid_1's l1: 0.640461\n",
      "[54500]\ttraining's l1: 0.0515738\tvalid_1's l1: 0.64038\n",
      "[55000]\ttraining's l1: 0.0506013\tvalid_1's l1: 0.640298\n",
      "[55500]\ttraining's l1: 0.0496619\tvalid_1's l1: 0.640231\n",
      "[56000]\ttraining's l1: 0.0487467\tvalid_1's l1: 0.640179\n",
      "[56500]\ttraining's l1: 0.0478516\tvalid_1's l1: 0.640099\n",
      "[57000]\ttraining's l1: 0.0469797\tvalid_1's l1: 0.640052\n",
      "[57500]\ttraining's l1: 0.0461161\tvalid_1's l1: 0.640002\n",
      "[58000]\ttraining's l1: 0.0452837\tvalid_1's l1: 0.639936\n",
      "[58500]\ttraining's l1: 0.0444568\tvalid_1's l1: 0.639873\n",
      "[59000]\ttraining's l1: 0.0436552\tvalid_1's l1: 0.639808\n",
      "[59500]\ttraining's l1: 0.0428659\tvalid_1's l1: 0.639751\n",
      "[60000]\ttraining's l1: 0.0420931\tvalid_1's l1: 0.639705\n",
      "[60500]\ttraining's l1: 0.0413289\tvalid_1's l1: 0.639648\n",
      "[61000]\ttraining's l1: 0.0405761\tvalid_1's l1: 0.6396\n",
      "[61500]\ttraining's l1: 0.0398298\tvalid_1's l1: 0.639543\n",
      "[62000]\ttraining's l1: 0.0391095\tvalid_1's l1: 0.639484\n",
      "[62500]\ttraining's l1: 0.0384074\tvalid_1's l1: 0.63944\n",
      "[63000]\ttraining's l1: 0.0377238\tvalid_1's l1: 0.639393\n",
      "[63500]\ttraining's l1: 0.0370461\tvalid_1's l1: 0.639363\n",
      "[64000]\ttraining's l1: 0.036398\tvalid_1's l1: 0.639322\n",
      "[64500]\ttraining's l1: 0.0357502\tvalid_1's l1: 0.63928\n",
      "[65000]\ttraining's l1: 0.0351116\tvalid_1's l1: 0.639233\n",
      "[65500]\ttraining's l1: 0.0344916\tvalid_1's l1: 0.639175\n",
      "[66000]\ttraining's l1: 0.0338872\tvalid_1's l1: 0.639133\n",
      "[66500]\ttraining's l1: 0.0332852\tvalid_1's l1: 0.639095\n",
      "[67000]\ttraining's l1: 0.032706\tvalid_1's l1: 0.639048\n",
      "[67500]\ttraining's l1: 0.0321194\tvalid_1's l1: 0.639013\n",
      "[68000]\ttraining's l1: 0.0315553\tvalid_1's l1: 0.638976\n",
      "[68500]\ttraining's l1: 0.0310032\tvalid_1's l1: 0.638942\n",
      "[69000]\ttraining's l1: 0.0304583\tvalid_1's l1: 0.638902\n",
      "[69500]\ttraining's l1: 0.0299235\tvalid_1's l1: 0.638866\n",
      "[70000]\ttraining's l1: 0.0294021\tvalid_1's l1: 0.638837\n",
      "[70500]\ttraining's l1: 0.028892\tvalid_1's l1: 0.638805\n",
      "[71000]\ttraining's l1: 0.0283898\tvalid_1's l1: 0.638779\n",
      "[71500]\ttraining's l1: 0.0279031\tvalid_1's l1: 0.638746\n",
      "[72000]\ttraining's l1: 0.0274245\tvalid_1's l1: 0.63871\n",
      "[72500]\ttraining's l1: 0.0269557\tvalid_1's l1: 0.638674\n",
      "[73000]\ttraining's l1: 0.0264901\tvalid_1's l1: 0.638647\n",
      "[73500]\ttraining's l1: 0.0260387\tvalid_1's l1: 0.638616\n",
      "[74000]\ttraining's l1: 0.0255913\tvalid_1's l1: 0.638592\n",
      "[74500]\ttraining's l1: 0.0251532\tvalid_1's l1: 0.638569\n",
      "[75000]\ttraining's l1: 0.0247299\tvalid_1's l1: 0.63854\n",
      "[75500]\ttraining's l1: 0.0243017\tvalid_1's l1: 0.638517\n",
      "[76000]\ttraining's l1: 0.0238897\tvalid_1's l1: 0.638488\n",
      "[76500]\ttraining's l1: 0.0234891\tvalid_1's l1: 0.638461\n",
      "[77000]\ttraining's l1: 0.023092\tvalid_1's l1: 0.63843\n",
      "[77500]\ttraining's l1: 0.0227081\tvalid_1's l1: 0.638408\n",
      "[78000]\ttraining's l1: 0.0223261\tvalid_1's l1: 0.638386\n",
      "[78500]\ttraining's l1: 0.0219481\tvalid_1's l1: 0.638363\n",
      "[79000]\ttraining's l1: 0.0215781\tvalid_1's l1: 0.63834\n",
      "[79500]\ttraining's l1: 0.0212169\tvalid_1's l1: 0.638321\n",
      "[80000]\ttraining's l1: 0.0208603\tvalid_1's l1: 0.638295\n",
      "[80500]\ttraining's l1: 0.0205145\tvalid_1's l1: 0.638274\n",
      "[81000]\ttraining's l1: 0.0201681\tvalid_1's l1: 0.638251\n",
      "[81500]\ttraining's l1: 0.0198352\tvalid_1's l1: 0.638229\n",
      "[82000]\ttraining's l1: 0.019509\tvalid_1's l1: 0.638199\n",
      "[82500]\ttraining's l1: 0.0191841\tvalid_1's l1: 0.63818\n",
      "[83000]\ttraining's l1: 0.0188676\tvalid_1's l1: 0.63816\n",
      "[83500]\ttraining's l1: 0.0185608\tvalid_1's l1: 0.638142\n",
      "[84000]\ttraining's l1: 0.0182556\tvalid_1's l1: 0.638128\n",
      "[84500]\ttraining's l1: 0.0179591\tvalid_1's l1: 0.638104\n",
      "[85000]\ttraining's l1: 0.0176705\tvalid_1's l1: 0.638087\n",
      "[85500]\ttraining's l1: 0.0173829\tvalid_1's l1: 0.638069\n",
      "[86000]\ttraining's l1: 0.0171009\tvalid_1's l1: 0.638054\n",
      "[86500]\ttraining's l1: 0.0168226\tvalid_1's l1: 0.638038\n",
      "[87000]\ttraining's l1: 0.0165542\tvalid_1's l1: 0.638022\n",
      "[87500]\ttraining's l1: 0.0162839\tvalid_1's l1: 0.638007\n",
      "[88000]\ttraining's l1: 0.0160175\tvalid_1's l1: 0.63799\n",
      "[88500]\ttraining's l1: 0.015759\tvalid_1's l1: 0.637976\n",
      "[89000]\ttraining's l1: 0.0155062\tvalid_1's l1: 0.637959\n",
      "[89500]\ttraining's l1: 0.0152589\tvalid_1's l1: 0.637945\n",
      "[90000]\ttraining's l1: 0.0150159\tvalid_1's l1: 0.637934\n",
      "[90500]\ttraining's l1: 0.0147758\tvalid_1's l1: 0.637921\n",
      "[91000]\ttraining's l1: 0.0145436\tvalid_1's l1: 0.637904\n",
      "[91500]\ttraining's l1: 0.0143131\tvalid_1's l1: 0.63789\n",
      "[92000]\ttraining's l1: 0.0140872\tvalid_1's l1: 0.637878\n",
      "[92500]\ttraining's l1: 0.0138666\tvalid_1's l1: 0.637863\n",
      "[93000]\ttraining's l1: 0.0136469\tvalid_1's l1: 0.637852\n",
      "[93500]\ttraining's l1: 0.0134331\tvalid_1's l1: 0.637847\n",
      "[94000]\ttraining's l1: 0.0132251\tvalid_1's l1: 0.637836\n",
      "[94500]\ttraining's l1: 0.0130201\tvalid_1's l1: 0.637826\n",
      "[95000]\ttraining's l1: 0.0128161\tvalid_1's l1: 0.637815\n",
      "[95500]\ttraining's l1: 0.0126162\tvalid_1's l1: 0.637805\n",
      "[96000]\ttraining's l1: 0.0124197\tvalid_1's l1: 0.637796\n",
      "[96500]\ttraining's l1: 0.012227\tvalid_1's l1: 0.637787\n",
      "[97000]\ttraining's l1: 0.0120395\tvalid_1's l1: 0.637777\n",
      "[97500]\ttraining's l1: 0.0118553\tvalid_1's l1: 0.637769\n",
      "[98000]\ttraining's l1: 0.0116741\tvalid_1's l1: 0.637759\n",
      "[98500]\ttraining's l1: 0.0114981\tvalid_1's l1: 0.637749\n",
      "[99000]\ttraining's l1: 0.0113216\tvalid_1's l1: 0.637739\n",
      "[99500]\ttraining's l1: 0.0111502\tvalid_1's l1: 0.637731\n",
      "[100000]\ttraining's l1: 0.0109807\tvalid_1's l1: 0.637723\n",
      "[100500]\ttraining's l1: 0.0108128\tvalid_1's l1: 0.637713\n",
      "[101000]\ttraining's l1: 0.0106498\tvalid_1's l1: 0.637707\n",
      "[101500]\ttraining's l1: 0.0104897\tvalid_1's l1: 0.637698\n",
      "[102000]\ttraining's l1: 0.0103323\tvalid_1's l1: 0.637689\n",
      "[102500]\ttraining's l1: 0.0101793\tvalid_1's l1: 0.63768\n",
      "[103000]\ttraining's l1: 0.0100282\tvalid_1's l1: 0.637669\n",
      "[103500]\ttraining's l1: 0.00987967\tvalid_1's l1: 0.63766\n",
      "[104000]\ttraining's l1: 0.00973313\tvalid_1's l1: 0.637653\n",
      "[104500]\ttraining's l1: 0.00958937\tvalid_1's l1: 0.637645\n",
      "[105000]\ttraining's l1: 0.0094488\tvalid_1's l1: 0.637638\n",
      "[105500]\ttraining's l1: 0.00931119\tvalid_1's l1: 0.637632\n",
      "[106000]\ttraining's l1: 0.00917525\tvalid_1's l1: 0.637626\n",
      "[106500]\ttraining's l1: 0.00904333\tvalid_1's l1: 0.637621\n",
      "[107000]\ttraining's l1: 0.00891198\tvalid_1's l1: 0.637613\n",
      "[107500]\ttraining's l1: 0.00878408\tvalid_1's l1: 0.637606\n",
      "[108000]\ttraining's l1: 0.00865885\tvalid_1's l1: 0.637599\n",
      "[108500]\ttraining's l1: 0.00853386\tvalid_1's l1: 0.63759\n",
      "[109000]\ttraining's l1: 0.00841071\tvalid_1's l1: 0.637585\n",
      "[109500]\ttraining's l1: 0.0082914\tvalid_1's l1: 0.637577\n",
      "[110000]\ttraining's l1: 0.00817509\tvalid_1's l1: 0.637571\n",
      "[110500]\ttraining's l1: 0.00805865\tvalid_1's l1: 0.637563\n",
      "[111000]\ttraining's l1: 0.00794452\tvalid_1's l1: 0.637556\n",
      "[111500]\ttraining's l1: 0.00783192\tvalid_1's l1: 0.637549\n",
      "[112000]\ttraining's l1: 0.00772127\tvalid_1's l1: 0.637544\n",
      "[112500]\ttraining's l1: 0.00761063\tvalid_1's l1: 0.63754\n",
      "[113000]\ttraining's l1: 0.00750437\tvalid_1's l1: 0.637535\n",
      "[113500]\ttraining's l1: 0.0074012\tvalid_1's l1: 0.637531\n",
      "[114000]\ttraining's l1: 0.00729929\tvalid_1's l1: 0.637527\n",
      "[114500]\ttraining's l1: 0.00719787\tvalid_1's l1: 0.637524\n",
      "[115000]\ttraining's l1: 0.00709888\tvalid_1's l1: 0.637518\n",
      "[115500]\ttraining's l1: 0.00700125\tvalid_1's l1: 0.637513\n",
      "[116000]\ttraining's l1: 0.00690691\tvalid_1's l1: 0.63751\n",
      "[116500]\ttraining's l1: 0.00681326\tvalid_1's l1: 0.637506\n",
      "[117000]\ttraining's l1: 0.00672198\tvalid_1's l1: 0.637501\n",
      "[117500]\ttraining's l1: 0.0066307\tvalid_1's l1: 0.637499\n",
      "[118000]\ttraining's l1: 0.00654063\tvalid_1's l1: 0.637493\n",
      "[118500]\ttraining's l1: 0.0064526\tvalid_1's l1: 0.63749\n",
      "[119000]\ttraining's l1: 0.00636693\tvalid_1's l1: 0.637486\n",
      "[119500]\ttraining's l1: 0.00628282\tvalid_1's l1: 0.637481\n",
      "[120000]\ttraining's l1: 0.00620009\tvalid_1's l1: 0.637477\n",
      "[120500]\ttraining's l1: 0.0061175\tvalid_1's l1: 0.637474\n",
      "[121000]\ttraining's l1: 0.00603655\tvalid_1's l1: 0.637469\n",
      "[121500]\ttraining's l1: 0.00595757\tvalid_1's l1: 0.637464\n",
      "[122000]\ttraining's l1: 0.0058791\tvalid_1's l1: 0.637461\n",
      "[122500]\ttraining's l1: 0.00580307\tvalid_1's l1: 0.637458\n",
      "[123000]\ttraining's l1: 0.00572783\tvalid_1's l1: 0.637452\n",
      "[123500]\ttraining's l1: 0.00565382\tvalid_1's l1: 0.637448\n",
      "[124000]\ttraining's l1: 0.00558237\tvalid_1's l1: 0.637445\n",
      "[124500]\ttraining's l1: 0.00551167\tvalid_1's l1: 0.637441\n",
      "[125000]\ttraining's l1: 0.00544141\tvalid_1's l1: 0.637437\n",
      "[125500]\ttraining's l1: 0.00537221\tvalid_1's l1: 0.637433\n",
      "[126000]\ttraining's l1: 0.00530424\tvalid_1's l1: 0.637428\n",
      "[126500]\ttraining's l1: 0.00523743\tvalid_1's l1: 0.637425\n",
      "[127000]\ttraining's l1: 0.00517203\tvalid_1's l1: 0.637421\n",
      "[127500]\ttraining's l1: 0.00510768\tvalid_1's l1: 0.637417\n",
      "[128000]\ttraining's l1: 0.00504451\tvalid_1's l1: 0.637415\n",
      "[128500]\ttraining's l1: 0.00498209\tvalid_1's l1: 0.637412\n",
      "[129000]\ttraining's l1: 0.00492135\tvalid_1's l1: 0.637409\n",
      "[129500]\ttraining's l1: 0.00486109\tvalid_1's l1: 0.637405\n",
      "[130000]\ttraining's l1: 0.00480224\tvalid_1's l1: 0.637403\n",
      "[130500]\ttraining's l1: 0.00474388\tvalid_1's l1: 0.637401\n",
      "[131000]\ttraining's l1: 0.00468725\tvalid_1's l1: 0.637398\n",
      "[131500]\ttraining's l1: 0.00463099\tvalid_1's l1: 0.637394\n",
      "[132000]\ttraining's l1: 0.00457503\tvalid_1's l1: 0.637391\n",
      "[132500]\ttraining's l1: 0.00452068\tvalid_1's l1: 0.637388\n",
      "[133000]\ttraining's l1: 0.00446723\tvalid_1's l1: 0.637385\n",
      "[133500]\ttraining's l1: 0.00441407\tvalid_1's l1: 0.637383\n",
      "[134000]\ttraining's l1: 0.00436217\tvalid_1's l1: 0.63738\n",
      "[134500]\ttraining's l1: 0.00431113\tvalid_1's l1: 0.637378\n",
      "[135000]\ttraining's l1: 0.00426132\tvalid_1's l1: 0.637375\n",
      "[135500]\ttraining's l1: 0.00421191\tvalid_1's l1: 0.637373\n",
      "[136000]\ttraining's l1: 0.00416297\tvalid_1's l1: 0.637369\n",
      "[136500]\ttraining's l1: 0.0041148\tvalid_1's l1: 0.637367\n",
      "[137000]\ttraining's l1: 0.00406791\tvalid_1's l1: 0.637364\n",
      "[137500]\ttraining's l1: 0.00402133\tvalid_1's l1: 0.637361\n",
      "[138000]\ttraining's l1: 0.00397601\tvalid_1's l1: 0.637358\n",
      "[138500]\ttraining's l1: 0.00393153\tvalid_1's l1: 0.637355\n",
      "[139000]\ttraining's l1: 0.00388788\tvalid_1's l1: 0.637353\n",
      "Early stopping, best iteration is:\n",
      "[139108]\ttraining's l1: 0.00387853\tvalid_1's l1: 0.637353\n",
      "CV mean score: -0.4476, std: 0.0028.\n",
      "Training of type 7\n",
      "Fold 1 started at Tue Jun 25 10:34:19 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.207169\tvalid_1's l1: 0.283373\n",
      "[1000]\ttraining's l1: 0.150739\tvalid_1's l1: 0.265205\n",
      "[1500]\ttraining's l1: 0.117212\tvalid_1's l1: 0.257366\n",
      "[2000]\ttraining's l1: 0.0937767\tvalid_1's l1: 0.252754\n",
      "[2500]\ttraining's l1: 0.0763516\tvalid_1's l1: 0.249715\n",
      "[3000]\ttraining's l1: 0.0630105\tvalid_1's l1: 0.247539\n",
      "[3500]\ttraining's l1: 0.0526478\tvalid_1's l1: 0.246245\n",
      "[4000]\ttraining's l1: 0.0442871\tvalid_1's l1: 0.245291\n",
      "[4500]\ttraining's l1: 0.0375648\tvalid_1's l1: 0.244607\n",
      "[5000]\ttraining's l1: 0.0320233\tvalid_1's l1: 0.244039\n",
      "[5500]\ttraining's l1: 0.0274582\tvalid_1's l1: 0.243544\n",
      "[6000]\ttraining's l1: 0.0236091\tvalid_1's l1: 0.243178\n",
      "[6500]\ttraining's l1: 0.0204095\tvalid_1's l1: 0.242848\n",
      "[7000]\ttraining's l1: 0.0177311\tvalid_1's l1: 0.242567\n",
      "[7500]\ttraining's l1: 0.0154518\tvalid_1's l1: 0.242404\n",
      "[8000]\ttraining's l1: 0.0135306\tvalid_1's l1: 0.242231\n",
      "[8500]\ttraining's l1: 0.0118823\tvalid_1's l1: 0.242102\n",
      "[9000]\ttraining's l1: 0.0104871\tvalid_1's l1: 0.241995\n",
      "[9500]\ttraining's l1: 0.00929786\tvalid_1's l1: 0.241899\n",
      "[10000]\ttraining's l1: 0.00826922\tvalid_1's l1: 0.241828\n",
      "[10500]\ttraining's l1: 0.00739194\tvalid_1's l1: 0.241782\n",
      "[11000]\ttraining's l1: 0.00663466\tvalid_1's l1: 0.241726\n",
      "[11500]\ttraining's l1: 0.00598241\tvalid_1's l1: 0.241679\n",
      "[12000]\ttraining's l1: 0.00541616\tvalid_1's l1: 0.241646\n",
      "[12500]\ttraining's l1: 0.00492471\tvalid_1's l1: 0.24161\n",
      "[13000]\ttraining's l1: 0.00450102\tvalid_1's l1: 0.241584\n",
      "[13500]\ttraining's l1: 0.00412446\tvalid_1's l1: 0.241566\n",
      "[14000]\ttraining's l1: 0.00379623\tvalid_1's l1: 0.241553\n",
      "[14500]\ttraining's l1: 0.00351158\tvalid_1's l1: 0.241537\n",
      "[15000]\ttraining's l1: 0.00325818\tvalid_1's l1: 0.24152\n",
      "[15500]\ttraining's l1: 0.00303549\tvalid_1's l1: 0.241505\n",
      "[16000]\ttraining's l1: 0.00283938\tvalid_1's l1: 0.241492\n",
      "[16500]\ttraining's l1: 0.00266056\tvalid_1's l1: 0.241484\n",
      "[17000]\ttraining's l1: 0.00250251\tvalid_1's l1: 0.241475\n",
      "[17500]\ttraining's l1: 0.00236076\tvalid_1's l1: 0.241466\n",
      "[18000]\ttraining's l1: 0.00223347\tvalid_1's l1: 0.241458\n",
      "[18500]\ttraining's l1: 0.00211688\tvalid_1's l1: 0.241455\n",
      "[19000]\ttraining's l1: 0.00201076\tvalid_1's l1: 0.241451\n",
      "[19500]\ttraining's l1: 0.00191453\tvalid_1's l1: 0.241446\n",
      "[20000]\ttraining's l1: 0.00182693\tvalid_1's l1: 0.241439\n",
      "[20500]\ttraining's l1: 0.00174612\tvalid_1's l1: 0.241434\n",
      "[21000]\ttraining's l1: 0.00167149\tvalid_1's l1: 0.241429\n",
      "[21500]\ttraining's l1: 0.0016019\tvalid_1's l1: 0.241425\n",
      "[22000]\ttraining's l1: 0.00153841\tvalid_1's l1: 0.241423\n",
      "[22500]\ttraining's l1: 0.0014789\tvalid_1's l1: 0.24142\n",
      "[23000]\ttraining's l1: 0.00142362\tvalid_1's l1: 0.241417\n",
      "[23500]\ttraining's l1: 0.00137188\tvalid_1's l1: 0.241414\n",
      "[24000]\ttraining's l1: 0.00132274\tvalid_1's l1: 0.241411\n",
      "[24500]\ttraining's l1: 0.00127768\tvalid_1's l1: 0.241407\n",
      "[25000]\ttraining's l1: 0.00123538\tvalid_1's l1: 0.241405\n",
      "[25500]\ttraining's l1: 0.00119593\tvalid_1's l1: 0.241402\n",
      "[26000]\ttraining's l1: 0.00115895\tvalid_1's l1: 0.2414\n",
      "Early stopping, best iteration is:\n",
      "[26143]\ttraining's l1: 0.00114882\tvalid_1's l1: 0.241399\n",
      "Fold 2 started at Tue Jun 25 10:41:23 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.208774\tvalid_1's l1: 0.281721\n",
      "[1000]\ttraining's l1: 0.152343\tvalid_1's l1: 0.263505\n",
      "[1500]\ttraining's l1: 0.118012\tvalid_1's l1: 0.255224\n",
      "[2000]\ttraining's l1: 0.0944332\tvalid_1's l1: 0.250514\n",
      "[2500]\ttraining's l1: 0.0769798\tvalid_1's l1: 0.247316\n",
      "[3000]\ttraining's l1: 0.0636601\tvalid_1's l1: 0.245352\n",
      "[3500]\ttraining's l1: 0.0531303\tvalid_1's l1: 0.244049\n",
      "[4000]\ttraining's l1: 0.0446955\tvalid_1's l1: 0.24295\n",
      "[4500]\ttraining's l1: 0.0378578\tvalid_1's l1: 0.242201\n",
      "[5000]\ttraining's l1: 0.0322772\tvalid_1's l1: 0.241615\n",
      "[5500]\ttraining's l1: 0.0276292\tvalid_1's l1: 0.241158\n",
      "[6000]\ttraining's l1: 0.0237873\tvalid_1's l1: 0.24082\n",
      "[6500]\ttraining's l1: 0.0205654\tvalid_1's l1: 0.240535\n",
      "[7000]\ttraining's l1: 0.0178543\tvalid_1's l1: 0.240322\n",
      "[7500]\ttraining's l1: 0.0155628\tvalid_1's l1: 0.240134\n",
      "[8000]\ttraining's l1: 0.0136239\tvalid_1's l1: 0.239967\n",
      "[8500]\ttraining's l1: 0.0119767\tvalid_1's l1: 0.239858\n",
      "[9000]\ttraining's l1: 0.0105674\tvalid_1's l1: 0.239763\n",
      "[9500]\ttraining's l1: 0.00936662\tvalid_1's l1: 0.239675\n",
      "[10000]\ttraining's l1: 0.00833957\tvalid_1's l1: 0.23962\n",
      "[10500]\ttraining's l1: 0.00746221\tvalid_1's l1: 0.239562\n",
      "[11000]\ttraining's l1: 0.00669268\tvalid_1's l1: 0.239505\n",
      "[11500]\ttraining's l1: 0.00603656\tvalid_1's l1: 0.239448\n",
      "[12000]\ttraining's l1: 0.00546367\tvalid_1's l1: 0.239411\n",
      "[12500]\ttraining's l1: 0.00496186\tvalid_1's l1: 0.239383\n",
      "[13000]\ttraining's l1: 0.00453007\tvalid_1's l1: 0.239356\n",
      "[13500]\ttraining's l1: 0.00415302\tvalid_1's l1: 0.239327\n",
      "[14000]\ttraining's l1: 0.00382101\tvalid_1's l1: 0.239311\n",
      "[14500]\ttraining's l1: 0.00353249\tvalid_1's l1: 0.239291\n",
      "[15000]\ttraining's l1: 0.00327777\tvalid_1's l1: 0.239275\n",
      "[15500]\ttraining's l1: 0.00305287\tvalid_1's l1: 0.239265\n",
      "[16000]\ttraining's l1: 0.00285276\tvalid_1's l1: 0.239255\n",
      "[16500]\ttraining's l1: 0.00267422\tvalid_1's l1: 0.239246\n",
      "[17000]\ttraining's l1: 0.00251281\tvalid_1's l1: 0.239236\n",
      "[17500]\ttraining's l1: 0.0023702\tvalid_1's l1: 0.239227\n",
      "[18000]\ttraining's l1: 0.00224145\tvalid_1's l1: 0.239221\n",
      "[18500]\ttraining's l1: 0.00212481\tvalid_1's l1: 0.239215\n",
      "[19000]\ttraining's l1: 0.00201694\tvalid_1's l1: 0.239209\n",
      "[19500]\ttraining's l1: 0.00192044\tvalid_1's l1: 0.239204\n",
      "[20000]\ttraining's l1: 0.00183116\tvalid_1's l1: 0.239198\n",
      "[20500]\ttraining's l1: 0.00174977\tvalid_1's l1: 0.239195\n",
      "[21000]\ttraining's l1: 0.00167521\tvalid_1's l1: 0.239188\n",
      "[21500]\ttraining's l1: 0.00160594\tvalid_1's l1: 0.239185\n",
      "[22000]\ttraining's l1: 0.00154209\tvalid_1's l1: 0.239181\n",
      "[22500]\ttraining's l1: 0.00148245\tvalid_1's l1: 0.239177\n",
      "Early stopping, best iteration is:\n",
      "[22545]\ttraining's l1: 0.00147743\tvalid_1's l1: 0.239176\n",
      "Fold 3 started at Tue Jun 25 10:48:38 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.207145\tvalid_1's l1: 0.282852\n",
      "[1000]\ttraining's l1: 0.151439\tvalid_1's l1: 0.265047\n",
      "[1500]\ttraining's l1: 0.117927\tvalid_1's l1: 0.25712\n",
      "[2000]\ttraining's l1: 0.0944824\tvalid_1's l1: 0.252785\n",
      "[2500]\ttraining's l1: 0.0770759\tvalid_1's l1: 0.249666\n",
      "[3000]\ttraining's l1: 0.0638158\tvalid_1's l1: 0.247853\n",
      "[3500]\ttraining's l1: 0.0532336\tvalid_1's l1: 0.246305\n",
      "[4000]\ttraining's l1: 0.0448282\tvalid_1's l1: 0.245249\n",
      "[4500]\ttraining's l1: 0.0379882\tvalid_1's l1: 0.244516\n",
      "[5000]\ttraining's l1: 0.0323673\tvalid_1's l1: 0.243896\n",
      "[5500]\ttraining's l1: 0.0276851\tvalid_1's l1: 0.243462\n",
      "[6000]\ttraining's l1: 0.0238792\tvalid_1's l1: 0.243151\n",
      "[6500]\ttraining's l1: 0.0206214\tvalid_1's l1: 0.242836\n",
      "[7000]\ttraining's l1: 0.0179015\tvalid_1's l1: 0.242645\n",
      "[7500]\ttraining's l1: 0.0155974\tvalid_1's l1: 0.242443\n",
      "[8000]\ttraining's l1: 0.01367\tvalid_1's l1: 0.242302\n",
      "[8500]\ttraining's l1: 0.012018\tvalid_1's l1: 0.242188\n",
      "[9000]\ttraining's l1: 0.0105926\tvalid_1's l1: 0.242048\n",
      "[9500]\ttraining's l1: 0.00938766\tvalid_1's l1: 0.241953\n",
      "[10000]\ttraining's l1: 0.008354\tvalid_1's l1: 0.241889\n",
      "[10500]\ttraining's l1: 0.0074742\tvalid_1's l1: 0.241833\n",
      "[11000]\ttraining's l1: 0.00670988\tvalid_1's l1: 0.241787\n",
      "[11500]\ttraining's l1: 0.00604699\tvalid_1's l1: 0.241744\n",
      "[12000]\ttraining's l1: 0.00546478\tvalid_1's l1: 0.241714\n",
      "[12500]\ttraining's l1: 0.00496907\tvalid_1's l1: 0.24168\n",
      "[13000]\ttraining's l1: 0.00453916\tvalid_1's l1: 0.241653\n",
      "[13500]\ttraining's l1: 0.00415904\tvalid_1's l1: 0.241622\n",
      "[14000]\ttraining's l1: 0.00382546\tvalid_1's l1: 0.241598\n",
      "[14500]\ttraining's l1: 0.00353209\tvalid_1's l1: 0.24158\n",
      "[15000]\ttraining's l1: 0.00327676\tvalid_1's l1: 0.241566\n",
      "[15500]\ttraining's l1: 0.00304913\tvalid_1's l1: 0.241552\n",
      "[16000]\ttraining's l1: 0.00285003\tvalid_1's l1: 0.241544\n",
      "[16500]\ttraining's l1: 0.00267163\tvalid_1's l1: 0.241533\n",
      "[17000]\ttraining's l1: 0.00250976\tvalid_1's l1: 0.241524\n",
      "[17500]\ttraining's l1: 0.00236569\tvalid_1's l1: 0.241514\n",
      "[18000]\ttraining's l1: 0.00223639\tvalid_1's l1: 0.241505\n",
      "[18500]\ttraining's l1: 0.00211939\tvalid_1's l1: 0.241499\n",
      "[19000]\ttraining's l1: 0.00201259\tvalid_1's l1: 0.241492\n",
      "[19500]\ttraining's l1: 0.0019169\tvalid_1's l1: 0.241487\n",
      "[20000]\ttraining's l1: 0.00182812\tvalid_1's l1: 0.241481\n",
      "[20500]\ttraining's l1: 0.00174703\tvalid_1's l1: 0.241476\n",
      "[21000]\ttraining's l1: 0.00167238\tvalid_1's l1: 0.241474\n",
      "[21500]\ttraining's l1: 0.00160405\tvalid_1's l1: 0.24147\n",
      "[22000]\ttraining's l1: 0.00153931\tvalid_1's l1: 0.241465\n",
      "[22500]\ttraining's l1: 0.00148026\tvalid_1's l1: 0.241462\n",
      "[23000]\ttraining's l1: 0.00142562\tvalid_1's l1: 0.241458\n",
      "[23500]\ttraining's l1: 0.00137348\tvalid_1's l1: 0.241453\n",
      "[24000]\ttraining's l1: 0.00132552\tvalid_1's l1: 0.241452\n",
      "[24500]\ttraining's l1: 0.00128043\tvalid_1's l1: 0.241449\n",
      "Early stopping, best iteration is:\n",
      "[24668]\ttraining's l1: 0.00126606\tvalid_1's l1: 0.241448\n",
      "Fold 4 started at Tue Jun 25 10:55:31 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.20584\tvalid_1's l1: 0.282821\n",
      "[1000]\ttraining's l1: 0.150671\tvalid_1's l1: 0.264831\n",
      "[1500]\ttraining's l1: 0.117381\tvalid_1's l1: 0.25688\n",
      "[2000]\ttraining's l1: 0.0938397\tvalid_1's l1: 0.251981\n",
      "[2500]\ttraining's l1: 0.0764878\tvalid_1's l1: 0.2492\n",
      "[3000]\ttraining's l1: 0.0631833\tvalid_1's l1: 0.247344\n",
      "[3500]\ttraining's l1: 0.0527538\tvalid_1's l1: 0.245923\n",
      "[4000]\ttraining's l1: 0.0444491\tvalid_1's l1: 0.244847\n",
      "[4500]\ttraining's l1: 0.0376569\tvalid_1's l1: 0.244077\n",
      "[5000]\ttraining's l1: 0.0321706\tvalid_1's l1: 0.243462\n",
      "[5500]\ttraining's l1: 0.0276014\tvalid_1's l1: 0.242987\n",
      "[6000]\ttraining's l1: 0.0237414\tvalid_1's l1: 0.24264\n",
      "[6500]\ttraining's l1: 0.0205417\tvalid_1's l1: 0.242329\n",
      "[7000]\ttraining's l1: 0.0178509\tvalid_1's l1: 0.242075\n",
      "[7500]\ttraining's l1: 0.015562\tvalid_1's l1: 0.241859\n",
      "[8000]\ttraining's l1: 0.0136474\tvalid_1's l1: 0.241712\n",
      "[8500]\ttraining's l1: 0.0119934\tvalid_1's l1: 0.241584\n",
      "[9000]\ttraining's l1: 0.0105801\tvalid_1's l1: 0.241498\n",
      "[9500]\ttraining's l1: 0.00936706\tvalid_1's l1: 0.241411\n",
      "[10000]\ttraining's l1: 0.00832629\tvalid_1's l1: 0.241346\n",
      "[10500]\ttraining's l1: 0.00744644\tvalid_1's l1: 0.241277\n",
      "[11000]\ttraining's l1: 0.00668378\tvalid_1's l1: 0.241218\n",
      "[11500]\ttraining's l1: 0.00602896\tvalid_1's l1: 0.241177\n",
      "[12000]\ttraining's l1: 0.00545866\tvalid_1's l1: 0.24113\n",
      "[12500]\ttraining's l1: 0.00496396\tvalid_1's l1: 0.241108\n",
      "[13000]\ttraining's l1: 0.00452968\tvalid_1's l1: 0.241091\n",
      "[13500]\ttraining's l1: 0.00415413\tvalid_1's l1: 0.241074\n",
      "[14000]\ttraining's l1: 0.00382513\tvalid_1's l1: 0.241054\n",
      "[14500]\ttraining's l1: 0.00353676\tvalid_1's l1: 0.241035\n",
      "[15000]\ttraining's l1: 0.00328244\tvalid_1's l1: 0.241017\n",
      "[15500]\ttraining's l1: 0.00305579\tvalid_1's l1: 0.241009\n",
      "[16000]\ttraining's l1: 0.00285691\tvalid_1's l1: 0.241005\n",
      "[16500]\ttraining's l1: 0.00267963\tvalid_1's l1: 0.240991\n",
      "[17000]\ttraining's l1: 0.0025191\tvalid_1's l1: 0.240984\n",
      "[17500]\ttraining's l1: 0.00237283\tvalid_1's l1: 0.240977\n",
      "[18000]\ttraining's l1: 0.00224353\tvalid_1's l1: 0.240968\n",
      "Early stopping, best iteration is:\n",
      "[18083]\ttraining's l1: 0.00222312\tvalid_1's l1: 0.240966\n",
      "Fold 5 started at Tue Jun 25 11:00:18 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.207867\tvalid_1's l1: 0.28202\n",
      "[1000]\ttraining's l1: 0.151288\tvalid_1's l1: 0.2635\n",
      "[1500]\ttraining's l1: 0.117359\tvalid_1's l1: 0.254855\n",
      "[2000]\ttraining's l1: 0.0938947\tvalid_1's l1: 0.250209\n",
      "[2500]\ttraining's l1: 0.0765693\tvalid_1's l1: 0.247516\n",
      "[3000]\ttraining's l1: 0.0633049\tvalid_1's l1: 0.245675\n",
      "[3500]\ttraining's l1: 0.0528205\tvalid_1's l1: 0.244331\n",
      "[4000]\ttraining's l1: 0.0444594\tvalid_1's l1: 0.24326\n",
      "[4500]\ttraining's l1: 0.0377074\tvalid_1's l1: 0.242479\n",
      "[5000]\ttraining's l1: 0.0321233\tvalid_1's l1: 0.241854\n",
      "[5500]\ttraining's l1: 0.0275111\tvalid_1's l1: 0.241461\n",
      "[6000]\ttraining's l1: 0.023682\tvalid_1's l1: 0.241131\n",
      "[6500]\ttraining's l1: 0.0204604\tvalid_1's l1: 0.240863\n",
      "[7000]\ttraining's l1: 0.0177807\tvalid_1's l1: 0.240628\n",
      "[7500]\ttraining's l1: 0.0155244\tvalid_1's l1: 0.240455\n",
      "[8000]\ttraining's l1: 0.0136158\tvalid_1's l1: 0.240296\n",
      "[8500]\ttraining's l1: 0.0119973\tvalid_1's l1: 0.240194\n",
      "[9000]\ttraining's l1: 0.010596\tvalid_1's l1: 0.240096\n",
      "[9500]\ttraining's l1: 0.00939374\tvalid_1's l1: 0.240009\n",
      "[10000]\ttraining's l1: 0.00835517\tvalid_1's l1: 0.239942\n",
      "[10500]\ttraining's l1: 0.00747032\tvalid_1's l1: 0.239872\n",
      "[11000]\ttraining's l1: 0.00669693\tvalid_1's l1: 0.239832\n",
      "[11500]\ttraining's l1: 0.00603901\tvalid_1's l1: 0.239799\n",
      "[12000]\ttraining's l1: 0.00546887\tvalid_1's l1: 0.239763\n",
      "[12500]\ttraining's l1: 0.00496553\tvalid_1's l1: 0.239738\n",
      "[13000]\ttraining's l1: 0.00452855\tvalid_1's l1: 0.239711\n",
      "[13500]\ttraining's l1: 0.00415006\tvalid_1's l1: 0.239688\n",
      "[14000]\ttraining's l1: 0.00381922\tvalid_1's l1: 0.239662\n",
      "[14500]\ttraining's l1: 0.00353038\tvalid_1's l1: 0.239639\n",
      "[15000]\ttraining's l1: 0.00327241\tvalid_1's l1: 0.239626\n",
      "[15500]\ttraining's l1: 0.00304763\tvalid_1's l1: 0.239611\n",
      "[16000]\ttraining's l1: 0.00284825\tvalid_1's l1: 0.239599\n",
      "[16500]\ttraining's l1: 0.00266958\tvalid_1's l1: 0.239587\n",
      "[17000]\ttraining's l1: 0.00251116\tvalid_1's l1: 0.239576\n",
      "[17500]\ttraining's l1: 0.00236675\tvalid_1's l1: 0.239564\n",
      "[18000]\ttraining's l1: 0.00223741\tvalid_1's l1: 0.239554\n",
      "[18500]\ttraining's l1: 0.0021208\tvalid_1's l1: 0.239548\n",
      "[19000]\ttraining's l1: 0.0020147\tvalid_1's l1: 0.239544\n",
      "[19500]\ttraining's l1: 0.00191856\tvalid_1's l1: 0.239538\n",
      "[20000]\ttraining's l1: 0.00182911\tvalid_1's l1: 0.239533\n",
      "[20500]\ttraining's l1: 0.00174796\tvalid_1's l1: 0.23953\n",
      "[21000]\ttraining's l1: 0.00167325\tvalid_1's l1: 0.239525\n",
      "[21500]\ttraining's l1: 0.00160446\tvalid_1's l1: 0.239521\n",
      "[22000]\ttraining's l1: 0.00153921\tvalid_1's l1: 0.239516\n",
      "[22500]\ttraining's l1: 0.00148021\tvalid_1's l1: 0.239512\n",
      "[23000]\ttraining's l1: 0.00142456\tvalid_1's l1: 0.23951\n",
      "[23500]\ttraining's l1: 0.0013725\tvalid_1's l1: 0.239506\n",
      "[24000]\ttraining's l1: 0.00132476\tvalid_1's l1: 0.239505\n",
      "[24500]\ttraining's l1: 0.00127953\tvalid_1's l1: 0.239502\n",
      "[25000]\ttraining's l1: 0.00123697\tvalid_1's l1: 0.2395\n",
      "[25500]\ttraining's l1: 0.00119762\tvalid_1's l1: 0.239498\n",
      "[26000]\ttraining's l1: 0.00116051\tvalid_1's l1: 0.239495\n",
      "[26500]\ttraining's l1: 0.00112562\tvalid_1's l1: 0.239493\n",
      "[27000]\ttraining's l1: 0.00109217\tvalid_1's l1: 0.239491\n",
      "[27500]\ttraining's l1: 0.00106113\tvalid_1's l1: 0.239489\n",
      "[28000]\ttraining's l1: 0.0010315\tvalid_1's l1: 0.239488\n",
      "[28500]\ttraining's l1: 0.00100378\tvalid_1's l1: 0.239486\n",
      "[29000]\ttraining's l1: 0.000977795\tvalid_1's l1: 0.239484\n",
      "[29500]\ttraining's l1: 0.000953368\tvalid_1's l1: 0.239483\n",
      "Early stopping, best iteration is:\n",
      "[29664]\ttraining's l1: 0.000945694\tvalid_1's l1: 0.239482\n",
      "CV mean score: -1.4251, std: 0.0040.\n"
     ]
    }
   ],
   "source": [
    "X.to_csv('../input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['oof_fc'] = X_short['oof']\n",
    "X_test['oof_fc'] = X_short_test['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_hdf('../data/FE006-X.hdf', key=\"X\")\n",
    "X_test.to_hdf('../data/FE006-X_test.hdf', key=\"X_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_columns = ['type','oof_fc',\n",
    " 'bond_lengths_mean_y',\n",
    " 'bond_lengths_std_y',\n",
    " 'bond_lengths_mean_x',\n",
    " 'molecule_atom_index_0_dist_min_div',\n",
    " 'molecule_atom_index_0_dist_std_div',\n",
    " 'molecule_atom_index_0_dist_mean',\n",
    " 'molecule_atom_index_0_dist_max',\n",
    " 'dist_y',\n",
    " 'molecule_atom_index_1_dist_std_diff',\n",
    " 'z_0',\n",
    " 'molecule_type_dist_min',\n",
    " 'molecule_atom_index_0_y_1_mean_div',\n",
    " 'dist_x',\n",
    " 'x_0',\n",
    " 'y_0',\n",
    " 'molecule_type_dist_std',\n",
    " 'molecule_atom_index_0_y_1_std',\n",
    " 'molecule_dist_mean',\n",
    " 'molecule_atom_index_0_dist_std_diff',\n",
    " 'dist_z',\n",
    " 'molecule_atom_index_0_dist_std',\n",
    " 'molecule_atom_index_0_x_1_std',\n",
    " 'molecule_type_dist_std_diff',\n",
    " 'molecule_type_0_dist_std',\n",
    " 'dist',\n",
    " 'molecule_atom_index_0_dist_mean_diff',\n",
    " 'molecule_atom_index_1_dist_min_div',\n",
    " 'molecule_atom_index_1_dist_mean_diff',\n",
    " 'y_1',\n",
    " 'molecule_type_dist_mean_div',\n",
    " 'molecule_dist_max',\n",
    " 'molecule_atom_index_0_dist_mean_div',\n",
    " 'z_1',\n",
    " 'molecule_atom_index_0_z_1_std',\n",
    " 'molecule_atom_index_1_dist_mean_div',\n",
    " 'molecule_atom_index_1_dist_min_diff',\n",
    " 'molecule_atom_index_1_dist_mean',\n",
    " 'molecule_atom_index_1_dist_min',\n",
    " 'molecule_atom_index_1_dist_max',\n",
    " 'molecule_type_0_dist_std_diff',\n",
    " 'molecule_atom_index_0_dist_min_diff',\n",
    " 'molecule_type_dist_mean_diff',\n",
    " 'x_1',\n",
    " 'molecule_atom_index_0_y_1_max',\n",
    " 'molecule_atom_index_0_y_1_mean_diff',\n",
    " 'molecule_atom_1_dist_std_diff',\n",
    " 'molecule_atom_index_0_y_1_mean',\n",
    " 'molecule_atom_1_dist_std',\n",
    " 'molecule_type_dist_max']\n",
    "\n",
    "X = X[good_columns].copy()\n",
    "X_test = X_test[good_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Best Feature for Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 50,\n",
    "          'min_child_samples': 79,\n",
    "          'min_data_in_leaf': 100,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.2,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }\n",
    "#result_dict_lgb2 = train_model_regression(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='group_mae', plot_feature_importance=True,\n",
    "#                                                      verbose=500, early_stopping_rounds=200, n_estimators=n_estimators_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nfeature_importance = result_dict_lgb2['feature_importance']\\nbest_features = feature_importance[['feature','importance']].groupby(['feature']).mean().sort_values(\\n        by='importance',ascending=False).iloc[:50,0:0].index.tolist()\\nbest_features\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best Features? \n",
    "''' \n",
    "feature_importance = result_dict_lgb2['feature_importance']\n",
    "best_features = feature_importance[['feature','importance']].groupby(['feature']).mean().sort_values(\n",
    "        by='importance',ascending=False).iloc[:50,0:0].index.tolist()\n",
    "best_features'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id6\"></a> <br> \n",
    "# **6. Final Model** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models for each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of type 0\n",
      "Fold 1 started at Tue Jun 25 11:21:47 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's l1: 1.33406\tvalid_1's l1: 1.34506\n",
      "Fold 2 started at Tue Jun 25 11:21:54 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[121]\ttraining's l1: 1.31288\tvalid_1's l1: 1.34063\n",
      "Fold 3 started at Tue Jun 25 11:22:13 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's l1: 1.30781\tvalid_1's l1: 1.34314\n",
      "Fold 4 started at Tue Jun 25 11:22:29 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's l1: 1.33213\tvalid_1's l1: 1.3427\n",
      "Fold 5 started at Tue Jun 25 11:22:49 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttraining's l1: 1.32653\tvalid_1's l1: 1.33783\n",
      "CV mean score: 0.2941, std: 0.0018.\n",
      "Training of type 3\n",
      "Fold 1 started at Tue Jun 25 11:22:58 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.28061\tvalid_1's l1: 0.329963\n",
      "[1000]\ttraining's l1: 0.244381\tvalid_1's l1: 0.327232\n",
      "[1500]\ttraining's l1: 0.216556\tvalid_1's l1: 0.326135\n",
      "[2000]\ttraining's l1: 0.193859\tvalid_1's l1: 0.32486\n",
      "[2500]\ttraining's l1: 0.174808\tvalid_1's l1: 0.323857\n",
      "[3000]\ttraining's l1: 0.158339\tvalid_1's l1: 0.32289\n",
      "[3500]\ttraining's l1: 0.144272\tvalid_1's l1: 0.322258\n",
      "[4000]\ttraining's l1: 0.131725\tvalid_1's l1: 0.321712\n",
      "[4500]\ttraining's l1: 0.120642\tvalid_1's l1: 0.320737\n",
      "[5000]\ttraining's l1: 0.11071\tvalid_1's l1: 0.320082\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.11071\tvalid_1's l1: 0.320082\n",
      "Fold 2 started at Tue Jun 25 11:25:55 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.281441\tvalid_1's l1: 0.331718\n",
      "[1000]\ttraining's l1: 0.244657\tvalid_1's l1: 0.328684\n",
      "[1500]\ttraining's l1: 0.216683\tvalid_1's l1: 0.327354\n",
      "[2000]\ttraining's l1: 0.193798\tvalid_1's l1: 0.32606\n",
      "[2500]\ttraining's l1: 0.174618\tvalid_1's l1: 0.324982\n",
      "[3000]\ttraining's l1: 0.158183\tvalid_1's l1: 0.324091\n",
      "[3500]\ttraining's l1: 0.143898\tvalid_1's l1: 0.323282\n",
      "[4000]\ttraining's l1: 0.131419\tvalid_1's l1: 0.322506\n",
      "[4500]\ttraining's l1: 0.120401\tvalid_1's l1: 0.321609\n",
      "[5000]\ttraining's l1: 0.110598\tvalid_1's l1: 0.321077\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.110598\tvalid_1's l1: 0.321077\n",
      "Fold 3 started at Tue Jun 25 11:28:29 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.280129\tvalid_1's l1: 0.33071\n",
      "[1000]\ttraining's l1: 0.244225\tvalid_1's l1: 0.32949\n",
      "[1500]\ttraining's l1: 0.216423\tvalid_1's l1: 0.328164\n",
      "[2000]\ttraining's l1: 0.193669\tvalid_1's l1: 0.327018\n",
      "[2500]\ttraining's l1: 0.174463\tvalid_1's l1: 0.325637\n",
      "[3000]\ttraining's l1: 0.158121\tvalid_1's l1: 0.324903\n",
      "[3500]\ttraining's l1: 0.144039\tvalid_1's l1: 0.324114\n",
      "[4000]\ttraining's l1: 0.13156\tvalid_1's l1: 0.32344\n",
      "[4500]\ttraining's l1: 0.120568\tvalid_1's l1: 0.322894\n",
      "[5000]\ttraining's l1: 0.110665\tvalid_1's l1: 0.322317\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.110665\tvalid_1's l1: 0.322317\n",
      "Fold 4 started at Tue Jun 25 11:31:09 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.28062\tvalid_1's l1: 0.334036\n",
      "[1000]\ttraining's l1: 0.244869\tvalid_1's l1: 0.332156\n",
      "[1500]\ttraining's l1: 0.216551\tvalid_1's l1: 0.330423\n",
      "[2000]\ttraining's l1: 0.193732\tvalid_1's l1: 0.329338\n",
      "[2500]\ttraining's l1: 0.17455\tvalid_1's l1: 0.328076\n",
      "[3000]\ttraining's l1: 0.158195\tvalid_1's l1: 0.326745\n",
      "[3500]\ttraining's l1: 0.144121\tvalid_1's l1: 0.325874\n",
      "[4000]\ttraining's l1: 0.131532\tvalid_1's l1: 0.32509\n",
      "[4500]\ttraining's l1: 0.120652\tvalid_1's l1: 0.324327\n",
      "[5000]\ttraining's l1: 0.110825\tvalid_1's l1: 0.323943\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.110825\tvalid_1's l1: 0.323943\n",
      "Fold 5 started at Tue Jun 25 11:33:10 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.281239\tvalid_1's l1: 0.33\n",
      "[1000]\ttraining's l1: 0.244595\tvalid_1's l1: 0.327545\n",
      "[1500]\ttraining's l1: 0.216764\tvalid_1's l1: 0.326486\n",
      "[2000]\ttraining's l1: 0.193938\tvalid_1's l1: 0.325092\n",
      "[2500]\ttraining's l1: 0.17458\tvalid_1's l1: 0.323996\n",
      "[3000]\ttraining's l1: 0.15832\tvalid_1's l1: 0.323059\n",
      "[3500]\ttraining's l1: 0.144129\tvalid_1's l1: 0.322155\n",
      "[4000]\ttraining's l1: 0.131564\tvalid_1's l1: 0.321561\n",
      "[4500]\ttraining's l1: 0.120463\tvalid_1's l1: 0.321065\n",
      "[5000]\ttraining's l1: 0.110727\tvalid_1's l1: 0.320651\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.110727\tvalid_1's l1: 0.320651\n",
      "CV mean score: -1.1344, std: 0.0043.\n",
      "Training of type 1\n",
      "Fold 1 started at Tue Jun 25 11:35:10 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's l1: 0.558588\tvalid_1's l1: 0.598046\n",
      "Fold 2 started at Tue Jun 25 11:35:13 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's l1: 0.56263\tvalid_1's l1: 0.584717\n",
      "Fold 3 started at Tue Jun 25 11:35:16 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's l1: 0.56206\tvalid_1's l1: 0.596938\n",
      "Fold 4 started at Tue Jun 25 11:35:18 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's l1: 0.559527\tvalid_1's l1: 0.595586\n",
      "Fold 5 started at Tue Jun 25 11:35:21 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's l1: 0.558817\tvalid_1's l1: 0.586976\n",
      "CV mean score: -0.5235, std: 0.0093.\n",
      "Training of type 4\n",
      "Fold 1 started at Tue Jun 25 11:35:24 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.215827\tvalid_1's l1: 0.322892\n",
      "[1000]\ttraining's l1: 0.15446\tvalid_1's l1: 0.316577\n",
      "[1500]\ttraining's l1: 0.115686\tvalid_1's l1: 0.313031\n",
      "[2000]\ttraining's l1: 0.0887537\tvalid_1's l1: 0.31068\n",
      "[2500]\ttraining's l1: 0.0695568\tvalid_1's l1: 0.309205\n",
      "[3000]\ttraining's l1: 0.0552502\tvalid_1's l1: 0.308063\n",
      "[3500]\ttraining's l1: 0.0443724\tvalid_1's l1: 0.307422\n",
      "[4000]\ttraining's l1: 0.0359867\tvalid_1's l1: 0.30675\n",
      "[4500]\ttraining's l1: 0.0294562\tvalid_1's l1: 0.306222\n",
      "[5000]\ttraining's l1: 0.0242901\tvalid_1's l1: 0.305853\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0242901\tvalid_1's l1: 0.305853\n",
      "Fold 2 started at Tue Jun 25 11:37:05 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.213621\tvalid_1's l1: 0.321342\n",
      "[1000]\ttraining's l1: 0.153\tvalid_1's l1: 0.31551\n",
      "[1500]\ttraining's l1: 0.114092\tvalid_1's l1: 0.311939\n",
      "[2000]\ttraining's l1: 0.0874824\tvalid_1's l1: 0.309757\n",
      "[2500]\ttraining's l1: 0.0685608\tvalid_1's l1: 0.308333\n",
      "[3000]\ttraining's l1: 0.0543852\tvalid_1's l1: 0.307244\n",
      "[3500]\ttraining's l1: 0.0436323\tvalid_1's l1: 0.306558\n",
      "[4000]\ttraining's l1: 0.0354135\tvalid_1's l1: 0.305947\n",
      "[4500]\ttraining's l1: 0.0289471\tvalid_1's l1: 0.305643\n",
      "[5000]\ttraining's l1: 0.0238523\tvalid_1's l1: 0.305336\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0238523\tvalid_1's l1: 0.305336\n",
      "Fold 3 started at Tue Jun 25 11:39:09 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.216222\tvalid_1's l1: 0.318923\n",
      "[1000]\ttraining's l1: 0.154997\tvalid_1's l1: 0.314594\n",
      "[1500]\ttraining's l1: 0.115356\tvalid_1's l1: 0.310335\n",
      "[2000]\ttraining's l1: 0.0885643\tvalid_1's l1: 0.307237\n",
      "[2500]\ttraining's l1: 0.0695761\tvalid_1's l1: 0.305686\n",
      "[3000]\ttraining's l1: 0.0551619\tvalid_1's l1: 0.304622\n",
      "[3500]\ttraining's l1: 0.0445388\tvalid_1's l1: 0.303931\n",
      "[4000]\ttraining's l1: 0.0362125\tvalid_1's l1: 0.30335\n",
      "[4500]\ttraining's l1: 0.029684\tvalid_1's l1: 0.302899\n",
      "[5000]\ttraining's l1: 0.0245029\tvalid_1's l1: 0.302546\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0245029\tvalid_1's l1: 0.302546\n",
      "Fold 4 started at Tue Jun 25 11:41:02 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.216381\tvalid_1's l1: 0.323523\n",
      "[1000]\ttraining's l1: 0.154062\tvalid_1's l1: 0.316702\n",
      "[1500]\ttraining's l1: 0.115243\tvalid_1's l1: 0.312907\n",
      "[2000]\ttraining's l1: 0.088466\tvalid_1's l1: 0.310708\n",
      "[2500]\ttraining's l1: 0.069023\tvalid_1's l1: 0.309288\n",
      "[3000]\ttraining's l1: 0.0548594\tvalid_1's l1: 0.3081\n",
      "[3500]\ttraining's l1: 0.0440351\tvalid_1's l1: 0.307306\n",
      "[4000]\ttraining's l1: 0.0356958\tvalid_1's l1: 0.306796\n",
      "[4500]\ttraining's l1: 0.0291555\tvalid_1's l1: 0.306413\n",
      "[5000]\ttraining's l1: 0.0240357\tvalid_1's l1: 0.306093\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0240357\tvalid_1's l1: 0.306093\n",
      "Fold 5 started at Tue Jun 25 11:43:00 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.214273\tvalid_1's l1: 0.322674\n",
      "[1000]\ttraining's l1: 0.153185\tvalid_1's l1: 0.317422\n",
      "[1500]\ttraining's l1: 0.114626\tvalid_1's l1: 0.314563\n",
      "[2000]\ttraining's l1: 0.088197\tvalid_1's l1: 0.311904\n",
      "[2500]\ttraining's l1: 0.0689099\tvalid_1's l1: 0.309995\n",
      "[3000]\ttraining's l1: 0.0546137\tvalid_1's l1: 0.308776\n",
      "[3500]\ttraining's l1: 0.0439323\tvalid_1's l1: 0.307905\n",
      "[4000]\ttraining's l1: 0.0356213\tvalid_1's l1: 0.307284\n",
      "[4500]\ttraining's l1: 0.0291559\tvalid_1's l1: 0.306863\n",
      "[5000]\ttraining's l1: 0.0240071\tvalid_1's l1: 0.306423\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0240071\tvalid_1's l1: 0.306423\n",
      "CV mean score: -1.1866, std: 0.0046.\n",
      "Training of type 2\n",
      "Fold 1 started at Tue Jun 25 11:45:14 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.550013\tvalid_1's l1: 0.582056\n",
      "[1000]\ttraining's l1: 0.51688\tvalid_1's l1: 0.576372\n",
      "[1500]\ttraining's l1: 0.488791\tvalid_1's l1: 0.572256\n",
      "[2000]\ttraining's l1: 0.465496\tvalid_1's l1: 0.571131\n",
      "[2500]\ttraining's l1: 0.443298\tvalid_1's l1: 0.569559\n",
      "[3000]\ttraining's l1: 0.424124\tvalid_1's l1: 0.568847\n",
      "[3500]\ttraining's l1: 0.405696\tvalid_1's l1: 0.567659\n",
      "[4000]\ttraining's l1: 0.388895\tvalid_1's l1: 0.56659\n",
      "[4500]\ttraining's l1: 0.373221\tvalid_1's l1: 0.56591\n",
      "[5000]\ttraining's l1: 0.358487\tvalid_1's l1: 0.565231\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.358487\tvalid_1's l1: 0.565231\n",
      "Fold 2 started at Tue Jun 25 11:50:46 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.550382\tvalid_1's l1: 0.585291\n",
      "[1000]\ttraining's l1: 0.516535\tvalid_1's l1: 0.578286\n",
      "[1500]\ttraining's l1: 0.48859\tvalid_1's l1: 0.574054\n",
      "[2000]\ttraining's l1: 0.464601\tvalid_1's l1: 0.571977\n",
      "[2500]\ttraining's l1: 0.442741\tvalid_1's l1: 0.570228\n",
      "[3000]\ttraining's l1: 0.422842\tvalid_1's l1: 0.568563\n",
      "[3500]\ttraining's l1: 0.404689\tvalid_1's l1: 0.567362\n",
      "[4000]\ttraining's l1: 0.387507\tvalid_1's l1: 0.566386\n",
      "[4500]\ttraining's l1: 0.372115\tvalid_1's l1: 0.56587\n",
      "[5000]\ttraining's l1: 0.357326\tvalid_1's l1: 0.564906\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.357326\tvalid_1's l1: 0.564906\n",
      "Fold 3 started at Tue Jun 25 11:55:18 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.548811\tvalid_1's l1: 0.583275\n",
      "[1000]\ttraining's l1: 0.515307\tvalid_1's l1: 0.577449\n",
      "[1500]\ttraining's l1: 0.487464\tvalid_1's l1: 0.573927\n",
      "[2000]\ttraining's l1: 0.463615\tvalid_1's l1: 0.57227\n",
      "[2500]\ttraining's l1: 0.442193\tvalid_1's l1: 0.571141\n",
      "[3000]\ttraining's l1: 0.422439\tvalid_1's l1: 0.569767\n",
      "[3500]\ttraining's l1: 0.404465\tvalid_1's l1: 0.569043\n",
      "Early stopping, best iteration is:\n",
      "[3372]\ttraining's l1: 0.408774\tvalid_1's l1: 0.568938\n",
      "Fold 4 started at Tue Jun 25 11:58:03 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.547902\tvalid_1's l1: 0.582918\n",
      "[1000]\ttraining's l1: 0.514495\tvalid_1's l1: 0.576955\n",
      "[1500]\ttraining's l1: 0.487487\tvalid_1's l1: 0.574567\n",
      "[2000]\ttraining's l1: 0.463401\tvalid_1's l1: 0.572589\n",
      "[2500]\ttraining's l1: 0.441975\tvalid_1's l1: 0.571117\n",
      "[3000]\ttraining's l1: 0.422621\tvalid_1's l1: 0.570017\n",
      "Early stopping, best iteration is:\n",
      "[3230]\ttraining's l1: 0.414101\tvalid_1's l1: 0.569592\n",
      "Fold 5 started at Tue Jun 25 12:00:35 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.549505\tvalid_1's l1: 0.584235\n",
      "[1000]\ttraining's l1: 0.515537\tvalid_1's l1: 0.578301\n",
      "[1500]\ttraining's l1: 0.487853\tvalid_1's l1: 0.57469\n",
      "[2000]\ttraining's l1: 0.464348\tvalid_1's l1: 0.572727\n",
      "[2500]\ttraining's l1: 0.443039\tvalid_1's l1: 0.571609\n",
      "[3000]\ttraining's l1: 0.423375\tvalid_1's l1: 0.570411\n",
      "[3500]\ttraining's l1: 0.40523\tvalid_1's l1: 0.569384\n",
      "[4000]\ttraining's l1: 0.38866\tvalid_1's l1: 0.568633\n",
      "[4500]\ttraining's l1: 0.372832\tvalid_1's l1: 0.567821\n",
      "[5000]\ttraining's l1: 0.358133\tvalid_1's l1: 0.567111\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.358133\tvalid_1's l1: 0.567111\n",
      "CV mean score: -0.5671, std: 0.0033.\n",
      "Training of type 6\n",
      "Fold 1 started at Tue Jun 25 12:03:59 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.312405\tvalid_1's l1: 0.347098\n",
      "[1000]\ttraining's l1: 0.283862\tvalid_1's l1: 0.344758\n",
      "[1500]\ttraining's l1: 0.260624\tvalid_1's l1: 0.343536\n",
      "[2000]\ttraining's l1: 0.240764\tvalid_1's l1: 0.342794\n",
      "[2500]\ttraining's l1: 0.223365\tvalid_1's l1: 0.34214\n",
      "[3000]\ttraining's l1: 0.207932\tvalid_1's l1: 0.341455\n",
      "Early stopping, best iteration is:\n",
      "[3046]\ttraining's l1: 0.20658\tvalid_1's l1: 0.341283\n",
      "Fold 2 started at Tue Jun 25 12:05:21 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.311817\tvalid_1's l1: 0.345435\n",
      "[1000]\ttraining's l1: 0.2836\tvalid_1's l1: 0.343831\n",
      "[1500]\ttraining's l1: 0.260318\tvalid_1's l1: 0.342951\n",
      "[2000]\ttraining's l1: 0.240327\tvalid_1's l1: 0.342491\n",
      "[2500]\ttraining's l1: 0.222982\tvalid_1's l1: 0.341816\n",
      "[3000]\ttraining's l1: 0.207597\tvalid_1's l1: 0.34157\n",
      "Early stopping, best iteration is:\n",
      "[2825]\ttraining's l1: 0.212657\tvalid_1's l1: 0.341449\n",
      "Fold 3 started at Tue Jun 25 12:08:40 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.311559\tvalid_1's l1: 0.344923\n",
      "[1000]\ttraining's l1: 0.283171\tvalid_1's l1: 0.343201\n",
      "[1500]\ttraining's l1: 0.260264\tvalid_1's l1: 0.342188\n",
      "[2000]\ttraining's l1: 0.240672\tvalid_1's l1: 0.341674\n",
      "[2500]\ttraining's l1: 0.223327\tvalid_1's l1: 0.341143\n",
      "[3000]\ttraining's l1: 0.207786\tvalid_1's l1: 0.340521\n",
      "[3500]\ttraining's l1: 0.193997\tvalid_1's l1: 0.33986\n",
      "[4000]\ttraining's l1: 0.181418\tvalid_1's l1: 0.33955\n",
      "Early stopping, best iteration is:\n",
      "[4163]\ttraining's l1: 0.177562\tvalid_1's l1: 0.339313\n",
      "Fold 4 started at Tue Jun 25 12:15:22 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.311369\tvalid_1's l1: 0.34457\n",
      "[1000]\ttraining's l1: 0.283626\tvalid_1's l1: 0.342958\n",
      "[1500]\ttraining's l1: 0.260271\tvalid_1's l1: 0.341935\n",
      "Early stopping, best iteration is:\n",
      "[1419]\ttraining's l1: 0.263692\tvalid_1's l1: 0.34181\n",
      "Fold 5 started at Tue Jun 25 12:18:15 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.311762\tvalid_1's l1: 0.346394\n",
      "[1000]\ttraining's l1: 0.283408\tvalid_1's l1: 0.344415\n",
      "[1500]\ttraining's l1: 0.259726\tvalid_1's l1: 0.342799\n",
      "Early stopping, best iteration is:\n",
      "[1534]\ttraining's l1: 0.258221\tvalid_1's l1: 0.342704\n",
      "CV mean score: -1.0750, std: 0.0033.\n",
      "Training of type 5\n",
      "Fold 1 started at Tue Jun 25 12:20:46 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.570899\tvalid_1's l1: 0.596113\n",
      "[1000]\ttraining's l1: 0.543608\tvalid_1's l1: 0.590417\n",
      "[1500]\ttraining's l1: 0.521397\tvalid_1's l1: 0.587655\n",
      "[2000]\ttraining's l1: 0.501252\tvalid_1's l1: 0.585686\n",
      "[2500]\ttraining's l1: 0.483188\tvalid_1's l1: 0.584596\n",
      "[3000]\ttraining's l1: 0.466677\tvalid_1's l1: 0.583406\n",
      "[3500]\ttraining's l1: 0.451193\tvalid_1's l1: 0.582534\n",
      "[4000]\ttraining's l1: 0.436425\tvalid_1's l1: 0.581873\n",
      "[4500]\ttraining's l1: 0.422385\tvalid_1's l1: 0.581201\n",
      "[5000]\ttraining's l1: 0.409218\tvalid_1's l1: 0.580717\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.409218\tvalid_1's l1: 0.580717\n",
      "Fold 2 started at Tue Jun 25 12:32:06 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.571751\tvalid_1's l1: 0.597737\n",
      "[1000]\ttraining's l1: 0.54369\tvalid_1's l1: 0.590777\n",
      "[1500]\ttraining's l1: 0.520922\tvalid_1's l1: 0.587016\n",
      "[2000]\ttraining's l1: 0.500985\tvalid_1's l1: 0.585322\n",
      "[2500]\ttraining's l1: 0.482808\tvalid_1's l1: 0.583637\n",
      "[3000]\ttraining's l1: 0.466136\tvalid_1's l1: 0.582721\n",
      "[3500]\ttraining's l1: 0.450663\tvalid_1's l1: 0.581742\n",
      "[4000]\ttraining's l1: 0.436172\tvalid_1's l1: 0.581124\n",
      "[4500]\ttraining's l1: 0.422452\tvalid_1's l1: 0.580389\n",
      "[5000]\ttraining's l1: 0.40923\tvalid_1's l1: 0.580009\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.40923\tvalid_1's l1: 0.580009\n",
      "Fold 3 started at Tue Jun 25 12:40:32 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.571273\tvalid_1's l1: 0.595743\n",
      "[1000]\ttraining's l1: 0.544138\tvalid_1's l1: 0.590388\n",
      "[1500]\ttraining's l1: 0.522079\tvalid_1's l1: 0.587798\n",
      "[2000]\ttraining's l1: 0.501457\tvalid_1's l1: 0.585006\n",
      "[2500]\ttraining's l1: 0.483275\tvalid_1's l1: 0.583667\n",
      "[3000]\ttraining's l1: 0.466635\tvalid_1's l1: 0.582598\n",
      "[3500]\ttraining's l1: 0.451112\tvalid_1's l1: 0.581619\n",
      "[4000]\ttraining's l1: 0.436413\tvalid_1's l1: 0.58095\n",
      "[4500]\ttraining's l1: 0.42269\tvalid_1's l1: 0.580177\n",
      "[5000]\ttraining's l1: 0.40965\tvalid_1's l1: 0.579506\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.40965\tvalid_1's l1: 0.579506\n",
      "Fold 4 started at Tue Jun 25 12:47:26 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.569896\tvalid_1's l1: 0.599796\n",
      "[1000]\ttraining's l1: 0.542808\tvalid_1's l1: 0.594416\n",
      "[1500]\ttraining's l1: 0.520008\tvalid_1's l1: 0.591294\n",
      "[2000]\ttraining's l1: 0.499864\tvalid_1's l1: 0.588995\n",
      "[2500]\ttraining's l1: 0.481474\tvalid_1's l1: 0.587252\n",
      "[3000]\ttraining's l1: 0.464949\tvalid_1's l1: 0.586329\n",
      "[3500]\ttraining's l1: 0.449393\tvalid_1's l1: 0.585299\n",
      "Early stopping, best iteration is:\n",
      "[3616]\ttraining's l1: 0.445832\tvalid_1's l1: 0.585175\n",
      "Fold 5 started at Tue Jun 25 12:53:15 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.570755\tvalid_1's l1: 0.595717\n",
      "[1000]\ttraining's l1: 0.543545\tvalid_1's l1: 0.590127\n",
      "[1500]\ttraining's l1: 0.520892\tvalid_1's l1: 0.586835\n",
      "[2000]\ttraining's l1: 0.500953\tvalid_1's l1: 0.584877\n",
      "[2500]\ttraining's l1: 0.482993\tvalid_1's l1: 0.583602\n",
      "[3000]\ttraining's l1: 0.466267\tvalid_1's l1: 0.582793\n",
      "[3500]\ttraining's l1: 0.450645\tvalid_1's l1: 0.582035\n",
      "[4000]\ttraining's l1: 0.436192\tvalid_1's l1: 0.581588\n",
      "[4500]\ttraining's l1: 0.422536\tvalid_1's l1: 0.580842\n",
      "[5000]\ttraining's l1: 0.409206\tvalid_1's l1: 0.579951\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.409206\tvalid_1's l1: 0.579951\n",
      "CV mean score: -0.5429, std: 0.0036.\n",
      "Training of type 7\n",
      "Fold 1 started at Tue Jun 25 12:59:12 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[229]\ttraining's l1: 0.191469\tvalid_1's l1: 0.227557\n",
      "Fold 2 started at Tue Jun 25 12:59:20 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.163766\tvalid_1's l1: 0.225245\n",
      "Early stopping, best iteration is:\n",
      "[300]\ttraining's l1: 0.183508\tvalid_1's l1: 0.224552\n",
      "Fold 3 started at Tue Jun 25 12:59:30 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.163297\tvalid_1's l1: 0.227446\n",
      "[1000]\ttraining's l1: 0.126325\tvalid_1's l1: 0.226176\n",
      "[1500]\ttraining's l1: 0.100536\tvalid_1's l1: 0.22557\n",
      "[2000]\ttraining's l1: 0.081225\tvalid_1's l1: 0.224963\n",
      "[2500]\ttraining's l1: 0.0664454\tvalid_1's l1: 0.224243\n",
      "[3000]\ttraining's l1: 0.054974\tvalid_1's l1: 0.223595\n",
      "[3500]\ttraining's l1: 0.0458692\tvalid_1's l1: 0.223098\n",
      "[4000]\ttraining's l1: 0.038524\tvalid_1's l1: 0.22248\n",
      "[4500]\ttraining's l1: 0.0325311\tvalid_1's l1: 0.222116\n",
      "[5000]\ttraining's l1: 0.0276483\tvalid_1's l1: 0.221723\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0276483\tvalid_1's l1: 0.221723\n",
      "Fold 4 started at Tue Jun 25 13:01:14 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.163186\tvalid_1's l1: 0.227419\n",
      "Early stopping, best iteration is:\n",
      "[313]\ttraining's l1: 0.182221\tvalid_1's l1: 0.227335\n",
      "Fold 5 started at Tue Jun 25 13:01:25 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.162791\tvalid_1's l1: 0.225852\n",
      "Early stopping, best iteration is:\n",
      "[342]\ttraining's l1: 0.17817\tvalid_1's l1: 0.225437\n",
      "CV mean score: -1.4903, std: 0.0095.\n"
     ]
    }
   ],
   "source": [
    "X_short = pd.DataFrame({'ind': list(X.index), 'type': X['type'].values, 'oof': [0] * len(X), 'target': y.values})\n",
    "X_short_test = pd.DataFrame({'ind': list(X_test.index), 'type': X_test['type'].values, 'prediction': [0] * len(X_test)})\n",
    "for t in X['type'].unique():\n",
    "    print(f'Training of type {t}')\n",
    "    X_t = X.loc[X['type'] == t]\n",
    "    X_test_t = X_test.loc[X_test['type'] == t]\n",
    "    y_t = X_short.loc[X_short['type'] == t, 'target']\n",
    "    result_dict_lgb3 = train_model_regression(X=X_t, X_test=X_test_t, y=y_t, params=params, folds=folds, model_type='lgb', eval_metric='group_mae', plot_feature_importance=False,\n",
    "                                                      verbose=500, early_stopping_rounds=200, n_estimators=5000)\n",
    "    X_short.loc[X_short['type'] == t, 'oof'] = result_dict_lgb3['oof']\n",
    "    X_short_test.loc[X_short_test['type'] == t, 'prediction'] = result_dict_lgb3['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id7\"></a> <br> \n",
    "# **7. Submittion** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>11.159473868181795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>184.559589580373114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>4.280129409511890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>184.052079240338571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>7.969273807902202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  scalar_coupling_constant\n",
       "0  4658147        11.159473868181795\n",
       "1  4658148       184.559589580373114\n",
       "2  4658149         4.280129409511890\n",
       "3  4658150       184.052079240338571\n",
       "4  4658151         7.969273807902202"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training models for type\n",
    "sub['scalar_coupling_constant'] = X_short_test['prediction']\n",
    "sub.to_csv('../submissions/PK_eachtype_moreiterations_submission_CV-1.4903.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain with more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of type 0\n",
      "Fold 1 started at Tue Jun 25 13:17:54 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's l1: 1.33406\tvalid_1's l1: 1.34506\n",
      "Fold 2 started at Tue Jun 25 13:18:09 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[121]\ttraining's l1: 1.31288\tvalid_1's l1: 1.34063\n",
      "Fold 3 started at Tue Jun 25 13:18:21 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's l1: 1.30781\tvalid_1's l1: 1.34314\n",
      "Fold 4 started at Tue Jun 25 13:18:38 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's l1: 1.33213\tvalid_1's l1: 1.3427\n",
      "Fold 5 started at Tue Jun 25 13:18:53 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttraining's l1: 1.32653\tvalid_1's l1: 1.33783\n",
      "CV mean score: 0.2941, std: 0.0018.\n",
      "Training of type 3\n",
      "Fold 1 started at Tue Jun 25 13:19:06 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.28061\tvalid_1's l1: 0.329963\n",
      "[1000]\ttraining's l1: 0.244381\tvalid_1's l1: 0.327232\n"
     ]
    }
   ],
   "source": [
    "X_short = pd.DataFrame({'ind': list(X.index), 'type': X['type'].values, 'oof': [0] * len(X), 'target': y.values})\n",
    "X_short_test = pd.DataFrame({'ind': list(X_test.index), 'type': X_test['type'].values, 'prediction': [0] * len(X_test)})\n",
    "for t in X['type'].unique():\n",
    "    print(f'Training of type {t}')\n",
    "    X_t = X.loc[X['type'] == t]\n",
    "    X_test_t = X_test.loc[X_test['type'] == t]\n",
    "    y_t = X_short.loc[X_short['type'] == t, 'target']\n",
    "    result_dict_lgb3 = train_model_regression(X=X_t, X_test=X_test_t, y=y_t, params=params, folds=folds, model_type='lgb', eval_metric='group_mae', plot_feature_importance=False,\n",
    "                                                      verbose=500, early_stopping_rounds=200, n_estimators=50000)\n",
    "    X_short.loc[X_short['type'] == t, 'oof'] = result_dict_lgb3['oof']\n",
    "    X_short_test.loc[X_short_test['type'] == t, 'prediction'] = result_dict_lgb3['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training models for type\n",
    "sub['scalar_coupling_constant'] = X_short_test['prediction']\n",
    "sub.to_csv('../submissions/PK002_eachtype_moreiterations_submission_CV-1.4903.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref\"></a> <br> \n",
    "# **8. References** \n",
    "\n",
    "[1] OOF Model: https://www.kaggle.com/adarshchavakula/out-of-fold-oof-model-cross-validation<br>\n",
    "[2] Using Meta Features: https://www.kaggle.com/artgor/using-meta-features-to-improve-model<br>\n",
    "[3] Lot of Features: https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b <br>\n",
    "[4] Angle Feature: https://www.kaggle.com/kmat2019/effective-feature <br>\n",
    "[5] Recovering bonds from structure: https://www.kaggle.com/aekoch95/bonds-from-structure-data <br>\n",
    "\n",
    "<h3 style=\"color:red\">If this Kernel Helps You! Please UP VOTE! üòÅ</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
