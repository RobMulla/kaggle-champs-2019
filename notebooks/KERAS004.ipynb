{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Pipeline 3-Folds\n",
    "- Use features including `fc` meta feature from tabular models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "def get_logger():\n",
    "    \"\"\"\n",
    "        credits to: https://www.kaggle.com/ogrellier/user-level-lightgbm-lb-1-4480\n",
    "    \"\"\"\n",
    "    FORMAT = \"[%(asctime)s] %(levelname)s : %(message)s\"\n",
    "    logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "    logger = logging.getLogger(\"main\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_struct=pd.read_csv('../input/structures.csv')\n",
    "df_train_sub_charge=pd.read_csv('../input/mulliken_charges.csv')\n",
    "df_train_sub_tensor=pd.read_csv('../input/magnetic_shielding_tensors.csv')\n",
    "train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGURABLES #######\n",
    "bond_type = '3JHH'\n",
    "MODEL_NUMBER = 'K004'\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, label):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(history.history['loss'][-100:])\n",
    "    plt.plot(history.history['val_loss'][-100:])\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _= plt.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GPU preferences\n",
    "config = tf.ConfigProto( device_count = {'GPU': 2} ) \n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "sess = tf.Session(config=config) \n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df_1,df_2, atom_idx):\n",
    "    #logging.info('Mapping...', df_1.shape, df_2.shape, atom_idx)\n",
    "    df = pd.merge(df_1, df_2.drop_duplicates(subset=['molecule_name', 'atom_index']), how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    return df\n",
    "\n",
    "def create_datasets(bond_type, X_train_file, fold):\n",
    "    logger.info('Creating Datasets')\n",
    "    # Read input for first fold\n",
    "    X_test_file = X_train_file.replace('X_train','X_test')\n",
    "    X_valid_file = X_train_file.replace('X_train','X_valid')\n",
    "    X_train = pd.read_parquet(f'../type_results/{bond_type}/meta/{X_train_file}') \n",
    "    X_test = pd.read_parquet(f'../type_results/{bond_type}/meta/{X_test_file}') \n",
    "    X_valid = pd.read_parquet(f'../type_results/{bond_type}/meta/{X_valid_file}') \n",
    "    X_train['split'] = 'TRAIN'\n",
    "    X_test['split'] = 'TEST'\n",
    "    X_valid['split'] = 'VALID'\n",
    "    logger.info('Adding target to dataset')\n",
    "    # Add target to train and val\n",
    "    X_tr_val = pd.concat([X_train, X_valid])\n",
    "    X_tr_val = X_tr_val.sort_index()\n",
    "    X_tr_val['scalar_coupling_constant'] = train.loc[train['type'] == '3JHH']['scalar_coupling_constant'].tolist()\n",
    "    X_tr_val['molecule_name'] = train.loc[train['type'] == '3JHH']['molecule_name'].tolist()\n",
    "    X_tr_val['atom_index_0'] = train.loc[train['type'] == '3JHH']['atom_index_0'].tolist()\n",
    "    X_tr_val['atom_index_1'] = train.loc[train['type'] == '3JHH']['atom_index_1'].tolist()\n",
    "\n",
    "    # Combine all\n",
    "    X_all = pd.concat([X_tr_val, X_test])\n",
    "    logger.info('Adding custom target features')\n",
    "    for atom_idx in [0,1]:\n",
    "        X_all = map_atom_info(X_all,df_struct, atom_idx)\n",
    "        X_all = map_atom_info(X_all,df_train_sub_charge, atom_idx)\n",
    "        X_all = map_atom_info(X_all,df_train_sub_tensor, atom_idx)\n",
    "        X_all = X_all.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                                            'x': f'x_{atom_idx}',\n",
    "                                            'y': f'y_{atom_idx}',\n",
    "                                            'z': f'z_{atom_idx}',\n",
    "                                            'mulliken_charge': f'charge_{atom_idx}',\n",
    "                                            'XX': f'XX_{atom_idx}',\n",
    "                                            'YX': f'YX_{atom_idx}',\n",
    "                                            'ZX': f'ZX_{atom_idx}',\n",
    "                                            'XY': f'XY_{atom_idx}',\n",
    "                                            'YY': f'YY_{atom_idx}',\n",
    "                                            'ZY': f'ZY_{atom_idx}',\n",
    "                                            'XZ': f'XZ_{atom_idx}',\n",
    "                                            'YZ': f'YZ_{atom_idx}',\n",
    "                                            'ZZ': f'ZZ_{atom_idx}',})\n",
    "    \n",
    "    ys_all = X_all[['scalar_coupling_constant',\"charge_0\",\"charge_1\",\n",
    "                \"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\",\"YX_0\",\"ZX_0\",\n",
    "                \"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\n",
    "                \"XZ_1\",\"YZ_1\",\"split\"]].copy()\n",
    "    target_all = ys_all[['scalar_coupling_constant','split']].copy()\n",
    "    splits = X_all['split']\n",
    "    ys_all = ys_all.drop('split', axis=1)\n",
    "    \n",
    "    X_all = X_all.drop(['scalar_coupling_constant',\"charge_0\",\"charge_1\",\n",
    "                \"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\",\"YX_0\",\"ZX_0\",\n",
    "                \"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\n",
    "                \"XZ_1\",\"YZ_1\"], axis=1)\n",
    "    # Remove non numeric columns\n",
    "    X_all = X_all.drop(['molecule_name', 'split', 'atom_0', 'atom_1'], axis=1)\n",
    "\n",
    "    #Impute NA with mean\n",
    "    # THIS PART TAKES A LONG TIME\n",
    "    logger.info('Filling in NA vaules with the mean value')\n",
    "    MEAN = X_all.mean()\n",
    "    X_all.fillna( value=MEAN, inplace=True )\n",
    "    \n",
    "    # STANDARD SCALAR STUFF\n",
    "    logger.info('Applying Standard scalar to data')\n",
    "    X_all[X_all.columns] = StandardScaler().fit_transform(X_all[X_all.columns])\n",
    "    ys_all[ys_all.columns] = StandardScaler().fit_transform(ys_all[ys_all.columns])\n",
    "    \n",
    "    X_train = X_all.loc[splits == 'TRAIN']\n",
    "    X_valid = X_all.loc[splits == 'VALID']\n",
    "    X_test = X_all.loc[splits == 'TEST']\n",
    "    \n",
    "    y_train = ys_all.loc[splits == 'TRAIN']\n",
    "    y_valid = ys_all.loc[splits == 'VALID']\n",
    "    y_test = ys_all.loc[splits == 'TEST']\n",
    "    \n",
    "    target_train = target_all[target_all['split'] == 'TRAIN']['scalar_coupling_constant']\n",
    "    target_valid = target_all[target_all['split']  == 'VALID']['scalar_coupling_constant']\n",
    "    target_test = target_all[target_all['split']  == 'TEST']['scalar_coupling_constant']\n",
    "\n",
    "    m1=2\n",
    "    m2=4\n",
    "    m3=1\n",
    "\n",
    "    train_input=X_train.values\n",
    "    cv_input=X_valid.values\n",
    "    train_target=target_train.values\n",
    "    cv_target=target_valid.values\n",
    "    train_target_1=m1 * y_train[[\"charge_0\",\"charge_1\"]].values\n",
    "    cv_target_1=m1 * y_valid[[\"charge_0\",\"charge_1\"]].values\n",
    "    train_target_2=m2 * y_train[[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]].values\n",
    "    cv_target_2=m2 * y_valid[[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]].values\n",
    "    train_target_3=m3 * y_train[[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]].values\n",
    "    cv_target_3=m3 * y_valid[[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]].values\n",
    "    test_input=X_test.values\n",
    "    logger.info('Done creating data for model')\n",
    "    return train_input, cv_input, train_target, cv_target, train_target_1, cv_target_1, train_target_2, cv_target_2, train_target_3, cv_target_3, test_input, X_valid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(1024)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    out1 = Dense(2, activation=\"linear\",name='outM2')(x)#mulliken charge 2\n",
    "    out2 = Dense(6, activation=\"linear\",name='outT6')(x)#tensor 6(xx,yy,zz)\n",
    "    out3 = Dense(12, activation=\"linear\",name='outT12')(x)#tensor 12(others) \n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    out = Dense(1, activation=\"linear\",name='out')(x)#scalar_coupling_constant    \n",
    "    model = Model(inputs=inp, outputs=[out,out1,out2,out3])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_type_data(\n",
    "    type_,\n",
    "    oof,\n",
    "    sub,\n",
    "    fi,\n",
    "    MODEL_NUMBER,\n",
    "    run_id,\n",
    "    MODEL_TYPE,\n",
    "    N_FOLDS,\n",
    "    N_ESTIMATORS,\n",
    "    LEARNING_RATE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves the oof, sub, and fi files int he type_results folder with correct naming convention\n",
    "    \"\"\"\n",
    "    oof_type = oof.loc[oof[\"type\"] == type_]\n",
    "    score = mean_absolute_error(\n",
    "        oof_type[\"scalar_coupling_constant\"], oof_type[\"oof_preds\"]\n",
    "    )\n",
    "    logscore = np.log(score)\n",
    "    if score > 1:\n",
    "        logger.error(f\"No predictions for {type_}\")\n",
    "    print(\n",
    "        f\"===== Saving results for for type {type_} - mae {score} - log mae {logscore}\"\n",
    "    )\n",
    "\n",
    "    oof_type = oof.loc[oof[\"type\"] == type_]\n",
    "\n",
    "    sub_type = test[[\"id\", \"molecule_name\", \"type\"]].merge(sub, on=\"id\")\n",
    "    sub_type = sub_type.loc[sub_type[\"type\"] == type_]\n",
    "    if np.sum(sub_type[\"scalar_coupling_constant\"] == 0) > 10:\n",
    "        logger.error(\"ERROR! Sub has too many zero predictions\")\n",
    "    expected_len = len(test.loc[test[\"type\"] == type_])\n",
    "    if expected_len != len(sub_type):\n",
    "        logger.error(\"ERRROR LENGTHS NOT THE SAME\")\n",
    "\n",
    "    # Name Files and save\n",
    "    fn_template = \"../type_results/{}/{}_{}_{}_XXXXXXX_{:0.4f}MAE_{:0.4}LMAE_{}_{}folds_{}iter_{}lr.parquet\".format(\n",
    "        type_,\n",
    "        MODEL_NUMBER,\n",
    "        run_id,\n",
    "        type_,\n",
    "        score,\n",
    "        logscore,\n",
    "        MODEL_TYPE,\n",
    "        N_FOLDS,\n",
    "        N_ESTIMATORS,\n",
    "        LEARNING_RATE,\n",
    "    )\n",
    "    sub_name = fn_template.replace(\"XXXXXXX\", \"sub\")\n",
    "    oof_name = fn_template.replace(\"XXXXXXX\", \"oof\")\n",
    "    sub_type.to_parquet(sub_name)\n",
    "    oof_type.to_parquet(oof_name)\n",
    "\n",
    "    logger.info(f'{type_}: Saving sub to {sub_name}')\n",
    "    logger.info(f'{type_}: Saving oof to {oof_name}')\n",
    "\n",
    "    if fi is not None:\n",
    "        fi_type = fi.loc[fi[\"type\"] == type_]\n",
    "        fi_name = fn_template.replace(\"XXXXXXX\", \"fi\")\n",
    "        print(fi_name)\n",
    "        fi_type.to_parquet(fi_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keras_model(fold, MODEL_NUMBER, bond_type, X_train_file, EPOCH_N=500, BATCH_SIZE=4096):\n",
    "    logger.info(f'Training model for fold {fold}')\n",
    "    model_name_wrt = (f'{MODEL_NUMBER}molecule_model_{bond_type}-{fold}.hdf5')\n",
    "    train_input, cv_input, train_target, cv_target, train_target_1, cv_target_1, train_target_2, cv_target_2, train_target_3, cv_target_3, test_input, val_idx = create_datasets(bond_type, X_train_file, fold)\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model=create_nn_model(train_input.shape[1])\n",
    "    nn_model.compile(loss=\"mae\", optimizer=Adam())\n",
    "\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor=\"val_out_loss\",\n",
    "        min_delta=0.0005,\n",
    "        patience=30,\n",
    "        verbose=1,\n",
    "        mode=\"auto\",\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    rlr = callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_out_loss\",\n",
    "        factor=0.3333,\n",
    "        patience=15,\n",
    "        min_lr=1e-6,\n",
    "        mode=\"auto\",\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    sv_mod = callbacks.ModelCheckpoint(\n",
    "        model_name_wrt, monitor=\"val_out_loss\", save_best_only=True, period=1\n",
    "    )\n",
    "\n",
    "    history = nn_model.fit(train_input,[train_target,train_target_1,train_target_2,train_target_3], \n",
    "                       validation_data=(cv_input,[cv_target,cv_target_1,cv_target_2,cv_target_3]), \n",
    "                       callbacks=[es, rlr, sv_mod],\n",
    "                       epochs=EPOCH_N,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       verbose=verbose)\n",
    "    \n",
    "    cv_predict=nn_model.predict(cv_input)\n",
    "    test_predict=nn_model.predict(test_input)\n",
    "    plot_history(history, bond_type)\n",
    "    \n",
    "    # CREATE OOF SUB AND STUFF\n",
    "    return cv_predict, test_predict, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-31 21:55:41,639 | INFO : Using X_train file M053_0725_0821_3JHH_X_train_meta_fc_f1_0.1753MAE_-1.7413LMAE.parquet\n",
      "2019-07-31 21:55:41,640 | INFO : Training model for fold 1\n",
      "2019-07-31 21:55:41,641 | INFO : Creating Datasets\n",
      "2019-07-31 21:55:42,611 | INFO : Adding target to dataset\n",
      "2019-07-31 21:55:48,572 | INFO : Adding custom target features\n",
      "2019-07-31 21:57:03,175 | INFO : Filling in NA vaules with the mean value\n",
      "2019-07-31 21:57:06,852 | INFO : Applying Standard scalar to data\n",
      "2019-07-31 21:58:13,942 | INFO : Done creating data for model\n",
      "2019-07-31 21:58:14,080 | WARNING : From /home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2019-07-31 21:58:14,081 | WARNING : From /home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2019-07-31 21:58:14,087 | WARNING : From /home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2019-07-31 21:58:14,154 | WARNING : From /home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2019-07-31 21:58:14,169 | WARNING : From /home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "2019-07-31 21:58:14,649 | WARNING : From /home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2019-07-31 21:58:15,561 | WARNING : From /home/robmulla/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 393740 samples, validate on 196871 samples\n",
      "Epoch 1/500\n",
      " - 9s - loss: 4.6447 - out_loss: 1.1379 - outM2_loss: 0.3927 - outT6_loss: 2.3148 - outT12_loss: 0.7993 - val_loss: 4.0876 - val_out_loss: 0.9249 - val_outM2_loss: 0.3730 - val_outT6_loss: 2.0265 - val_outT12_loss: 0.7631\n",
      "Epoch 2/500\n",
      " - 7s - loss: 3.3520 - out_loss: 0.3842 - outM2_loss: 0.2436 - outT6_loss: 1.9626 - outT12_loss: 0.7616 - val_loss: 3.5302 - val_out_loss: 0.6835 - val_outM2_loss: 0.2589 - val_outT6_loss: 1.8310 - val_outT12_loss: 0.7568\n",
      "Epoch 3/500\n",
      " - 7s - loss: 3.1199 - out_loss: 0.3292 - outM2_loss: 0.2066 - outT6_loss: 1.8264 - outT12_loss: 0.7577 - val_loss: 3.1088 - val_out_loss: 0.4604 - val_outM2_loss: 0.1928 - val_outT6_loss: 1.6991 - val_outT12_loss: 0.7566\n",
      "Epoch 4/500\n",
      " - 7s - loss: 2.9998 - out_loss: 0.3145 - outM2_loss: 0.1886 - outT6_loss: 1.7404 - outT12_loss: 0.7564 - val_loss: 3.0719 - val_out_loss: 0.4819 - val_outM2_loss: 0.1758 - val_outT6_loss: 1.6597 - val_outT12_loss: 0.7545\n",
      "Epoch 5/500\n",
      " - 6s - loss: 2.9194 - out_loss: 0.3009 - outM2_loss: 0.1764 - outT6_loss: 1.6866 - outT12_loss: 0.7555 - val_loss: 2.9051 - val_out_loss: 0.4338 - val_outM2_loss: 0.1469 - val_outT6_loss: 1.5710 - val_outT12_loss: 0.7535\n",
      "Epoch 6/500\n",
      " - 7s - loss: 2.8340 - out_loss: 0.2862 - outM2_loss: 0.1639 - outT6_loss: 1.6297 - outT12_loss: 0.7542 - val_loss: 2.6890 - val_out_loss: 0.2767 - val_outM2_loss: 0.1208 - val_outT6_loss: 1.5389 - val_outT12_loss: 0.7525\n",
      "Epoch 7/500\n",
      " - 7s - loss: 2.7739 - out_loss: 0.2751 - outM2_loss: 0.1569 - outT6_loss: 1.5884 - outT12_loss: 0.7536 - val_loss: 2.5968 - val_out_loss: 0.2298 - val_outM2_loss: 0.1232 - val_outT6_loss: 1.4911 - val_outT12_loss: 0.7527\n",
      "Epoch 8/500\n",
      " - 7s - loss: 2.7391 - out_loss: 0.2777 - outM2_loss: 0.1511 - outT6_loss: 1.5575 - outT12_loss: 0.7529 - val_loss: 2.6050 - val_out_loss: 0.2564 - val_outM2_loss: 0.1421 - val_outT6_loss: 1.4540 - val_outT12_loss: 0.7524\n",
      "Epoch 9/500\n",
      " - 7s - loss: 2.6944 - out_loss: 0.2679 - outM2_loss: 0.1465 - outT6_loss: 1.5277 - outT12_loss: 0.7523 - val_loss: 2.5049 - val_out_loss: 0.2180 - val_outM2_loss: 0.1133 - val_outT6_loss: 1.4224 - val_outT12_loss: 0.7511\n",
      "Epoch 10/500\n",
      " - 6s - loss: 2.6630 - out_loss: 0.2683 - outM2_loss: 0.1417 - outT6_loss: 1.5013 - outT12_loss: 0.7517 - val_loss: 2.4992 - val_out_loss: 0.2432 - val_outM2_loss: 0.0976 - val_outT6_loss: 1.4073 - val_outT12_loss: 0.7510\n",
      "Epoch 11/500\n",
      " - 7s - loss: 2.6233 - out_loss: 0.2575 - outM2_loss: 0.1388 - outT6_loss: 1.4756 - outT12_loss: 0.7514 - val_loss: 2.4392 - val_out_loss: 0.2210 - val_outM2_loss: 0.0812 - val_outT6_loss: 1.3874 - val_outT12_loss: 0.7497\n",
      "Epoch 12/500\n",
      " - 6s - loss: 2.5972 - out_loss: 0.2550 - outM2_loss: 0.1347 - outT6_loss: 1.4564 - outT12_loss: 0.7511 - val_loss: 2.4494 - val_out_loss: 0.2136 - val_outM2_loss: 0.1089 - val_outT6_loss: 1.3771 - val_outT12_loss: 0.7497\n",
      "Epoch 13/500\n",
      " - 7s - loss: 2.5781 - out_loss: 0.2559 - outM2_loss: 0.1336 - outT6_loss: 1.4380 - outT12_loss: 0.7506 - val_loss: 2.4034 - val_out_loss: 0.2007 - val_outM2_loss: 0.1023 - val_outT6_loss: 1.3516 - val_outT12_loss: 0.7488\n",
      "Epoch 14/500\n",
      " - 6s - loss: 2.5565 - out_loss: 0.2512 - outM2_loss: 0.1350 - outT6_loss: 1.4200 - outT12_loss: 0.7503 - val_loss: 2.3855 - val_out_loss: 0.2107 - val_outM2_loss: 0.0965 - val_outT6_loss: 1.3285 - val_outT12_loss: 0.7497\n",
      "Epoch 15/500\n",
      " - 6s - loss: 2.5319 - out_loss: 0.2485 - outM2_loss: 0.1296 - outT6_loss: 1.4038 - outT12_loss: 0.7500 - val_loss: 2.3690 - val_out_loss: 0.2021 - val_outM2_loss: 0.1016 - val_outT6_loss: 1.3165 - val_outT12_loss: 0.7488\n",
      "Epoch 16/500\n",
      " - 6s - loss: 2.5102 - out_loss: 0.2445 - outM2_loss: 0.1297 - outT6_loss: 1.3866 - outT12_loss: 0.7493 - val_loss: 2.3333 - val_out_loss: 0.2052 - val_outM2_loss: 0.0719 - val_outT6_loss: 1.3082 - val_outT12_loss: 0.7481\n",
      "Epoch 17/500\n",
      " - 6s - loss: 2.4941 - out_loss: 0.2435 - outM2_loss: 0.1263 - outT6_loss: 1.3754 - outT12_loss: 0.7490 - val_loss: 2.3215 - val_out_loss: 0.1969 - val_outM2_loss: 0.0808 - val_outT6_loss: 1.2953 - val_outT12_loss: 0.7484\n",
      "Epoch 18/500\n",
      " - 6s - loss: 2.4702 - out_loss: 0.2387 - outM2_loss: 0.1232 - outT6_loss: 1.3596 - outT12_loss: 0.7488 - val_loss: 2.3178 - val_out_loss: 0.2023 - val_outM2_loss: 0.0975 - val_outT6_loss: 1.2694 - val_outT12_loss: 0.7487\n",
      "Epoch 19/500\n",
      " - 7s - loss: 2.4684 - out_loss: 0.2428 - outM2_loss: 0.1282 - outT6_loss: 1.3488 - outT12_loss: 0.7486 - val_loss: 2.2999 - val_out_loss: 0.1998 - val_outM2_loss: 0.0902 - val_outT6_loss: 1.2628 - val_outT12_loss: 0.7471\n",
      "Epoch 20/500\n",
      " - 6s - loss: 2.4470 - out_loss: 0.2391 - outM2_loss: 0.1244 - outT6_loss: 1.3355 - outT12_loss: 0.7479 - val_loss: 2.2673 - val_out_loss: 0.1903 - val_outM2_loss: 0.0758 - val_outT6_loss: 1.2537 - val_outT12_loss: 0.7474\n",
      "Epoch 21/500\n",
      " - 6s - loss: 2.4343 - out_loss: 0.2399 - outM2_loss: 0.1225 - outT6_loss: 1.3244 - outT12_loss: 0.7475 - val_loss: 2.2885 - val_out_loss: 0.2212 - val_outM2_loss: 0.0765 - val_outT6_loss: 1.2450 - val_outT12_loss: 0.7459\n",
      "Epoch 22/500\n",
      " - 6s - loss: 2.4241 - out_loss: 0.2379 - outM2_loss: 0.1237 - outT6_loss: 1.3153 - outT12_loss: 0.7473 - val_loss: 2.2655 - val_out_loss: 0.2043 - val_outM2_loss: 0.0797 - val_outT6_loss: 1.2350 - val_outT12_loss: 0.7464\n",
      "Epoch 23/500\n",
      " - 6s - loss: 2.4071 - out_loss: 0.2369 - outM2_loss: 0.1182 - outT6_loss: 1.3052 - outT12_loss: 0.7468 - val_loss: 2.2530 - val_out_loss: 0.2110 - val_outM2_loss: 0.0661 - val_outT6_loss: 1.2301 - val_outT12_loss: 0.7458\n",
      "Epoch 24/500\n",
      " - 6s - loss: 2.3942 - out_loss: 0.2373 - outM2_loss: 0.1177 - outT6_loss: 1.2927 - outT12_loss: 0.7465 - val_loss: 2.2648 - val_out_loss: 0.1978 - val_outM2_loss: 0.0956 - val_outT6_loss: 1.2256 - val_outT12_loss: 0.7458\n",
      "Epoch 25/500\n",
      " - 6s - loss: 2.3798 - out_loss: 0.2313 - outM2_loss: 0.1174 - outT6_loss: 1.2850 - outT12_loss: 0.7461 - val_loss: 2.2224 - val_out_loss: 0.1863 - val_outM2_loss: 0.0809 - val_outT6_loss: 1.2097 - val_outT12_loss: 0.7455\n",
      "Epoch 26/500\n",
      " - 6s - loss: 2.3719 - out_loss: 0.2326 - outM2_loss: 0.1176 - outT6_loss: 1.2757 - outT12_loss: 0.7460 - val_loss: 2.2331 - val_out_loss: 0.2099 - val_outM2_loss: 0.0747 - val_outT6_loss: 1.2039 - val_outT12_loss: 0.7447\n",
      "Epoch 27/500\n",
      " - 6s - loss: 2.3662 - out_loss: 0.2330 - outM2_loss: 0.1182 - outT6_loss: 1.2694 - outT12_loss: 0.7457 - val_loss: 2.2129 - val_out_loss: 0.1944 - val_outM2_loss: 0.0819 - val_outT6_loss: 1.1913 - val_outT12_loss: 0.7452\n",
      "Epoch 28/500\n",
      " - 6s - loss: 2.3468 - out_loss: 0.2285 - outM2_loss: 0.1161 - outT6_loss: 1.2571 - outT12_loss: 0.7451 - val_loss: 2.2071 - val_out_loss: 0.1888 - val_outM2_loss: 0.0893 - val_outT6_loss: 1.1847 - val_outT12_loss: 0.7442\n",
      "Epoch 29/500\n",
      " - 7s - loss: 2.3447 - out_loss: 0.2303 - outM2_loss: 0.1172 - outT6_loss: 1.2525 - outT12_loss: 0.7448 - val_loss: 2.2195 - val_out_loss: 0.1862 - val_outM2_loss: 0.1061 - val_outT6_loss: 1.1836 - val_outT12_loss: 0.7436\n",
      "Epoch 30/500\n",
      " - 6s - loss: 2.3300 - out_loss: 0.2264 - outM2_loss: 0.1142 - outT6_loss: 1.2452 - outT12_loss: 0.7442 - val_loss: 2.1884 - val_out_loss: 0.2022 - val_outM2_loss: 0.0668 - val_outT6_loss: 1.1764 - val_outT12_loss: 0.7430\n",
      "Epoch 31/500\n",
      " - 6s - loss: 2.3219 - out_loss: 0.2295 - outM2_loss: 0.1123 - outT6_loss: 1.2363 - outT12_loss: 0.7438 - val_loss: 2.1696 - val_out_loss: 0.1878 - val_outM2_loss: 0.0798 - val_outT6_loss: 1.1599 - val_outT12_loss: 0.7420\n",
      "Epoch 32/500\n",
      " - 6s - loss: 2.3157 - out_loss: 0.2277 - outM2_loss: 0.1137 - outT6_loss: 1.2310 - outT12_loss: 0.7434 - val_loss: 2.1649 - val_out_loss: 0.2008 - val_outM2_loss: 0.0675 - val_outT6_loss: 1.1534 - val_outT12_loss: 0.7432\n",
      "Epoch 33/500\n",
      " - 7s - loss: 2.3116 - out_loss: 0.2284 - outM2_loss: 0.1137 - outT6_loss: 1.2263 - outT12_loss: 0.7433 - val_loss: 2.1838 - val_out_loss: 0.2027 - val_outM2_loss: 0.0756 - val_outT6_loss: 1.1640 - val_outT12_loss: 0.7415\n",
      "Epoch 34/500\n",
      " - 6s - loss: 2.3017 - out_loss: 0.2285 - outM2_loss: 0.1121 - outT6_loss: 1.2185 - outT12_loss: 0.7427 - val_loss: 2.1432 - val_out_loss: 0.1894 - val_outM2_loss: 0.0684 - val_outT6_loss: 1.1444 - val_outT12_loss: 0.7410\n",
      "Epoch 35/500\n",
      " - 6s - loss: 2.2890 - out_loss: 0.2246 - outM2_loss: 0.1099 - outT6_loss: 1.2119 - outT12_loss: 0.7426 - val_loss: 2.1720 - val_out_loss: 0.2206 - val_outM2_loss: 0.0728 - val_outT6_loss: 1.1379 - val_outT12_loss: 0.7407\n",
      "Epoch 36/500\n",
      " - 6s - loss: 2.2784 - out_loss: 0.2210 - outM2_loss: 0.1124 - outT6_loss: 1.2031 - outT12_loss: 0.7418 - val_loss: 2.1558 - val_out_loss: 0.2099 - val_outM2_loss: 0.0693 - val_outT6_loss: 1.1365 - val_outT12_loss: 0.7400\n",
      "Epoch 37/500\n",
      " - 7s - loss: 2.2802 - out_loss: 0.2248 - outM2_loss: 0.1126 - outT6_loss: 1.2013 - outT12_loss: 0.7415 - val_loss: 2.1377 - val_out_loss: 0.1880 - val_outM2_loss: 0.0712 - val_outT6_loss: 1.1389 - val_outT12_loss: 0.7396\n",
      "Epoch 38/500\n",
      " - 6s - loss: 2.2766 - out_loss: 0.2248 - outM2_loss: 0.1133 - outT6_loss: 1.1975 - outT12_loss: 0.7410 - val_loss: 2.1213 - val_out_loss: 0.1786 - val_outM2_loss: 0.0704 - val_outT6_loss: 1.1327 - val_outT12_loss: 0.7396\n",
      "Epoch 39/500\n",
      " - 6s - loss: 2.2586 - out_loss: 0.2187 - outM2_loss: 0.1100 - outT6_loss: 1.1894 - outT12_loss: 0.7406 - val_loss: 2.1080 - val_out_loss: 0.1783 - val_outM2_loss: 0.0674 - val_outT6_loss: 1.1224 - val_outT12_loss: 0.7399\n",
      "Epoch 40/500\n",
      " - 6s - loss: 2.2527 - out_loss: 0.2218 - outM2_loss: 0.1074 - outT6_loss: 1.1833 - outT12_loss: 0.7402 - val_loss: 2.1474 - val_out_loss: 0.2004 - val_outM2_loss: 0.0758 - val_outT6_loss: 1.1322 - val_outT12_loss: 0.7390\n",
      "Epoch 41/500\n",
      " - 6s - loss: 2.2525 - out_loss: 0.2208 - outM2_loss: 0.1136 - outT6_loss: 1.1785 - outT12_loss: 0.7396 - val_loss: 2.1072 - val_out_loss: 0.1862 - val_outM2_loss: 0.0763 - val_outT6_loss: 1.1061 - val_outT12_loss: 0.7386\n",
      "Epoch 42/500\n",
      " - 6s - loss: 2.2420 - out_loss: 0.2194 - outM2_loss: 0.1090 - outT6_loss: 1.1744 - outT12_loss: 0.7391 - val_loss: 2.1125 - val_out_loss: 0.1930 - val_outM2_loss: 0.0744 - val_outT6_loss: 1.1072 - val_outT12_loss: 0.7379\n",
      "Epoch 43/500\n",
      " - 6s - loss: 2.2343 - out_loss: 0.2176 - outM2_loss: 0.1086 - outT6_loss: 1.1693 - outT12_loss: 0.7389 - val_loss: 2.0947 - val_out_loss: 0.1750 - val_outM2_loss: 0.0682 - val_outT6_loss: 1.1144 - val_outT12_loss: 0.7370\n",
      "Epoch 44/500\n",
      " - 6s - loss: 2.2306 - out_loss: 0.2184 - outM2_loss: 0.1086 - outT6_loss: 1.1655 - outT12_loss: 0.7381 - val_loss: 2.0764 - val_out_loss: 0.1776 - val_outM2_loss: 0.0625 - val_outT6_loss: 1.1001 - val_outT12_loss: 0.7362\n",
      "Epoch 45/500\n",
      " - 6s - loss: 2.2215 - out_loss: 0.2165 - outM2_loss: 0.1057 - outT6_loss: 1.1617 - outT12_loss: 0.7377 - val_loss: 2.0885 - val_out_loss: 0.1783 - val_outM2_loss: 0.0675 - val_outT6_loss: 1.1065 - val_outT12_loss: 0.7362\n",
      "Epoch 46/500\n",
      " - 6s - loss: 2.2178 - out_loss: 0.2169 - outM2_loss: 0.1071 - outT6_loss: 1.1567 - outT12_loss: 0.7372 - val_loss: 2.1246 - val_out_loss: 0.2128 - val_outM2_loss: 0.0847 - val_outT6_loss: 1.0912 - val_outT12_loss: 0.7359\n",
      "Epoch 47/500\n",
      " - 7s - loss: 2.2181 - out_loss: 0.2193 - outM2_loss: 0.1093 - outT6_loss: 1.1528 - outT12_loss: 0.7367 - val_loss: 2.0973 - val_out_loss: 0.1973 - val_outM2_loss: 0.0756 - val_outT6_loss: 1.0897 - val_outT12_loss: 0.7348\n",
      "Epoch 48/500\n",
      " - 6s - loss: 2.2087 - out_loss: 0.2165 - outM2_loss: 0.1079 - outT6_loss: 1.1485 - outT12_loss: 0.7359 - val_loss: 2.0704 - val_out_loss: 0.1877 - val_outM2_loss: 0.0602 - val_outT6_loss: 1.0875 - val_outT12_loss: 0.7350\n",
      "Epoch 49/500\n",
      " - 6s - loss: 2.2066 - out_loss: 0.2185 - outM2_loss: 0.1055 - outT6_loss: 1.1470 - outT12_loss: 0.7355 - val_loss: 2.0773 - val_out_loss: 0.1870 - val_outM2_loss: 0.0701 - val_outT6_loss: 1.0868 - val_outT12_loss: 0.7333\n",
      "Epoch 50/500\n",
      " - 6s - loss: 2.1961 - out_loss: 0.2133 - outM2_loss: 0.1069 - outT6_loss: 1.1408 - outT12_loss: 0.7351 - val_loss: 2.0919 - val_out_loss: 0.2110 - val_outM2_loss: 0.0632 - val_outT6_loss: 1.0845 - val_outT12_loss: 0.7332\n",
      "Epoch 51/500\n",
      " - 6s - loss: 2.1948 - out_loss: 0.2161 - outM2_loss: 0.1050 - outT6_loss: 1.1391 - outT12_loss: 0.7346 - val_loss: 2.0562 - val_out_loss: 0.1879 - val_outM2_loss: 0.0570 - val_outT6_loss: 1.0785 - val_outT12_loss: 0.7329\n",
      "Epoch 52/500\n",
      " - 6s - loss: 2.1946 - out_loss: 0.2163 - outM2_loss: 0.1086 - outT6_loss: 1.1357 - outT12_loss: 0.7340 - val_loss: 2.0726 - val_out_loss: 0.1924 - val_outM2_loss: 0.0706 - val_outT6_loss: 1.0782 - val_outT12_loss: 0.7314\n",
      "Epoch 53/500\n",
      " - 6s - loss: 2.1859 - out_loss: 0.2187 - outM2_loss: 0.1045 - outT6_loss: 1.1293 - outT12_loss: 0.7334 - val_loss: 2.0717 - val_out_loss: 0.1935 - val_outM2_loss: 0.0791 - val_outT6_loss: 1.0686 - val_outT12_loss: 0.7306\n",
      "Epoch 54/500\n",
      " - 6s - loss: 2.1829 - out_loss: 0.2178 - outM2_loss: 0.1064 - outT6_loss: 1.1259 - outT12_loss: 0.7328 - val_loss: 2.0678 - val_out_loss: 0.1771 - val_outM2_loss: 0.0914 - val_outT6_loss: 1.0685 - val_outT12_loss: 0.7309\n",
      "Epoch 55/500\n",
      " - 6s - loss: 2.1779 - out_loss: 0.2169 - outM2_loss: 0.1053 - outT6_loss: 1.1233 - outT12_loss: 0.7324 - val_loss: 2.0641 - val_out_loss: 0.1947 - val_outM2_loss: 0.0668 - val_outT6_loss: 1.0722 - val_outT12_loss: 0.7304\n",
      "Epoch 56/500\n",
      " - 6s - loss: 2.1733 - out_loss: 0.2123 - outM2_loss: 0.1073 - outT6_loss: 1.1217 - outT12_loss: 0.7320 - val_loss: 2.0243 - val_out_loss: 0.1708 - val_outM2_loss: 0.0619 - val_outT6_loss: 1.0610 - val_outT12_loss: 0.7305\n",
      "Epoch 57/500\n",
      " - 6s - loss: 2.1641 - out_loss: 0.2126 - outM2_loss: 0.1043 - outT6_loss: 1.1158 - outT12_loss: 0.7314 - val_loss: 2.0453 - val_out_loss: 0.1867 - val_outM2_loss: 0.0634 - val_outT6_loss: 1.0658 - val_outT12_loss: 0.7295\n",
      "Epoch 58/500\n",
      " - 6s - loss: 2.1596 - out_loss: 0.2114 - outM2_loss: 0.1050 - outT6_loss: 1.1125 - outT12_loss: 0.7306 - val_loss: 2.0386 - val_out_loss: 0.1805 - val_outM2_loss: 0.0657 - val_outT6_loss: 1.0634 - val_outT12_loss: 0.7290\n",
      "Epoch 59/500\n",
      " - 6s - loss: 2.1538 - out_loss: 0.2118 - outM2_loss: 0.1031 - outT6_loss: 1.1087 - outT12_loss: 0.7303 - val_loss: 2.0407 - val_out_loss: 0.1832 - val_outM2_loss: 0.0771 - val_outT6_loss: 1.0530 - val_outT12_loss: 0.7275\n",
      "Epoch 60/500\n",
      " - 6s - loss: 2.1505 - out_loss: 0.2102 - outM2_loss: 0.1042 - outT6_loss: 1.1068 - outT12_loss: 0.7294 - val_loss: 2.0215 - val_out_loss: 0.1759 - val_outM2_loss: 0.0732 - val_outT6_loss: 1.0452 - val_outT12_loss: 0.7272\n",
      "Epoch 61/500\n",
      " - 6s - loss: 2.1471 - out_loss: 0.2111 - outM2_loss: 0.1047 - outT6_loss: 1.1021 - outT12_loss: 0.7292 - val_loss: 2.0340 - val_out_loss: 0.1900 - val_outM2_loss: 0.0666 - val_outT6_loss: 1.0511 - val_outT12_loss: 0.7263\n",
      "Epoch 62/500\n",
      " - 6s - loss: 2.1426 - out_loss: 0.2083 - outM2_loss: 0.1053 - outT6_loss: 1.1006 - outT12_loss: 0.7285 - val_loss: 1.9993 - val_out_loss: 0.1748 - val_outM2_loss: 0.0607 - val_outT6_loss: 1.0376 - val_outT12_loss: 0.7263\n",
      "Epoch 63/500\n",
      " - 6s - loss: 2.1455 - out_loss: 0.2122 - outM2_loss: 0.1049 - outT6_loss: 1.1002 - outT12_loss: 0.7282 - val_loss: 2.0463 - val_out_loss: 0.1901 - val_outM2_loss: 0.0793 - val_outT6_loss: 1.0512 - val_outT12_loss: 0.7257\n",
      "Epoch 64/500\n",
      " - 6s - loss: 2.1345 - out_loss: 0.2082 - outM2_loss: 0.1023 - outT6_loss: 1.0967 - outT12_loss: 0.7273 - val_loss: 1.9977 - val_out_loss: 0.1738 - val_outM2_loss: 0.0640 - val_outT6_loss: 1.0353 - val_outT12_loss: 0.7246\n",
      "Epoch 65/500\n",
      " - 6s - loss: 2.1346 - out_loss: 0.2113 - outM2_loss: 0.1032 - outT6_loss: 1.0929 - outT12_loss: 0.7272 - val_loss: 2.0106 - val_out_loss: 0.1778 - val_outM2_loss: 0.0630 - val_outT6_loss: 1.0450 - val_outT12_loss: 0.7248\n",
      "Epoch 66/500\n",
      " - 6s - loss: 2.1221 - out_loss: 0.2062 - outM2_loss: 0.1007 - outT6_loss: 1.0888 - outT12_loss: 0.7264 - val_loss: 2.0173 - val_out_loss: 0.1866 - val_outM2_loss: 0.0636 - val_outT6_loss: 1.0427 - val_outT12_loss: 0.7244\n",
      "Epoch 67/500\n",
      " - 6s - loss: 2.1294 - out_loss: 0.2104 - outM2_loss: 0.1036 - outT6_loss: 1.0895 - outT12_loss: 0.7259 - val_loss: 1.9994 - val_out_loss: 0.1791 - val_outM2_loss: 0.0624 - val_outT6_loss: 1.0344 - val_outT12_loss: 0.7235\n",
      "Epoch 68/500\n",
      " - 6s - loss: 2.1224 - out_loss: 0.2088 - outM2_loss: 0.1039 - outT6_loss: 1.0844 - outT12_loss: 0.7253 - val_loss: 2.0062 - val_out_loss: 0.1735 - val_outM2_loss: 0.0800 - val_outT6_loss: 1.0297 - val_outT12_loss: 0.7229\n",
      "Epoch 69/500\n",
      " - 6s - loss: 2.1161 - out_loss: 0.2064 - outM2_loss: 0.1030 - outT6_loss: 1.0822 - outT12_loss: 0.7246 - val_loss: 2.0049 - val_out_loss: 0.1840 - val_outM2_loss: 0.0761 - val_outT6_loss: 1.0220 - val_outT12_loss: 0.7228\n",
      "Epoch 70/500\n",
      " - 6s - loss: 2.1162 - out_loss: 0.2066 - outM2_loss: 0.1057 - outT6_loss: 1.0797 - outT12_loss: 0.7242 - val_loss: 1.9822 - val_out_loss: 0.1740 - val_outM2_loss: 0.0635 - val_outT6_loss: 1.0230 - val_outT12_loss: 0.7218\n",
      "Epoch 71/500\n",
      " - 6s - loss: 2.1155 - out_loss: 0.2087 - outM2_loss: 0.1057 - outT6_loss: 1.0777 - outT12_loss: 0.7235 - val_loss: 1.9769 - val_out_loss: 0.1750 - val_outM2_loss: 0.0532 - val_outT6_loss: 1.0268 - val_outT12_loss: 0.7220\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0003333000158309005.\n",
      "Epoch 72/500\n",
      " - 6s - loss: 2.0523 - out_loss: 0.1958 - outM2_loss: 0.0941 - outT6_loss: 1.0417 - outT12_loss: 0.7206 - val_loss: 1.9080 - val_out_loss: 0.1548 - val_outM2_loss: 0.0494 - val_outT6_loss: 0.9853 - val_outT12_loss: 0.7184\n",
      "Epoch 73/500\n",
      " - 6s - loss: 2.0379 - out_loss: 0.1942 - outM2_loss: 0.0932 - outT6_loss: 1.0310 - outT12_loss: 0.7195 - val_loss: 1.9039 - val_out_loss: 0.1581 - val_outM2_loss: 0.0443 - val_outT6_loss: 0.9833 - val_outT12_loss: 0.7181\n",
      "Epoch 74/500\n",
      " - 6s - loss: 2.0344 - out_loss: 0.1947 - outM2_loss: 0.0937 - outT6_loss: 1.0269 - outT12_loss: 0.7190 - val_loss: 1.9106 - val_out_loss: 0.1580 - val_outM2_loss: 0.0510 - val_outT6_loss: 0.9839 - val_outT12_loss: 0.7176\n",
      "Epoch 75/500\n",
      " - 6s - loss: 2.0311 - out_loss: 0.1939 - outM2_loss: 0.0928 - outT6_loss: 1.0256 - outT12_loss: 0.7188 - val_loss: 1.9029 - val_out_loss: 0.1554 - val_outM2_loss: 0.0464 - val_outT6_loss: 0.9839 - val_outT12_loss: 0.7172\n",
      "Epoch 76/500\n",
      " - 6s - loss: 2.0259 - out_loss: 0.1919 - outM2_loss: 0.0933 - outT6_loss: 1.0224 - outT12_loss: 0.7182 - val_loss: 1.8957 - val_out_loss: 0.1534 - val_outM2_loss: 0.0475 - val_outT6_loss: 0.9782 - val_outT12_loss: 0.7167\n",
      "Epoch 77/500\n",
      " - 6s - loss: 2.0264 - out_loss: 0.1935 - outM2_loss: 0.0945 - outT6_loss: 1.0203 - outT12_loss: 0.7180 - val_loss: 1.9001 - val_out_loss: 0.1575 - val_outM2_loss: 0.0488 - val_outT6_loss: 0.9771 - val_outT12_loss: 0.7166\n",
      "Epoch 78/500\n",
      " - 6s - loss: 2.0245 - out_loss: 0.1938 - outM2_loss: 0.0937 - outT6_loss: 1.0192 - outT12_loss: 0.7178 - val_loss: 1.9044 - val_out_loss: 0.1543 - val_outM2_loss: 0.0581 - val_outT6_loss: 0.9758 - val_outT12_loss: 0.7162\n",
      "Epoch 79/500\n",
      " - 6s - loss: 2.0205 - out_loss: 0.1918 - outM2_loss: 0.0932 - outT6_loss: 1.0181 - outT12_loss: 0.7175 - val_loss: 1.8985 - val_out_loss: 0.1564 - val_outM2_loss: 0.0525 - val_outT6_loss: 0.9736 - val_outT12_loss: 0.7160\n",
      "Epoch 80/500\n",
      " - 6s - loss: 2.0212 - out_loss: 0.1930 - outM2_loss: 0.0941 - outT6_loss: 1.0169 - outT12_loss: 0.7172 - val_loss: 1.9005 - val_out_loss: 0.1572 - val_outM2_loss: 0.0529 - val_outT6_loss: 0.9748 - val_outT12_loss: 0.7157\n",
      "Epoch 81/500\n",
      " - 7s - loss: 2.0196 - out_loss: 0.1916 - outM2_loss: 0.0939 - outT6_loss: 1.0173 - outT12_loss: 0.7167 - val_loss: 1.8953 - val_out_loss: 0.1544 - val_outM2_loss: 0.0504 - val_outT6_loss: 0.9754 - val_outT12_loss: 0.7152\n",
      "Epoch 82/500\n",
      " - 6s - loss: 2.0169 - out_loss: 0.1922 - outM2_loss: 0.0923 - outT6_loss: 1.0159 - outT12_loss: 0.7164 - val_loss: 1.8905 - val_out_loss: 0.1555 - val_outM2_loss: 0.0481 - val_outT6_loss: 0.9719 - val_outT12_loss: 0.7150\n",
      "Epoch 83/500\n",
      " - 6s - loss: 2.0129 - out_loss: 0.1907 - outM2_loss: 0.0928 - outT6_loss: 1.0131 - outT12_loss: 0.7162 - val_loss: 1.8919 - val_out_loss: 0.1582 - val_outM2_loss: 0.0462 - val_outT6_loss: 0.9727 - val_outT12_loss: 0.7146\n",
      "Epoch 84/500\n",
      " - 6s - loss: 2.0143 - out_loss: 0.1924 - outM2_loss: 0.0927 - outT6_loss: 1.0129 - outT12_loss: 0.7163 - val_loss: 1.8848 - val_out_loss: 0.1529 - val_outM2_loss: 0.0466 - val_outT6_loss: 0.9705 - val_outT12_loss: 0.7148\n",
      "Epoch 85/500\n",
      " - 6s - loss: 2.0110 - out_loss: 0.1905 - outM2_loss: 0.0927 - outT6_loss: 1.0120 - outT12_loss: 0.7158 - val_loss: 1.8896 - val_out_loss: 0.1568 - val_outM2_loss: 0.0498 - val_outT6_loss: 0.9685 - val_outT12_loss: 0.7144\n",
      "Epoch 86/500\n",
      " - 6s - loss: 2.0140 - out_loss: 0.1946 - outM2_loss: 0.0928 - outT6_loss: 1.0111 - outT12_loss: 0.7155 - val_loss: 1.8832 - val_out_loss: 0.1546 - val_outM2_loss: 0.0470 - val_outT6_loss: 0.9677 - val_outT12_loss: 0.7139\n",
      "Epoch 87/500\n",
      " - 6s - loss: 2.0076 - out_loss: 0.1916 - outM2_loss: 0.0915 - outT6_loss: 1.0093 - outT12_loss: 0.7152 - val_loss: 1.8807 - val_out_loss: 0.1543 - val_outM2_loss: 0.0455 - val_outT6_loss: 0.9674 - val_outT12_loss: 0.7136\n",
      "Epoch 88/500\n",
      " - 6s - loss: 2.0060 - out_loss: 0.1911 - outM2_loss: 0.0916 - outT6_loss: 1.0083 - outT12_loss: 0.7150 - val_loss: 1.8801 - val_out_loss: 0.1534 - val_outM2_loss: 0.0450 - val_outT6_loss: 0.9681 - val_outT12_loss: 0.7135\n",
      "Epoch 89/500\n",
      " - 6s - loss: 2.0053 - out_loss: 0.1926 - outM2_loss: 0.0914 - outT6_loss: 1.0064 - outT12_loss: 0.7150 - val_loss: 1.8861 - val_out_loss: 0.1548 - val_outM2_loss: 0.0503 - val_outT6_loss: 0.9676 - val_outT12_loss: 0.7134\n",
      "Epoch 90/500\n",
      " - 6s - loss: 2.0050 - out_loss: 0.1911 - outM2_loss: 0.0928 - outT6_loss: 1.0067 - outT12_loss: 0.7145 - val_loss: 1.8884 - val_out_loss: 0.1537 - val_outM2_loss: 0.0568 - val_outT6_loss: 0.9648 - val_outT12_loss: 0.7131\n",
      "Epoch 91/500\n",
      " - 6s - loss: 2.0003 - out_loss: 0.1890 - outM2_loss: 0.0927 - outT6_loss: 1.0043 - outT12_loss: 0.7142 - val_loss: 1.8753 - val_out_loss: 0.1520 - val_outM2_loss: 0.0498 - val_outT6_loss: 0.9607 - val_outT12_loss: 0.7127\n",
      "Epoch 92/500\n",
      " - 6s - loss: 1.9985 - out_loss: 0.1897 - outM2_loss: 0.0916 - outT6_loss: 1.0034 - outT12_loss: 0.7138 - val_loss: 1.8755 - val_out_loss: 0.1548 - val_outM2_loss: 0.0456 - val_outT6_loss: 0.9625 - val_outT12_loss: 0.7126\n",
      "Epoch 93/500\n",
      " - 6s - loss: 1.9991 - out_loss: 0.1889 - outM2_loss: 0.0929 - outT6_loss: 1.0034 - outT12_loss: 0.7139 - val_loss: 1.8753 - val_out_loss: 0.1544 - val_outM2_loss: 0.0467 - val_outT6_loss: 0.9618 - val_outT12_loss: 0.7124\n",
      "Epoch 94/500\n",
      " - 6s - loss: 1.9974 - out_loss: 0.1888 - outM2_loss: 0.0920 - outT6_loss: 1.0029 - outT12_loss: 0.7137 - val_loss: 1.8718 - val_out_loss: 0.1559 - val_outM2_loss: 0.0436 - val_outT6_loss: 0.9603 - val_outT12_loss: 0.7119\n",
      "Epoch 95/500\n",
      " - 6s - loss: 1.9973 - out_loss: 0.1900 - outM2_loss: 0.0918 - outT6_loss: 1.0023 - outT12_loss: 0.7132 - val_loss: 1.8756 - val_out_loss: 0.1550 - val_outM2_loss: 0.0504 - val_outT6_loss: 0.9586 - val_outT12_loss: 0.7117\n",
      "Epoch 96/500\n",
      " - 7s - loss: 1.9946 - out_loss: 0.1888 - outM2_loss: 0.0935 - outT6_loss: 0.9996 - outT12_loss: 0.7127 - val_loss: 1.8772 - val_out_loss: 0.1598 - val_outM2_loss: 0.0477 - val_outT6_loss: 0.9583 - val_outT12_loss: 0.7115\n",
      "Epoch 97/500\n",
      " - 6s - loss: 1.9936 - out_loss: 0.1894 - outM2_loss: 0.0918 - outT6_loss: 0.9998 - outT12_loss: 0.7125 - val_loss: 1.8725 - val_out_loss: 0.1568 - val_outM2_loss: 0.0468 - val_outT6_loss: 0.9576 - val_outT12_loss: 0.7113\n",
      "Epoch 98/500\n",
      " - 6s - loss: 1.9945 - out_loss: 0.1910 - outM2_loss: 0.0919 - outT6_loss: 0.9989 - outT12_loss: 0.7127 - val_loss: 1.8694 - val_out_loss: 0.1540 - val_outM2_loss: 0.0488 - val_outT6_loss: 0.9555 - val_outT12_loss: 0.7110\n",
      "Epoch 99/500\n",
      " - 6s - loss: 1.9902 - out_loss: 0.1887 - outM2_loss: 0.0917 - outT6_loss: 0.9976 - outT12_loss: 0.7122 - val_loss: 1.8788 - val_out_loss: 0.1570 - val_outM2_loss: 0.0551 - val_outT6_loss: 0.9558 - val_outT12_loss: 0.7108\n",
      "Epoch 100/500\n",
      " - 6s - loss: 1.9903 - out_loss: 0.1899 - outM2_loss: 0.0923 - outT6_loss: 0.9962 - outT12_loss: 0.7120 - val_loss: 1.8734 - val_out_loss: 0.1566 - val_outM2_loss: 0.0470 - val_outT6_loss: 0.9594 - val_outT12_loss: 0.7104\n",
      "Epoch 101/500\n",
      " - 6s - loss: 1.9915 - out_loss: 0.1893 - outM2_loss: 0.0936 - outT6_loss: 0.9969 - outT12_loss: 0.7117 - val_loss: 1.8687 - val_out_loss: 0.1556 - val_outM2_loss: 0.0470 - val_outT6_loss: 0.9558 - val_outT12_loss: 0.7103\n",
      "Epoch 102/500\n",
      " - 6s - loss: 1.9854 - out_loss: 0.1890 - outM2_loss: 0.0909 - outT6_loss: 0.9939 - outT12_loss: 0.7115 - val_loss: 1.8717 - val_out_loss: 0.1540 - val_outM2_loss: 0.0524 - val_outT6_loss: 0.9552 - val_outT12_loss: 0.7102\n",
      "Epoch 103/500\n",
      " - 6s - loss: 1.9828 - out_loss: 0.1877 - outM2_loss: 0.0908 - outT6_loss: 0.9931 - outT12_loss: 0.7112 - val_loss: 1.8711 - val_out_loss: 0.1544 - val_outM2_loss: 0.0516 - val_outT6_loss: 0.9552 - val_outT12_loss: 0.7098\n",
      "Epoch 104/500\n",
      " - 6s - loss: 1.9871 - out_loss: 0.1904 - outM2_loss: 0.0913 - outT6_loss: 0.9946 - outT12_loss: 0.7109 - val_loss: 1.8681 - val_out_loss: 0.1551 - val_outM2_loss: 0.0497 - val_outT6_loss: 0.9535 - val_outT12_loss: 0.7097\n",
      "Epoch 105/500\n",
      " - 6s - loss: 1.9820 - out_loss: 0.1882 - outM2_loss: 0.0907 - outT6_loss: 0.9927 - outT12_loss: 0.7104 - val_loss: 1.8659 - val_out_loss: 0.1543 - val_outM2_loss: 0.0480 - val_outT6_loss: 0.9539 - val_outT12_loss: 0.7097\n",
      "Epoch 106/500\n",
      " - 6s - loss: 1.9812 - out_loss: 0.1883 - outM2_loss: 0.0906 - outT6_loss: 0.9919 - outT12_loss: 0.7105 - val_loss: 1.8629 - val_out_loss: 0.1566 - val_outM2_loss: 0.0440 - val_outT6_loss: 0.9526 - val_outT12_loss: 0.7097\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.00011108889195893425.\n",
      "Epoch 107/500\n",
      " - 6s - loss: 1.9599 - out_loss: 0.1836 - outM2_loss: 0.0889 - outT6_loss: 0.9784 - outT12_loss: 0.7090 - val_loss: 1.8380 - val_out_loss: 0.1479 - val_outM2_loss: 0.0418 - val_outT6_loss: 0.9403 - val_outT12_loss: 0.7081\n",
      "Epoch 108/500\n",
      " - 6s - loss: 1.9541 - out_loss: 0.1832 - outM2_loss: 0.0880 - outT6_loss: 0.9744 - outT12_loss: 0.7087 - val_loss: 1.8365 - val_out_loss: 0.1481 - val_outM2_loss: 0.0413 - val_outT6_loss: 0.9392 - val_outT12_loss: 0.7079\n",
      "Epoch 109/500\n",
      " - 6s - loss: 1.9531 - out_loss: 0.1827 - outM2_loss: 0.0886 - outT6_loss: 0.9734 - outT12_loss: 0.7084 - val_loss: 1.8364 - val_out_loss: 0.1483 - val_outM2_loss: 0.0426 - val_outT6_loss: 0.9378 - val_outT12_loss: 0.7077\n",
      "Epoch 110/500\n",
      " - 6s - loss: 1.9536 - out_loss: 0.1846 - outM2_loss: 0.0885 - outT6_loss: 0.9722 - outT12_loss: 0.7083 - val_loss: 1.8378 - val_out_loss: 0.1476 - val_outM2_loss: 0.0433 - val_outT6_loss: 0.9391 - val_outT12_loss: 0.7078\n",
      "Epoch 111/500\n",
      " - 6s - loss: 1.9492 - out_loss: 0.1827 - outM2_loss: 0.0884 - outT6_loss: 0.9703 - outT12_loss: 0.7079 - val_loss: 1.8347 - val_out_loss: 0.1481 - val_outM2_loss: 0.0421 - val_outT6_loss: 0.9371 - val_outT12_loss: 0.7075\n",
      "Epoch 112/500\n",
      " - 6s - loss: 1.9498 - out_loss: 0.1826 - outM2_loss: 0.0882 - outT6_loss: 0.9708 - outT12_loss: 0.7082 - val_loss: 1.8372 - val_out_loss: 0.1498 - val_outM2_loss: 0.0442 - val_outT6_loss: 0.9358 - val_outT12_loss: 0.7073\n",
      "Epoch 113/500\n",
      " - 6s - loss: 1.9486 - out_loss: 0.1827 - outM2_loss: 0.0878 - outT6_loss: 0.9703 - outT12_loss: 0.7078 - val_loss: 1.8343 - val_out_loss: 0.1476 - val_outM2_loss: 0.0433 - val_outT6_loss: 0.9361 - val_outT12_loss: 0.7073\n",
      "Epoch 114/500\n",
      " - 7s - loss: 1.9472 - out_loss: 0.1823 - outM2_loss: 0.0875 - outT6_loss: 0.9697 - outT12_loss: 0.7077 - val_loss: 1.8348 - val_out_loss: 0.1488 - val_outM2_loss: 0.0431 - val_outT6_loss: 0.9358 - val_outT12_loss: 0.7071\n",
      "Epoch 115/500\n",
      " - 6s - loss: 1.9471 - out_loss: 0.1825 - outM2_loss: 0.0879 - outT6_loss: 0.9688 - outT12_loss: 0.7079 - val_loss: 1.8310 - val_out_loss: 0.1479 - val_outM2_loss: 0.0412 - val_outT6_loss: 0.9349 - val_outT12_loss: 0.7070\n",
      "Epoch 116/500\n",
      " - 6s - loss: 1.9460 - out_loss: 0.1825 - outM2_loss: 0.0875 - outT6_loss: 0.9687 - outT12_loss: 0.7074 - val_loss: 1.8288 - val_out_loss: 0.1472 - val_outM2_loss: 0.0404 - val_outT6_loss: 0.9343 - val_outT12_loss: 0.7069\n",
      "Epoch 117/500\n",
      " - 6s - loss: 1.9459 - out_loss: 0.1822 - outM2_loss: 0.0879 - outT6_loss: 0.9683 - outT12_loss: 0.7075 - val_loss: 1.8341 - val_out_loss: 0.1492 - val_outM2_loss: 0.0432 - val_outT6_loss: 0.9348 - val_outT12_loss: 0.7068\n",
      "Epoch 118/500\n",
      " - 6s - loss: 1.9448 - out_loss: 0.1820 - outM2_loss: 0.0885 - outT6_loss: 0.9668 - outT12_loss: 0.7074 - val_loss: 1.8289 - val_out_loss: 0.1476 - val_outM2_loss: 0.0411 - val_outT6_loss: 0.9334 - val_outT12_loss: 0.7067\n",
      "Epoch 119/500\n",
      " - 6s - loss: 1.9469 - out_loss: 0.1830 - outM2_loss: 0.0888 - outT6_loss: 0.9680 - outT12_loss: 0.7072 - val_loss: 1.8318 - val_out_loss: 0.1483 - val_outM2_loss: 0.0432 - val_outT6_loss: 0.9337 - val_outT12_loss: 0.7066\n",
      "Epoch 120/500\n",
      " - 6s - loss: 1.9452 - out_loss: 0.1829 - outM2_loss: 0.0879 - outT6_loss: 0.9674 - outT12_loss: 0.7070 - val_loss: 1.8309 - val_out_loss: 0.1510 - val_outM2_loss: 0.0395 - val_outT6_loss: 0.9340 - val_outT12_loss: 0.7065\n",
      "Epoch 121/500\n",
      " - 6s - loss: 1.9424 - out_loss: 0.1811 - outM2_loss: 0.0878 - outT6_loss: 0.9663 - outT12_loss: 0.7072 - val_loss: 1.8283 - val_out_loss: 0.1500 - val_outM2_loss: 0.0396 - val_outT6_loss: 0.9323 - val_outT12_loss: 0.7064\n",
      "Epoch 122/500\n",
      " - 6s - loss: 1.9412 - out_loss: 0.1812 - outM2_loss: 0.0878 - outT6_loss: 0.9654 - outT12_loss: 0.7068 - val_loss: 1.8281 - val_out_loss: 0.1473 - val_outM2_loss: 0.0410 - val_outT6_loss: 0.9335 - val_outT12_loss: 0.7063\n",
      "Epoch 123/500\n",
      " - 6s - loss: 1.9441 - out_loss: 0.1824 - outM2_loss: 0.0887 - outT6_loss: 0.9661 - outT12_loss: 0.7069 - val_loss: 1.8283 - val_out_loss: 0.1482 - val_outM2_loss: 0.0420 - val_outT6_loss: 0.9318 - val_outT12_loss: 0.7062\n",
      "Epoch 124/500\n",
      " - 6s - loss: 1.9416 - out_loss: 0.1815 - outM2_loss: 0.0875 - outT6_loss: 0.9657 - outT12_loss: 0.7069 - val_loss: 1.8279 - val_out_loss: 0.1473 - val_outM2_loss: 0.0411 - val_outT6_loss: 0.9333 - val_outT12_loss: 0.7062\n",
      "Epoch 125/500\n",
      " - 6s - loss: 1.9385 - out_loss: 0.1809 - outM2_loss: 0.0878 - outT6_loss: 0.9633 - outT12_loss: 0.7065 - val_loss: 1.8328 - val_out_loss: 0.1464 - val_outM2_loss: 0.0477 - val_outT6_loss: 0.9326 - val_outT12_loss: 0.7061\n",
      "Epoch 126/500\n",
      " - 6s - loss: 1.9399 - out_loss: 0.1811 - outM2_loss: 0.0878 - outT6_loss: 0.9643 - outT12_loss: 0.7067 - val_loss: 1.8279 - val_out_loss: 0.1501 - val_outM2_loss: 0.0412 - val_outT6_loss: 0.9306 - val_outT12_loss: 0.7059\n",
      "Epoch 127/500\n",
      " - 6s - loss: 1.9412 - out_loss: 0.1817 - outM2_loss: 0.0881 - outT6_loss: 0.9650 - outT12_loss: 0.7063 - val_loss: 1.8272 - val_out_loss: 0.1499 - val_outM2_loss: 0.0407 - val_outT6_loss: 0.9307 - val_outT12_loss: 0.7059\n",
      "Epoch 128/500\n",
      " - 6s - loss: 1.9388 - out_loss: 0.1811 - outM2_loss: 0.0876 - outT6_loss: 0.9636 - outT12_loss: 0.7065 - val_loss: 1.8286 - val_out_loss: 0.1489 - val_outM2_loss: 0.0426 - val_outT6_loss: 0.9313 - val_outT12_loss: 0.7058\n",
      "Epoch 129/500\n",
      " - 6s - loss: 1.9374 - out_loss: 0.1807 - outM2_loss: 0.0875 - outT6_loss: 0.9629 - outT12_loss: 0.7063 - val_loss: 1.8245 - val_out_loss: 0.1494 - val_outM2_loss: 0.0389 - val_outT6_loss: 0.9303 - val_outT12_loss: 0.7058\n",
      "Epoch 130/500\n",
      " - 6s - loss: 1.9377 - out_loss: 0.1809 - outM2_loss: 0.0869 - outT6_loss: 0.9638 - outT12_loss: 0.7061 - val_loss: 1.8264 - val_out_loss: 0.1478 - val_outM2_loss: 0.0424 - val_outT6_loss: 0.9306 - val_outT12_loss: 0.7056\n",
      "Epoch 131/500\n",
      " - 6s - loss: 1.9378 - out_loss: 0.1810 - outM2_loss: 0.0883 - outT6_loss: 0.9626 - outT12_loss: 0.7059 - val_loss: 1.8228 - val_out_loss: 0.1471 - val_outM2_loss: 0.0409 - val_outT6_loss: 0.9292 - val_outT12_loss: 0.7056\n",
      "Epoch 132/500\n",
      " - 6s - loss: 1.9365 - out_loss: 0.1809 - outM2_loss: 0.0876 - outT6_loss: 0.9621 - outT12_loss: 0.7060 - val_loss: 1.8261 - val_out_loss: 0.1471 - val_outM2_loss: 0.0442 - val_outT6_loss: 0.9294 - val_outT12_loss: 0.7054\n",
      "Epoch 133/500\n",
      " - 6s - loss: 1.9363 - out_loss: 0.1814 - outM2_loss: 0.0874 - outT6_loss: 0.9616 - outT12_loss: 0.7059 - val_loss: 1.8305 - val_out_loss: 0.1480 - val_outM2_loss: 0.0475 - val_outT6_loss: 0.9295 - val_outT12_loss: 0.7055\n",
      "Epoch 134/500\n",
      " - 6s - loss: 1.9344 - out_loss: 0.1800 - outM2_loss: 0.0870 - outT6_loss: 0.9615 - outT12_loss: 0.7058 - val_loss: 1.8259 - val_out_loss: 0.1475 - val_outM2_loss: 0.0441 - val_outT6_loss: 0.9289 - val_outT12_loss: 0.7053\n",
      "Epoch 135/500\n",
      " - 6s - loss: 1.9343 - out_loss: 0.1806 - outM2_loss: 0.0879 - outT6_loss: 0.9601 - outT12_loss: 0.7057 - val_loss: 1.8257 - val_out_loss: 0.1475 - val_outM2_loss: 0.0429 - val_outT6_loss: 0.9299 - val_outT12_loss: 0.7053\n",
      "Epoch 136/500\n",
      " - 6s - loss: 1.9356 - out_loss: 0.1819 - outM2_loss: 0.0873 - outT6_loss: 0.9608 - outT12_loss: 0.7056 - val_loss: 1.8232 - val_out_loss: 0.1467 - val_outM2_loss: 0.0423 - val_outT6_loss: 0.9290 - val_outT12_loss: 0.7052\n",
      "Epoch 137/500\n",
      " - 6s - loss: 1.9348 - out_loss: 0.1826 - outM2_loss: 0.0877 - outT6_loss: 0.9589 - outT12_loss: 0.7056 - val_loss: 1.8185 - val_out_loss: 0.1470 - val_outM2_loss: 0.0385 - val_outT6_loss: 0.9279 - val_outT12_loss: 0.7051\n",
      "Epoch 138/500\n",
      " - 7s - loss: 1.9338 - out_loss: 0.1806 - outM2_loss: 0.0872 - outT6_loss: 0.9605 - outT12_loss: 0.7055 - val_loss: 1.8241 - val_out_loss: 0.1467 - val_outM2_loss: 0.0446 - val_outT6_loss: 0.9278 - val_outT12_loss: 0.7050\n",
      "Epoch 139/500\n",
      " - 6s - loss: 1.9315 - out_loss: 0.1803 - outM2_loss: 0.0874 - outT6_loss: 0.9586 - outT12_loss: 0.7052 - val_loss: 1.8194 - val_out_loss: 0.1463 - val_outM2_loss: 0.0403 - val_outT6_loss: 0.9279 - val_outT12_loss: 0.7049\n",
      "Epoch 140/500\n"
     ]
    }
   ],
   "source": [
    "EPOCH_N = 500\n",
    "INPUT_MODEL = 'M053_0725_0821_3JHH'\n",
    "\n",
    "cv_score=[]\n",
    "cv_score_total=0 \n",
    "verbose = 2\n",
    "\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "oof = train.loc[train['type'] == bond_type].reset_index(drop=True).drop(['molecule_name','atom_index_0','atom_index_1'], axis=1)\n",
    "sub = test.loc[test['type'] == bond_type]\n",
    "\n",
    "for fold in [1, 2, 3]:\n",
    "    # Find the correct input filename for this fold\n",
    "    for f in os.listdir(f'../type_results/{bond_type}/meta'):\n",
    "        if ('X_train' in f) and (INPUT_MODEL in f) and (f'_f{fold}_' in f):\n",
    "            X_train_file = f\n",
    "    logger.info(f'Using X_train file {X_train_file}')\n",
    "        \n",
    "    cv_predict, test_predict, val_idx = train_keras_model(fold=fold,\n",
    "                                                 MODEL_NUMBER=MODEL_NUMBER,\n",
    "                                                 bond_type=bond_type,\n",
    "                                                 X_train_file=X_train_file,\n",
    "                                                 EPOCH_N=EPOCH_N)\n",
    "    oof.loc[oof.index.isin(val_idx), 'oof_preds'] = cv_predict[0][:,0]\n",
    "    sub[f'scalar_coupling_constant_f{fold}'] = test_predict[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"{:%m%d_%H%M}\".format(datetime.now())\n",
    "logger.info('Plotting Results')\n",
    "oof.plot(x='scalar_coupling_constant', y='oof_preds', kind='scatter', figsize=(5, 5), title='OOF Preds vs Actual')\n",
    "plt.show()\n",
    "\n",
    "df = pd.read_csv('../submissions/BLEND035_sub_-2.00491CV.csv') # Pull a good submission\n",
    "sub['scc'] = df['scalar_coupling_constant']\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "sub.plot(x='scc', y='scalar_coupling_constant_f1', kind='scatter', ax=ax1)\n",
    "sub.plot(x='scc', y='scalar_coupling_constant_f2', kind='scatter', ax=ax2)\n",
    "sub.plot(x='scc', y='scalar_coupling_constant_f3', kind='scatter', ax=ax3)\n",
    "plt.show()\n",
    "\n",
    "sub['scalar_coupling_constant'] = sub[['scalar_coupling_constant_f1',\n",
    "                                       'scalar_coupling_constant_f2',\n",
    "                                       'scalar_coupling_constant_f3']].mean(axis=1)\n",
    "logger.info('Saving Results')\n",
    "save_type_data(\n",
    "    type_=bond_type,\n",
    "    oof=oof,\n",
    "    sub=sub.drop('type', axis=1),\n",
    "    fi=None,\n",
    "    MODEL_NUMBER=MODEL_NUMBER,\n",
    "    run_id=run_id,\n",
    "    MODEL_TYPE='keras',\n",
    "    N_FOLDS=3,\n",
    "    N_ESTIMATORS='',\n",
    "    LEARNING_RATE='',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
