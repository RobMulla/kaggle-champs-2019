{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n",
    "import os\n",
    "import gc\n",
    "#%cd /kaggle/input/champs-scalar-coupling\n",
    "#print(os.listdir(\".\"))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train =pd.read_csv('../input/train.csv')\n",
    "df_test  =pd.read_csv('../input/test.csv')\n",
    "df_struct=pd.read_csv('../input/structures.csv')\n",
    "\n",
    "df_train_sub_charge=pd.read_csv('../input/mulliken_charges.csv')\n",
    "df_train_sub_tensor=pd.read_csv('../input/magnetic_shielding_tensors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM usage: 0.9843254089355469 GB\n",
      "Mapping... (4658147, 6) (2358657, 6) 0\n",
      "Mapping... (4658147, 10) (1533537, 3) 0\n",
      "Mapping... (4658147, 11) (1533537, 11) 0\n",
      "Mapping... (2505542, 5) (2358657, 6) 0\n",
      "RAM usage: 1.7024726867675781 GB\n",
      "(4658147, 20) (2505542, 9)\n",
      "Mapping... (4658147, 20) (2358657, 10) 1\n",
      "Mapping... (4658147, 28) (1533537, 3) 1\n",
      "Mapping... (4658147, 29) (1533537, 11) 1\n",
      "Mapping... (2505542, 9) (2358657, 10) 1\n",
      "RAM usage: 2.4983367919921875 GB\n",
      "(4658147, 38) (2505542, 17)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Map atom info from the structures.csv into the train/test files\n",
    "'''\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def map_atom_info(df_1,df_2, atom_idx):\n",
    "    print('Mapping...', df_1.shape, df_2.shape, atom_idx)\n",
    "    df = pd.merge(df_1, df_2.drop_duplicates(subset=['molecule_name', 'atom_index']), how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    return df\n",
    "\n",
    "def show_ram_usage():\n",
    "    py = psutil.Process(os.getpid())\n",
    "    print('RAM usage: {} GB'.format(py.memory_info()[0]/2. ** 30))\n",
    "\n",
    "show_ram_usage()\n",
    "\n",
    "for atom_idx in [0,1]:\n",
    "    df_train = map_atom_info(df_train,df_struct, atom_idx)\n",
    "    df_train = map_atom_info(df_train,df_train_sub_charge, atom_idx)\n",
    "    df_train = map_atom_info(df_train,df_train_sub_tensor, atom_idx)\n",
    "    df_train = df_train.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                                        'x': f'x_{atom_idx}',\n",
    "                                        'y': f'y_{atom_idx}',\n",
    "                                        'z': f'z_{atom_idx}',\n",
    "                                        'mulliken_charge': f'charge_{atom_idx}',\n",
    "                                        'XX': f'XX_{atom_idx}',\n",
    "                                        'YX': f'YX_{atom_idx}',\n",
    "                                        'ZX': f'ZX_{atom_idx}',\n",
    "                                        'XY': f'XY_{atom_idx}',\n",
    "                                        'YY': f'YY_{atom_idx}',\n",
    "                                        'ZY': f'ZY_{atom_idx}',\n",
    "                                        'XZ': f'XZ_{atom_idx}',\n",
    "                                        'YZ': f'YZ_{atom_idx}',\n",
    "                                        'ZZ': f'ZZ_{atom_idx}',})\n",
    "    df_test = map_atom_info(df_test,df_struct, atom_idx)\n",
    "    df_test = df_test.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                                'x': f'x_{atom_idx}',\n",
    "                                'y': f'y_{atom_idx}',\n",
    "                                'z': f'z_{atom_idx}'})\n",
    "    #add some features\n",
    "    \n",
    "    df_struct['c_x']=df_struct.groupby('molecule_name')['x'].transform('mean')\n",
    "    df_struct['c_y']=df_struct.groupby('molecule_name')['y'].transform('mean')\n",
    "    df_struct['c_z']=df_struct.groupby('molecule_name')['z'].transform('mean')\n",
    "    df_struct['atom_n']=df_struct.groupby('molecule_name')['atom_index'].transform('max')\n",
    "    \n",
    "    show_ram_usage()\n",
    "    print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = '1JHC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FE021_train_1JHC = pd.read_parquet('../data/FE021/FE021-train-1JHC.parquet', engine='pyarrow')\n",
    "FE021_test_1JHC = pd.read_parquet('../data/FE021/FE021-test-1JHC.parquet', engine='pyarrow')\n",
    "\n",
    "FE021_train_1JHN = pd.read_parquet('../data/FE021/FE021-train-1JHN.parquet', engine='pyarrow')\n",
    "FE021_test_1JHN = pd.read_parquet('../data/FE021/FE021-test-1JHN.parquet', engine='pyarrow')\n",
    "\n",
    "FE021_train_2JHH = pd.read_parquet('../data/FE021/FE021-train-2JHH.parquet', engine='pyarrow')\n",
    "FE021_test_2JHH = pd.read_parquet('../data/FE021/FE021-test-2JHH.parquet', engine='pyarrow')\n",
    "\n",
    "FE021_train_2JHC = pd.read_parquet('../data/FE021/FE021-train-2JHC.parquet', engine='pyarrow')\n",
    "FE021_test_2JHC = pd.read_parquet('../data/FE021/FE021-test-2JHC.parquet', engine='pyarrow')\n",
    "\n",
    "FE021_train_2JHN = pd.read_parquet('../data/FE021/FE021-train-2JHN.parquet', engine='pyarrow')\n",
    "FE021_test_2JHN = pd.read_parquet('../data/FE021/FE021-test-2JHN.parquet', engine='pyarrow')\n",
    "\n",
    "FE021_train_3JHH = pd.read_parquet('../data/FE021/FE021-train-3JHH.parquet', engine='pyarrow')\n",
    "FE021_test_3JHH = pd.read_parquet('../data/FE021/FE021-test-3JHH.parquet', engine='pyarrow')\n",
    "\n",
    "FE021_train_3JHC = pd.read_parquet('../data/FE021/FE021-train-3JHC.parquet', engine='pyarrow')\n",
    "FE021_test_3JHC = pd.read_parquet('../data/FE021/FE021-test-3JHC.parquet', engine='pyarrow')\n",
    "\n",
    "FE021_train_3JHN = pd.read_parquet('../data/FE021/FE021-train-3JHN.parquet', engine='pyarrow')\n",
    "FE021_test_3JHN = pd.read_parquet('../data/FE021/FE021-test-3JHN.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>atom_0</th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>z_0</th>\n",
       "      <th>...</th>\n",
       "      <th>x_farthest_0</th>\n",
       "      <th>y_farthest_0</th>\n",
       "      <th>z_farthest_0</th>\n",
       "      <th>min_distance_x</th>\n",
       "      <th>atom_index_farthest_1</th>\n",
       "      <th>distance_farthest_1</th>\n",
       "      <th>x_farthest_1</th>\n",
       "      <th>y_farthest_1</th>\n",
       "      <th>z_farthest_1</th>\n",
       "      <th>min_distance_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.8076</td>\n",
       "      <td>H</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>1.091946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.2570</td>\n",
       "      <td>H</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>3</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>1.091952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.2548</td>\n",
       "      <td>H</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>2</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>1.091946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.2543</td>\n",
       "      <td>H</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>1.091948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.8074</td>\n",
       "      <td>H</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>1.091946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     molecule_name  atom_index_0  atom_index_1  type  \\\n",
       "0   0  dsgdb9nsd_000001             1             0  1JHC   \n",
       "1   1  dsgdb9nsd_000001             1             2  2JHH   \n",
       "2   2  dsgdb9nsd_000001             1             3  2JHH   \n",
       "3   3  dsgdb9nsd_000001             1             4  2JHH   \n",
       "4   4  dsgdb9nsd_000001             2             0  1JHC   \n",
       "\n",
       "   scalar_coupling_constant atom_0       x_0       y_0       z_0  ...  \\\n",
       "0                   84.8076      H  0.002150 -0.006031  0.001976  ...   \n",
       "1                  -11.2570      H  0.002150 -0.006031  0.001976  ...   \n",
       "2                  -11.2548      H  0.002150 -0.006031  0.001976  ...   \n",
       "3                  -11.2543      H  0.002150 -0.006031  0.001976  ...   \n",
       "4                   84.8074      H  1.011731  1.463751  0.000277  ...   \n",
       "\n",
       "   x_farthest_0  y_farthest_0  z_farthest_0  min_distance_x  \\\n",
       "0     -0.523814      1.437933      0.906397        1.091953   \n",
       "1     -0.523814      1.437933      0.906397        1.091953   \n",
       "2     -0.523814      1.437933      0.906397        1.091953   \n",
       "3     -0.523814      1.437933      0.906397        1.091953   \n",
       "4     -0.540815      1.447527     -0.876644        1.091952   \n",
       "\n",
       "   atom_index_farthest_1  distance_farthest_1  x_farthest_1  y_farthest_1  \\\n",
       "0                      1             1.091953      0.002150     -0.006031   \n",
       "1                      3             1.783158     -0.540815      1.447527   \n",
       "2                      2             1.783158      1.011731      1.463751   \n",
       "3                      1             1.783157      0.002150     -0.006031   \n",
       "4                      1             1.091953      0.002150     -0.006031   \n",
       "\n",
       "   z_farthest_1  min_distance_y  \n",
       "0      0.001976        1.091946  \n",
       "1     -0.876644        1.091952  \n",
       "2      0.000277        1.091946  \n",
       "3      0.001976        1.091948  \n",
       "4      0.001976        1.091946  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sort_values('id', inplace=True)\n",
    "df_test.sort_values( 'id', inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_case = df_train.loc[df_train.type==case, :].reset_index(drop=True)\n",
    "df_test_case = df_test.loc[df_test.type==case, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709416, 709416, 380609)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_case), len(FE021_train_1JHC), len(df_test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10th_closest_to_1_valence_x_cube_inv_dist</th>\n",
       "      <th>12th_closest_to_1_valence</th>\n",
       "      <th>13th_closest_to_1_exact_mass</th>\n",
       "      <th>17th_closest_to_0_spin_multiplicity</th>\n",
       "      <th>17th_closest_to_0_valence</th>\n",
       "      <th>17th_closest_to_0_valence_x_cube_inv_dist</th>\n",
       "      <th>17th_closest_to_1_spin_multiplicity</th>\n",
       "      <th>18th_closest_to_0_exact_mass</th>\n",
       "      <th>19th_closest_to_0_spin_multiplicity</th>\n",
       "      <th>19th_closest_to_1_spin_multiplicity_x_cube_inv_dist</th>\n",
       "      <th>...</th>\n",
       "      <th>tor_ang_2leftleft_count</th>\n",
       "      <th>tor_ang_2leftleft_max</th>\n",
       "      <th>tor_ang_2leftleft_min</th>\n",
       "      <th>val_not_0_mean</th>\n",
       "      <th>val_not_1_mean</th>\n",
       "      <th>val_not_1_std</th>\n",
       "      <th>yukawa_H.x</th>\n",
       "      <th>yukawa_H.y</th>\n",
       "      <th>yukawa_N.x</th>\n",
       "      <th>yukawa_O.y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.08638</td>\n",
       "      <td>1.969477</td>\n",
       "      <td>6.210087</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>1.538449</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>3.800028</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>4.615952</td>\n",
       "      <td>125.460482</td>\n",
       "      <td>-124.658907</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504328</td>\n",
       "      <td>1.342247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.08638</td>\n",
       "      <td>1.969477</td>\n",
       "      <td>6.210087</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>1.538449</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>3.800028</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>4.615952</td>\n",
       "      <td>125.460482</td>\n",
       "      <td>-124.658907</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504327</td>\n",
       "      <td>1.342247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08638</td>\n",
       "      <td>1.969477</td>\n",
       "      <td>6.210087</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>1.538449</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>3.800028</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>4.615952</td>\n",
       "      <td>125.460482</td>\n",
       "      <td>-124.658907</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504323</td>\n",
       "      <td>1.342247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.08638</td>\n",
       "      <td>1.969477</td>\n",
       "      <td>6.210087</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>1.538449</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>3.800028</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>4.615952</td>\n",
       "      <td>125.460482</td>\n",
       "      <td>-124.658907</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504323</td>\n",
       "      <td>1.342247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.08638</td>\n",
       "      <td>1.969477</td>\n",
       "      <td>6.210087</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>1.538449</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>3.800028</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>4.615952</td>\n",
       "      <td>125.460482</td>\n",
       "      <td>-124.658907</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344177</td>\n",
       "      <td>0.108789</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10th_closest_to_1_valence_x_cube_inv_dist  12th_closest_to_1_valence  \\\n",
       "0                                    0.08638                   1.969477   \n",
       "1                                    0.08638                   1.969477   \n",
       "2                                    0.08638                   1.969477   \n",
       "3                                    0.08638                   1.969477   \n",
       "4                                    0.08638                   1.969477   \n",
       "\n",
       "   13th_closest_to_1_exact_mass  17th_closest_to_0_spin_multiplicity  \\\n",
       "0                      6.210087                             0.001063   \n",
       "1                      6.210087                             0.001063   \n",
       "2                      6.210087                             0.001063   \n",
       "3                      6.210087                             0.001063   \n",
       "4                      6.210087                             0.001063   \n",
       "\n",
       "   17th_closest_to_0_valence  17th_closest_to_0_valence_x_cube_inv_dist  \\\n",
       "0                   1.538449                                   0.013848   \n",
       "1                   1.538449                                   0.013848   \n",
       "2                   1.538449                                   0.013848   \n",
       "3                   1.538449                                   0.013848   \n",
       "4                   1.538449                                   0.013848   \n",
       "\n",
       "   17th_closest_to_1_spin_multiplicity  18th_closest_to_0_exact_mass  \\\n",
       "0                             0.001074                      3.800028   \n",
       "1                             0.001074                      3.800028   \n",
       "2                             0.001074                      3.800028   \n",
       "3                             0.001074                      3.800028   \n",
       "4                             0.001074                      3.800028   \n",
       "\n",
       "   19th_closest_to_0_spin_multiplicity  \\\n",
       "0                              0.00024   \n",
       "1                              0.00024   \n",
       "2                              0.00024   \n",
       "3                              0.00024   \n",
       "4                              0.00024   \n",
       "\n",
       "   19th_closest_to_1_spin_multiplicity_x_cube_inv_dist  ...  \\\n",
       "0                                           0.000003    ...   \n",
       "1                                           0.000003    ...   \n",
       "2                                           0.000003    ...   \n",
       "3                                           0.000003    ...   \n",
       "4                                           0.000003    ...   \n",
       "\n",
       "   tor_ang_2leftleft_count  tor_ang_2leftleft_max  tor_ang_2leftleft_min  \\\n",
       "0                 4.615952             125.460482            -124.658907   \n",
       "1                 4.615952             125.460482            -124.658907   \n",
       "2                 4.615952             125.460482            -124.658907   \n",
       "3                 4.615952             125.460482            -124.658907   \n",
       "4                 4.615952             125.460482            -124.658907   \n",
       "\n",
       "   val_not_0_mean  val_not_1_mean  val_not_1_std  yukawa_H.x  yukawa_H.y  \\\n",
       "0            1.75             1.0            0.0    0.504328    1.342247   \n",
       "1            1.75             1.0            0.0    0.504327    1.342247   \n",
       "2            1.75             1.0            0.0    0.504323    1.342247   \n",
       "3            1.75             1.0            0.0    0.504323    1.342247   \n",
       "4            1.50             1.0            0.0    0.000000    0.344177   \n",
       "\n",
       "   yukawa_N.x  yukawa_O.y  \n",
       "0    0.000000         0.0  \n",
       "1    0.000000         0.0  \n",
       "2    0.000000         0.0  \n",
       "3    0.000000         0.0  \n",
       "4    0.108789         0.0  \n",
       "\n",
       "[5 rows x 351 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Impute NA with mean\n",
    "MEAN = pd.concat([FE021_train_1JHC,FE021_test_1JHC]).mean()\n",
    "FE021_train_1JHC.fillna( value=MEAN, inplace=True )\n",
    "FE021_test_1JHC.fillna( value=MEAN, inplace=True )\n",
    "FE021_train_1JHC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train.shape)\n",
    "df_train_case_final = pd.concat( [df_train_case,FE021_train_1JHC], axis=1 )\n",
    "df_test_case_final  = pd.concat( [df_test_case ,FE021_test_1JHC ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709416, 380609)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_case_final), len(df_test_case_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24th_closest_to_1_spin_multiplicity',\n",
       " '24th_closest_to_1_spin_multiplicity_x_cube_inv_dist',\n",
       " '25th_closest_to_0_spin_multiplicity_x_cube_inv_dist',\n",
       " '26th_closest_to_0_spin_multiplicity_x_cube_inv_dist',\n",
       " '2nd_closest_to_0_exact_mass',\n",
       " '2nd_closest_to_0_valence',\n",
       " '2nd_closest_to_0_valence_x_cube_inv_dist',\n",
       " '2nd_closest_to_1',\n",
       " '2nd_closest_to_1_exact_mass',\n",
       " '2nd_closest_to_1_valence']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_featus =  list(df_train_case_final.columns[81:])\n",
    "extra_featus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(1024)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "#     x = Dense(256)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = LeakyReLU(alpha=0.05)(x)\n",
    "#     #x = Dropout(0.4)(x)\n",
    "#     x = Dense(256)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = LeakyReLU(alpha=0.05)(x)\n",
    "    #x = Dropout(0.4)(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    #x = Dropout(0.4)(x)\n",
    "    out1 = Dense(2, activation=\"linear\",name='outM2')(x)#mulliken charge 2\n",
    "    out2 = Dense(6, activation=\"linear\",name='outT6')(x)#tensor 6(xx,yy,zz)\n",
    "    out3 = Dense(12, activation=\"linear\",name='outT12')(x)#tensor 12(others) \n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "    #x = Dropout(0.2)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.10)(x)\n",
    "#     x = Dense(128)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = LeakyReLU(alpha=0.05)(x)\n",
    "    #x = Dropout(0.2)(x)\n",
    "    out = Dense(1, activation=\"linear\",name='out')(x)#scalar_coupling_constant    \n",
    "    model = Model(inputs=inp, outputs=[out,out1,out2,out3])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/robmulla/anaconda3/envs/kaggle/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/robmulla/anaconda3/envs/kaggle/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         11264       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1024)         4096        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 1024)         0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         1049600     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1024)         4096        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 1024)         0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         1049600     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1024)         4096        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 1024)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1024)         0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         1049600     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1024)         4096        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 1024)         0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          262400      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256)          1024        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 256)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          32896       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128)          512         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 128)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 1)            129         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "outM2 (Dense)                   (None, 2)            2050        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "outT6 (Dense)                   (None, 6)            6150        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "outT12 (Dense)                  (None, 12)           12300       leaky_re_lu_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,493,909\n",
      "Trainable params: 3,484,949\n",
      "Non-trainable params: 8,960\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_model=create_nn_model( 10 )\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Function\n",
    "I rely a lot on loss plots to detect when learning has stopped as well as when overfitting begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, label):\n",
    "    plt.plot(history.history['loss'][-100:])\n",
    "    plt.plot(history.history['val_loss'][-100:])\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _= plt.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1JHC out of ['1JHC'] \n",
      "\n",
      "(567515, 340) (141901, 340)\n",
      "WARNING:tensorflow:From /home/robmulla/anaconda3/envs/kaggle/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 567515 samples, validate on 141901 samples\n",
      "Epoch 1/500\n",
      " - 151s - loss: 60.7071 - out_loss: 58.1913 - outM2_loss: 0.2723 - outT6_loss: 1.5156 - outT12_loss: 0.7280 - val_loss: 40.7646 - val_out_loss: 35.7063 - val_outM2_loss: 1.2970 - val_outT6_loss: 2.9663 - val_outT12_loss: 0.7949\n",
      "Epoch 2/500\n",
      " - 146s - loss: 4.4855 - out_loss: 2.1151 - outM2_loss: 0.2348 - outT6_loss: 1.4169 - outT12_loss: 0.7187 - val_loss: 3.8817 - val_out_loss: 1.6163 - val_outM2_loss: 0.2231 - val_outT6_loss: 1.3224 - val_outT12_loss: 0.7200\n",
      "Epoch 3/500\n",
      " - 147s - loss: 3.7707 - out_loss: 1.5247 - outM2_loss: 0.1964 - outT6_loss: 1.3323 - outT12_loss: 0.7172 - val_loss: 3.3815 - val_out_loss: 1.2752 - val_outM2_loss: 0.1451 - val_outT6_loss: 1.2449 - val_outT12_loss: 0.7163\n",
      "Epoch 4/500\n",
      " - 147s - loss: 3.5402 - out_loss: 1.3537 - outM2_loss: 0.1812 - outT6_loss: 1.2886 - outT12_loss: 0.7167 - val_loss: 3.2195 - val_out_loss: 1.1324 - val_outM2_loss: 0.1393 - val_outT6_loss: 1.2306 - val_outT12_loss: 0.7173\n",
      "Epoch 5/500\n",
      " - 148s - loss: 3.4248 - out_loss: 1.2781 - outM2_loss: 0.1743 - outT6_loss: 1.2562 - outT12_loss: 0.7161 - val_loss: 3.1496 - val_out_loss: 1.0861 - val_outM2_loss: 0.1858 - val_outT6_loss: 1.1596 - val_outT12_loss: 0.7182\n",
      "Epoch 6/500\n",
      " - 147s - loss: 3.3136 - out_loss: 1.2048 - outM2_loss: 0.1639 - outT6_loss: 1.2289 - outT12_loss: 0.7160 - val_loss: 3.0999 - val_out_loss: 1.0603 - val_outM2_loss: 0.1448 - val_outT6_loss: 1.1772 - val_outT12_loss: 0.7175\n",
      "Epoch 7/500\n",
      " - 146s - loss: 3.2481 - out_loss: 1.1627 - outM2_loss: 0.1619 - outT6_loss: 1.2078 - outT12_loss: 0.7156 - val_loss: 2.9444 - val_out_loss: 0.9846 - val_outM2_loss: 0.1099 - val_outT6_loss: 1.1333 - val_outT12_loss: 0.7166\n",
      "Epoch 8/500\n",
      " - 145s - loss: 3.1630 - out_loss: 1.1073 - outM2_loss: 0.1538 - outT6_loss: 1.1867 - outT12_loss: 0.7152 - val_loss: 2.9342 - val_out_loss: 0.9981 - val_outM2_loss: 0.0905 - val_outT6_loss: 1.1284 - val_outT12_loss: 0.7173\n",
      "Epoch 9/500\n",
      " - 146s - loss: 3.1315 - out_loss: 1.0944 - outM2_loss: 0.1494 - outT6_loss: 1.1726 - outT12_loss: 0.7151 - val_loss: 2.9720 - val_out_loss: 1.0322 - val_outM2_loss: 0.1170 - val_outT6_loss: 1.1050 - val_outT12_loss: 0.7179\n",
      "Epoch 10/500\n",
      " - 146s - loss: 3.0792 - out_loss: 1.0597 - outM2_loss: 0.1495 - outT6_loss: 1.1551 - outT12_loss: 0.7149 - val_loss: 2.9187 - val_out_loss: 1.0226 - val_outM2_loss: 0.1003 - val_outT6_loss: 1.0797 - val_outT12_loss: 0.7161\n",
      "Epoch 11/500\n",
      " - 146s - loss: 3.0394 - out_loss: 1.0369 - outM2_loss: 0.1453 - outT6_loss: 1.1425 - outT12_loss: 0.7148 - val_loss: 2.7529 - val_out_loss: 0.8551 - val_outM2_loss: 0.0990 - val_outT6_loss: 1.0820 - val_outT12_loss: 0.7168\n",
      "Epoch 12/500\n",
      " - 146s - loss: 3.0141 - out_loss: 1.0263 - outM2_loss: 0.1438 - outT6_loss: 1.1295 - outT12_loss: 0.7145 - val_loss: 2.7453 - val_out_loss: 0.8677 - val_outM2_loss: 0.1018 - val_outT6_loss: 1.0575 - val_outT12_loss: 0.7182\n",
      "Epoch 13/500\n",
      " - 146s - loss: 2.9833 - out_loss: 1.0096 - outM2_loss: 0.1412 - outT6_loss: 1.1180 - outT12_loss: 0.7144 - val_loss: 2.7793 - val_out_loss: 0.9114 - val_outM2_loss: 0.1030 - val_outT6_loss: 1.0483 - val_outT12_loss: 0.7166\n",
      "Epoch 14/500\n",
      " - 144s - loss: 2.9473 - out_loss: 0.9833 - outM2_loss: 0.1422 - outT6_loss: 1.1076 - outT12_loss: 0.7142 - val_loss: 2.7837 - val_out_loss: 0.9017 - val_outM2_loss: 0.1081 - val_outT6_loss: 1.0585 - val_outT12_loss: 0.7154\n",
      "Epoch 15/500\n",
      " - 146s - loss: 2.9139 - out_loss: 0.9607 - outM2_loss: 0.1409 - outT6_loss: 1.0983 - outT12_loss: 0.7140 - val_loss: 2.6440 - val_out_loss: 0.7880 - val_outM2_loss: 0.1192 - val_outT6_loss: 1.0216 - val_outT12_loss: 0.7152\n",
      "Epoch 16/500\n",
      " - 156s - loss: 2.8700 - out_loss: 0.9320 - outM2_loss: 0.1352 - outT6_loss: 1.0888 - outT12_loss: 0.7139 - val_loss: 2.6928 - val_out_loss: 0.8771 - val_outM2_loss: 0.0869 - val_outT6_loss: 1.0120 - val_outT12_loss: 0.7168\n",
      "Epoch 17/500\n",
      " - 164s - loss: 2.8504 - out_loss: 0.9231 - outM2_loss: 0.1331 - outT6_loss: 1.0806 - outT12_loss: 0.7136 - val_loss: 2.6893 - val_out_loss: 0.8324 - val_outM2_loss: 0.0946 - val_outT6_loss: 1.0459 - val_outT12_loss: 0.7163\n",
      "Epoch 18/500\n",
      " - 162s - loss: 2.8371 - out_loss: 0.9152 - outM2_loss: 0.1342 - outT6_loss: 1.0740 - outT12_loss: 0.7137 - val_loss: 2.7934 - val_out_loss: 0.9210 - val_outM2_loss: 0.1295 - val_outT6_loss: 1.0286 - val_outT12_loss: 0.7142\n",
      "Epoch 19/500\n",
      " - 162s - loss: 2.8197 - out_loss: 0.9072 - outM2_loss: 0.1327 - outT6_loss: 1.0665 - outT12_loss: 0.7133 - val_loss: 2.5684 - val_out_loss: 0.7553 - val_outM2_loss: 0.1038 - val_outT6_loss: 0.9952 - val_outT12_loss: 0.7142\n",
      "Epoch 20/500\n",
      " - 157s - loss: 2.7853 - out_loss: 0.8842 - outM2_loss: 0.1291 - outT6_loss: 1.0588 - outT12_loss: 0.7132 - val_loss: 2.5243 - val_out_loss: 0.7328 - val_outM2_loss: 0.0758 - val_outT6_loss: 1.0003 - val_outT12_loss: 0.7154\n",
      "Epoch 21/500\n",
      " - 146s - loss: 2.7606 - out_loss: 0.8654 - outM2_loss: 0.1291 - outT6_loss: 1.0530 - outT12_loss: 0.7131 - val_loss: 2.6387 - val_out_loss: 0.8362 - val_outM2_loss: 0.0905 - val_outT6_loss: 0.9984 - val_outT12_loss: 0.7135\n",
      "Epoch 22/500\n",
      " - 146s - loss: 2.7605 - out_loss: 0.8714 - outM2_loss: 0.1287 - outT6_loss: 1.0475 - outT12_loss: 0.7129 - val_loss: 2.4981 - val_out_loss: 0.7494 - val_outM2_loss: 0.0671 - val_outT6_loss: 0.9668 - val_outT12_loss: 0.7147\n",
      "Epoch 23/500\n",
      " - 146s - loss: 2.7394 - out_loss: 0.8565 - outM2_loss: 0.1277 - outT6_loss: 1.0422 - outT12_loss: 0.7129 - val_loss: 2.5017 - val_out_loss: 0.7215 - val_outM2_loss: 0.0885 - val_outT6_loss: 0.9784 - val_outT12_loss: 0.7133\n",
      "Epoch 24/500\n",
      " - 147s - loss: 2.7146 - out_loss: 0.8375 - outM2_loss: 0.1291 - outT6_loss: 1.0354 - outT12_loss: 0.7126 - val_loss: 2.4827 - val_out_loss: 0.7408 - val_outM2_loss: 0.0705 - val_outT6_loss: 0.9563 - val_outT12_loss: 0.7150\n",
      "Epoch 25/500\n",
      " - 147s - loss: 2.7076 - out_loss: 0.8381 - outM2_loss: 0.1262 - outT6_loss: 1.0306 - outT12_loss: 0.7127 - val_loss: 2.6172 - val_out_loss: 0.8085 - val_outM2_loss: 0.0919 - val_outT6_loss: 1.0030 - val_outT12_loss: 0.7139\n",
      "Epoch 26/500\n",
      " - 146s - loss: 2.6936 - out_loss: 0.8293 - outM2_loss: 0.1253 - outT6_loss: 1.0266 - outT12_loss: 0.7125 - val_loss: 2.5544 - val_out_loss: 0.7594 - val_outM2_loss: 0.0904 - val_outT6_loss: 0.9910 - val_outT12_loss: 0.7136\n",
      "Epoch 27/500\n",
      " - 145s - loss: 2.6929 - out_loss: 0.8318 - outM2_loss: 0.1284 - outT6_loss: 1.0205 - outT12_loss: 0.7122 - val_loss: 2.5140 - val_out_loss: 0.7395 - val_outM2_loss: 0.0783 - val_outT6_loss: 0.9821 - val_outT12_loss: 0.7142\n",
      "Epoch 28/500\n",
      " - 145s - loss: 2.6666 - out_loss: 0.8146 - outM2_loss: 0.1234 - outT6_loss: 1.0164 - outT12_loss: 0.7122 - val_loss: 2.4490 - val_out_loss: 0.7070 - val_outM2_loss: 0.0879 - val_outT6_loss: 0.9403 - val_outT12_loss: 0.7138\n",
      "Epoch 29/500\n",
      " - 145s - loss: 2.6545 - out_loss: 0.8075 - outM2_loss: 0.1228 - outT6_loss: 1.0122 - outT12_loss: 0.7120 - val_loss: 2.5710 - val_out_loss: 0.7835 - val_outM2_loss: 0.0906 - val_outT6_loss: 0.9824 - val_outT12_loss: 0.7145\n",
      "Epoch 30/500\n",
      " - 146s - loss: 2.6411 - out_loss: 0.7983 - outM2_loss: 0.1233 - outT6_loss: 1.0078 - outT12_loss: 0.7117 - val_loss: 2.4585 - val_out_loss: 0.6847 - val_outM2_loss: 0.0909 - val_outT6_loss: 0.9689 - val_outT12_loss: 0.7141\n",
      "Epoch 31/500\n",
      " - 145s - loss: 2.6385 - out_loss: 0.8018 - outM2_loss: 0.1212 - outT6_loss: 1.0036 - outT12_loss: 0.7119 - val_loss: 2.4711 - val_out_loss: 0.7079 - val_outM2_loss: 0.0826 - val_outT6_loss: 0.9686 - val_outT12_loss: 0.7121\n",
      "Epoch 32/500\n",
      " - 146s - loss: 2.6297 - out_loss: 0.7975 - outM2_loss: 0.1206 - outT6_loss: 1.0000 - outT12_loss: 0.7116 - val_loss: 2.4425 - val_out_loss: 0.7300 - val_outM2_loss: 0.0758 - val_outT6_loss: 0.9239 - val_outT12_loss: 0.7127\n",
      "Epoch 33/500\n",
      " - 145s - loss: 2.6124 - out_loss: 0.7862 - outM2_loss: 0.1188 - outT6_loss: 0.9958 - outT12_loss: 0.7116 - val_loss: 2.4045 - val_out_loss: 0.6557 - val_outM2_loss: 0.0874 - val_outT6_loss: 0.9485 - val_outT12_loss: 0.7128\n",
      "Epoch 34/500\n",
      " - 145s - loss: 2.6039 - out_loss: 0.7811 - outM2_loss: 0.1197 - outT6_loss: 0.9917 - outT12_loss: 0.7114 - val_loss: 2.4627 - val_out_loss: 0.7273 - val_outM2_loss: 0.0761 - val_outT6_loss: 0.9475 - val_outT12_loss: 0.7118\n",
      "Epoch 35/500\n",
      " - 145s - loss: 2.5882 - out_loss: 0.7715 - outM2_loss: 0.1186 - outT6_loss: 0.9869 - outT12_loss: 0.7113 - val_loss: 2.3655 - val_out_loss: 0.6574 - val_outM2_loss: 0.0778 - val_outT6_loss: 0.9183 - val_outT12_loss: 0.7120\n",
      "Epoch 36/500\n",
      " - 146s - loss: 2.5850 - out_loss: 0.7697 - outM2_loss: 0.1175 - outT6_loss: 0.9867 - outT12_loss: 0.7112 - val_loss: 2.4428 - val_out_loss: 0.7402 - val_outM2_loss: 0.0674 - val_outT6_loss: 0.9220 - val_outT12_loss: 0.7132\n",
      "Epoch 37/500\n",
      " - 146s - loss: 2.5711 - out_loss: 0.7609 - outM2_loss: 0.1181 - outT6_loss: 0.9811 - outT12_loss: 0.7110 - val_loss: 2.4791 - val_out_loss: 0.7516 - val_outM2_loss: 0.0796 - val_outT6_loss: 0.9345 - val_outT12_loss: 0.7135\n",
      "Epoch 38/500\n",
      " - 145s - loss: 2.5758 - out_loss: 0.7687 - outM2_loss: 0.1166 - outT6_loss: 0.9796 - outT12_loss: 0.7109 - val_loss: 2.4427 - val_out_loss: 0.7394 - val_outM2_loss: 0.0615 - val_outT6_loss: 0.9302 - val_outT12_loss: 0.7117\n",
      "Epoch 39/500\n",
      " - 145s - loss: 2.5579 - out_loss: 0.7535 - outM2_loss: 0.1170 - outT6_loss: 0.9767 - outT12_loss: 0.7108 - val_loss: 2.3541 - val_out_loss: 0.6447 - val_outM2_loss: 0.0701 - val_outT6_loss: 0.9276 - val_outT12_loss: 0.7117\n",
      "Epoch 40/500\n",
      " - 145s - loss: 2.5431 - out_loss: 0.7448 - outM2_loss: 0.1162 - outT6_loss: 0.9714 - outT12_loss: 0.7106 - val_loss: 2.3641 - val_out_loss: 0.6422 - val_outM2_loss: 0.0936 - val_outT6_loss: 0.9164 - val_outT12_loss: 0.7118\n",
      "Epoch 41/500\n",
      " - 145s - loss: 2.5505 - out_loss: 0.7528 - outM2_loss: 0.1171 - outT6_loss: 0.9702 - outT12_loss: 0.7105 - val_loss: 2.4090 - val_out_loss: 0.6725 - val_outM2_loss: 0.0784 - val_outT6_loss: 0.9457 - val_outT12_loss: 0.7124\n",
      "Epoch 42/500\n",
      " - 145s - loss: 2.5403 - out_loss: 0.7486 - outM2_loss: 0.1149 - outT6_loss: 0.9664 - outT12_loss: 0.7104 - val_loss: 2.3230 - val_out_loss: 0.6337 - val_outM2_loss: 0.0780 - val_outT6_loss: 0.9008 - val_outT12_loss: 0.7105\n",
      "Epoch 43/500\n",
      " - 146s - loss: 2.5349 - out_loss: 0.7454 - outM2_loss: 0.1150 - outT6_loss: 0.9642 - outT12_loss: 0.7103 - val_loss: 2.5163 - val_out_loss: 0.8048 - val_outM2_loss: 0.0801 - val_outT6_loss: 0.9195 - val_outT12_loss: 0.7119\n",
      "Epoch 44/500\n",
      " - 147s - loss: 2.5305 - out_loss: 0.7421 - outM2_loss: 0.1165 - outT6_loss: 0.9619 - outT12_loss: 0.7101 - val_loss: 2.3011 - val_out_loss: 0.6267 - val_outM2_loss: 0.0750 - val_outT6_loss: 0.8871 - val_outT12_loss: 0.7123\n",
      "Epoch 45/500\n",
      " - 146s - loss: 2.5184 - out_loss: 0.7356 - outM2_loss: 0.1138 - outT6_loss: 0.9591 - outT12_loss: 0.7100 - val_loss: 2.5210 - val_out_loss: 0.7839 - val_outM2_loss: 0.0971 - val_outT6_loss: 0.9277 - val_outT12_loss: 0.7123\n",
      "Epoch 46/500\n",
      " - 146s - loss: 2.5152 - out_loss: 0.7351 - outM2_loss: 0.1134 - outT6_loss: 0.9567 - outT12_loss: 0.7100 - val_loss: 2.3906 - val_out_loss: 0.7418 - val_outM2_loss: 0.0604 - val_outT6_loss: 0.8777 - val_outT12_loss: 0.7108\n",
      "Epoch 47/500\n",
      " - 146s - loss: 2.5130 - out_loss: 0.7341 - outM2_loss: 0.1144 - outT6_loss: 0.9548 - outT12_loss: 0.7097 - val_loss: 2.3747 - val_out_loss: 0.6846 - val_outM2_loss: 0.0723 - val_outT6_loss: 0.9057 - val_outT12_loss: 0.7122\n",
      "Epoch 48/500\n",
      " - 146s - loss: 2.4883 - out_loss: 0.7155 - outM2_loss: 0.1117 - outT6_loss: 0.9515 - outT12_loss: 0.7096 - val_loss: 2.4487 - val_out_loss: 0.7245 - val_outM2_loss: 0.0848 - val_outT6_loss: 0.9281 - val_outT12_loss: 0.7113\n",
      "Epoch 49/500\n",
      " - 145s - loss: 2.4976 - out_loss: 0.7268 - outM2_loss: 0.1115 - outT6_loss: 0.9497 - outT12_loss: 0.7096 - val_loss: 2.3501 - val_out_loss: 0.6317 - val_outM2_loss: 0.0715 - val_outT6_loss: 0.9357 - val_outT12_loss: 0.7113\n",
      "Epoch 50/500\n",
      " - 145s - loss: 2.4871 - out_loss: 0.7197 - outM2_loss: 0.1102 - outT6_loss: 0.9478 - outT12_loss: 0.7094 - val_loss: 2.3148 - val_out_loss: 0.6330 - val_outM2_loss: 0.0640 - val_outT6_loss: 0.9070 - val_outT12_loss: 0.7107\n",
      "Epoch 51/500\n",
      " - 145s - loss: 2.4840 - out_loss: 0.7172 - outM2_loss: 0.1129 - outT6_loss: 0.9446 - outT12_loss: 0.7093 - val_loss: 2.2754 - val_out_loss: 0.5982 - val_outM2_loss: 0.0728 - val_outT6_loss: 0.8935 - val_outT12_loss: 0.7108\n",
      "Epoch 52/500\n",
      " - 146s - loss: 2.4733 - out_loss: 0.7113 - outM2_loss: 0.1105 - outT6_loss: 0.9425 - outT12_loss: 0.7091 - val_loss: 2.3081 - val_out_loss: 0.6675 - val_outM2_loss: 0.0626 - val_outT6_loss: 0.8676 - val_outT12_loss: 0.7104\n",
      "Epoch 53/500\n",
      " - 146s - loss: 2.4713 - out_loss: 0.7104 - outM2_loss: 0.1114 - outT6_loss: 0.9404 - outT12_loss: 0.7091 - val_loss: 2.3454 - val_out_loss: 0.6904 - val_outM2_loss: 0.0663 - val_outT6_loss: 0.8789 - val_outT12_loss: 0.7099\n",
      "Epoch 54/500\n",
      " - 145s - loss: 2.4666 - out_loss: 0.7091 - outM2_loss: 0.1109 - outT6_loss: 0.9378 - outT12_loss: 0.7088 - val_loss: 2.2606 - val_out_loss: 0.5907 - val_outM2_loss: 0.0590 - val_outT6_loss: 0.9004 - val_outT12_loss: 0.7105\n",
      "Epoch 55/500\n",
      " - 146s - loss: 2.4649 - out_loss: 0.7105 - outM2_loss: 0.1092 - outT6_loss: 0.9363 - outT12_loss: 0.7088 - val_loss: 2.2617 - val_out_loss: 0.6090 - val_outM2_loss: 0.0798 - val_outT6_loss: 0.8636 - val_outT12_loss: 0.7094\n",
      "Epoch 56/500\n",
      " - 146s - loss: 2.4642 - out_loss: 0.7110 - outM2_loss: 0.1107 - outT6_loss: 0.9341 - outT12_loss: 0.7085 - val_loss: 2.2540 - val_out_loss: 0.5979 - val_outM2_loss: 0.0803 - val_outT6_loss: 0.8657 - val_outT12_loss: 0.7101\n",
      "Epoch 57/500\n",
      " - 146s - loss: 2.4515 - out_loss: 0.7017 - outM2_loss: 0.1095 - outT6_loss: 0.9317 - outT12_loss: 0.7085 - val_loss: 2.3588 - val_out_loss: 0.7108 - val_outM2_loss: 0.0670 - val_outT6_loss: 0.8706 - val_outT12_loss: 0.7105\n",
      "Epoch 58/500\n",
      " - 146s - loss: 2.4465 - out_loss: 0.7001 - outM2_loss: 0.1081 - outT6_loss: 0.9299 - outT12_loss: 0.7083 - val_loss: 2.2551 - val_out_loss: 0.6201 - val_outM2_loss: 0.0650 - val_outT6_loss: 0.8596 - val_outT12_loss: 0.7105\n",
      "Epoch 59/500\n",
      " - 145s - loss: 2.4423 - out_loss: 0.6978 - outM2_loss: 0.1088 - outT6_loss: 0.9275 - outT12_loss: 0.7083 - val_loss: 2.3509 - val_out_loss: 0.6775 - val_outM2_loss: 0.0922 - val_outT6_loss: 0.8713 - val_outT12_loss: 0.7100\n",
      "Epoch 60/500\n",
      " - 145s - loss: 2.4355 - out_loss: 0.6918 - outM2_loss: 0.1096 - outT6_loss: 0.9260 - outT12_loss: 0.7081 - val_loss: 2.3641 - val_out_loss: 0.6729 - val_outM2_loss: 0.0719 - val_outT6_loss: 0.9099 - val_outT12_loss: 0.7093\n",
      "Epoch 61/500\n",
      " - 146s - loss: 2.4376 - out_loss: 0.6967 - outM2_loss: 0.1085 - outT6_loss: 0.9244 - outT12_loss: 0.7080 - val_loss: 2.2327 - val_out_loss: 0.6100 - val_outM2_loss: 0.0599 - val_outT6_loss: 0.8539 - val_outT12_loss: 0.7089\n",
      "Epoch 62/500\n",
      " - 146s - loss: 2.4274 - out_loss: 0.6898 - outM2_loss: 0.1082 - outT6_loss: 0.9215 - outT12_loss: 0.7078 - val_loss: 2.2854 - val_out_loss: 0.6278 - val_outM2_loss: 0.0565 - val_outT6_loss: 0.8913 - val_outT12_loss: 0.7098\n",
      "Epoch 63/500\n",
      " - 145s - loss: 2.4228 - out_loss: 0.6848 - outM2_loss: 0.1090 - outT6_loss: 0.9212 - outT12_loss: 0.7078 - val_loss: 2.2521 - val_out_loss: 0.6396 - val_outM2_loss: 0.0557 - val_outT6_loss: 0.8481 - val_outT12_loss: 0.7088\n",
      "Epoch 64/500\n",
      " - 145s - loss: 2.4251 - out_loss: 0.6913 - outM2_loss: 0.1064 - outT6_loss: 0.9196 - outT12_loss: 0.7077 - val_loss: 2.2233 - val_out_loss: 0.6064 - val_outM2_loss: 0.0663 - val_outT6_loss: 0.8424 - val_outT12_loss: 0.7081\n",
      "Epoch 65/500\n",
      " - 147s - loss: 2.4136 - out_loss: 0.6813 - outM2_loss: 0.1069 - outT6_loss: 0.9177 - outT12_loss: 0.7076 - val_loss: 2.2140 - val_out_loss: 0.5847 - val_outM2_loss: 0.0729 - val_outT6_loss: 0.8486 - val_outT12_loss: 0.7077\n",
      "Epoch 66/500\n",
      " - 146s - loss: 2.4129 - out_loss: 0.6830 - outM2_loss: 0.1067 - outT6_loss: 0.9159 - outT12_loss: 0.7073 - val_loss: 2.2611 - val_out_loss: 0.6041 - val_outM2_loss: 0.0657 - val_outT6_loss: 0.8833 - val_outT12_loss: 0.7080\n",
      "Epoch 67/500\n",
      " - 149s - loss: 2.4065 - out_loss: 0.6789 - outM2_loss: 0.1056 - outT6_loss: 0.9148 - outT12_loss: 0.7073 - val_loss: 2.2270 - val_out_loss: 0.5640 - val_outM2_loss: 0.0665 - val_outT6_loss: 0.8872 - val_outT12_loss: 0.7093\n",
      "Epoch 68/500\n",
      " - 153s - loss: 2.4039 - out_loss: 0.6774 - outM2_loss: 0.1071 - outT6_loss: 0.9123 - outT12_loss: 0.7070 - val_loss: 2.2833 - val_out_loss: 0.6660 - val_outM2_loss: 0.0667 - val_outT6_loss: 0.8429 - val_outT12_loss: 0.7077\n",
      "Epoch 69/500\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#mol_types=df_train[\"type\"].unique()\n",
    "mol_types = ['1JHC']\n",
    "cv_score=[]\n",
    "cv_score_total=0 \n",
    "epoch_n = 500 # 300\n",
    "verbose = 2\n",
    "batch_size = 1024 # 512\n",
    "    \n",
    "# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\n",
    "retrain = False\n",
    "\n",
    "\n",
    "# Set up GPU preferences\n",
    "config = tf.ConfigProto( device_count = {'GPU': 2} ) \n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "sess = tf.Session(config=config) \n",
    "K.set_session(sess)\n",
    "\n",
    "start_time=datetime.now()\n",
    "\n",
    "# Loop through each molecule type\n",
    "for mol_type in mol_types:\n",
    "    model_name_rd = ('../keras-neural-net-for-champs/molecule_model_%s.hdf5' % mol_type)\n",
    "    model_name_wrt = ('molecule_model_%s.hdf5' % mol_type)\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "    \n",
    "    df_train_ = df_train_case_final[ df_train_case_final[\"type\"]==mol_type]\n",
    "    df_test_  = df_test_case_final [ df_test_case_final[\"type\"]==mol_type]\n",
    "    splits = df_train_.id%5\n",
    "\n",
    "    # Here's our best features.  We think.\n",
    "    input_features=extra_featus\n",
    "\n",
    "    # Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "    input_data=StandardScaler().fit_transform(pd.concat([df_train_.loc[:,input_features],df_test_.loc[:,input_features]]))\n",
    "    \n",
    "    target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n",
    "    target_data_1=df_train_.loc[:,[\"charge_0\",\"charge_1\"]]\n",
    "    target_data_2=df_train_.loc[:,[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]]\n",
    "    target_data_3=df_train_.loc[:,[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]]\n",
    "    \n",
    "    #following parameters should be adjusted to control the loss function\n",
    "    #if all parameters are zero, attractors do not work. (-> simple neural network)\n",
    "    m1=2\n",
    "    m2=4\n",
    "    m3=1\n",
    "    target_data_1=m1*(StandardScaler().fit_transform(target_data_1))\n",
    "    target_data_2=m2*(StandardScaler().fit_transform(target_data_2))\n",
    "    target_data_3=m3*(StandardScaler().fit_transform(target_data_3))\n",
    "    \n",
    "    # Simple split to provide us a validation set to do our CV checks with\n",
    "    #train_index, cv_index = train_test_split(np.arange(len(df_train_)),random_state=111, test_size=0.05)\n",
    "    train_index = np.where(splits != 0)[0]\n",
    "    cv_index = np.where(splits == 0)[0]\n",
    "    \n",
    "    # Split all our input and targets by train and cv indexes\n",
    "    train_input=input_data[train_index]\n",
    "    cv_input=input_data[cv_index]\n",
    "    train_target=target_data[train_index]\n",
    "    cv_target=target_data[cv_index]\n",
    "    train_target_1=target_data_1[train_index]\n",
    "    cv_target_1=target_data_1[cv_index]\n",
    "    train_target_2=target_data_2[train_index]\n",
    "    cv_target_2=target_data_2[cv_index]\n",
    "    train_target_3=target_data_3[train_index]\n",
    "    cv_target_3=target_data_3[cv_index]\n",
    "    test_input=input_data[len(df_train_):,:]\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model=create_nn_model(train_input.shape[1])\n",
    "    \n",
    "    # If retrain==False, then we load a previous saved model as a starting point.\n",
    "    if retrain:\n",
    "        nn_model = load_model(model_name_rd)\n",
    "        \n",
    "    nn_model.compile(loss='mae', optimizer=Adam())#, metrics=[auc])\n",
    "    #nn_model.compile(loss='mean_squared_error', optimizer=Adam())#, metrics=[auc])\n",
    "    \n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping(monitor='val_out_loss', min_delta=0.0005, patience=30,verbose=1, mode='auto', restore_best_weights=True)\n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor='val_out_loss', factor=0.3333,patience=15, min_lr=1e-6, mode='auto', verbose=1)\n",
    "    # Save the best value of the model for future use\n",
    "    sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_out_loss', save_best_only=True, period=1)\n",
    "\n",
    "    print (train_input.shape, cv_input.shape)\n",
    "    history = nn_model.fit(train_input,[train_target,train_target_1,train_target_2,train_target_3], \n",
    "                           validation_data=(cv_input,[cv_target,cv_target_1,cv_target_2,cv_target_3]), \n",
    "                           callbacks=[es, rlr, sv_mod],\n",
    "                           epochs=epoch_n,\n",
    "                           batch_size=batch_size,\n",
    "                           verbose=verbose)\n",
    "    \n",
    "    cv_predict=nn_model.predict(cv_input)\n",
    "    plot_history(history, mol_type)\n",
    "    \n",
    "    accuracy=np.mean(np.abs(cv_target-cv_predict[0][:,0]))\n",
    "    print( accuracy,np.log(accuracy) )\n",
    "    \n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total+=np.log(accuracy)\n",
    "    \n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict=nn_model.predict(test_input)\n",
    "    \n",
    "    # for each molecule type we'll grab the predicted values\n",
    "    test_prediction[df_test[\"type\"]==mol_type]=test_predict[0][:,0]\n",
    "    K.clear_session()\n",
    "\n",
    "cv_score_total/=len(mol_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time:  3:43:13.005300\n",
      "1JHC : cv score is  -0.8547819\n",
      "total cv score is -0.8547819256782532\n"
     ]
    }
   ],
   "source": [
    "print ('Total training time: ', datetime.now() - start_time)\n",
    "\n",
    "i=0\n",
    "for mol_type in mol_types: \n",
    "    print(mol_type,\": cv score is \",cv_score[i])\n",
    "    i+=1\n",
    "print(\"total cv score is\",cv_score_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare results for Submission\n",
    "\n",
    "The total CV score matches Kaggle's score pretty closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(predictions):\n",
    "    submit = pd.read_csv('../input/sample_submission.csv')\n",
    "    print(len(submit), len(predictions))   \n",
    "    submit[\"scalar_coupling_constant\"] = predictions\n",
    "    submit.to_csv(\"../submissions/keras-neural-net-1.csv\", index=False)\n",
    "submit(test_prediction)\n",
    "\n",
    "print ('Total training time: ', datetime.now() - start_time)\n",
    "\n",
    "i=0\n",
    "for mol_type in mol_types: \n",
    "    print(mol_type,\": cv score is \",cv_score[i])\n",
    "    i+=1\n",
    "print(\"total cv score is\",cv_score_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_types=df_train[\"type\"].unique()\n",
    "cv_score=[]\n",
    "cv_score_total=0\n",
    "epoch_n = 50\n",
    "verbose = 2\n",
    "batch_size = 512\n",
    "    \n",
    "# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\n",
    "retrain = True\n",
    "\n",
    "\n",
    "# Set up GPU preferences\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "sess = tf.Session(config=config) \n",
    "K.set_session(sess)\n",
    "\n",
    "start_time=datetime.now()\n",
    "\n",
    "# Loop through each molecule type\n",
    "for mol_type in mol_types:\n",
    "    model_name_rd = ('../keras-neural-net-for-champs/molecule_model_%s.hdf5' % mol_type)\n",
    "    model_name_wrt = ('working/molecule_model_%s.hdf5' % mol_type)\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "    \n",
    "    df_train_ = df_train[df_train[\"type\"]==mol_type]\n",
    "    df_test_  = df_test [ df_test[\"type\"]==mol_type]\n",
    "\n",
    "    # Here's our best features.  We think.\n",
    "    input_features=[\n",
    "        \"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\",\"c_x\",\"c_y\",\"c_z\",\n",
    "                    'x_closest_0','y_closest_0','z_closest_0','x_closest_1','y_closest_1','z_closest_1',\n",
    "                    \"distance\",\"distance_center0\",\"distance_center1\", \"distance_c0\",\"distance_c1\",\"distance_f0\",\"distance_f1\",\n",
    "                    \"cos_c0_c1\",\"cos_f0_f1\",\"cos_center0_center1\",\"cos_c0\",\"cos_c1\",\"cos_f0\",\"cos_f1\",\"cos_center0\",\"cos_center1\",\n",
    "                    \"atom_n\",\n",
    "                        'link0', 'dist_xyz', 'inv_dist0',\n",
    "       'inv_dist1', 'inv_distP', 'inv_dist0R', 'inv_dist1R', 'inv_distPR',\n",
    "       'inv_dist0E', 'linkM0', 'min_molecule_atom_0_dist_xyz',\n",
    "       'mean_molecule_atom_0_dist_xyz', 'max_molecule_atom_0_dist_xyz',\n",
    "       'sd_molecule_atom_0_dist_xyz', 'min_molecule_atom_1_dist_xyz',\n",
    "       'mean_molecule_atom_1_dist_xyz', 'max_molecule_atom_1_dist_xyz',\n",
    "       'distN0', 'distN1', 'bond_lengths_mean_y', 'bond_lengths_std_y',\n",
    "       'yukawa_C.x', 'yukawa_H.x', 'yukawa_N.x', 'yukawa_O.x', 'yukawa_C.y',\n",
    "       'yukawa_H.y', 'yukawa_N.y', 'yukawa_O.y', 'qm9_1', 'qm9_2', 'adH1',\n",
    "       'adC1', 'adC2', 'adC3', 'adC4', 'adC5', 'adN1', 'adN2', 'F1dist1',\n",
    "       'F1dist2', 'F1dist3', 'bond_atom', 'bond_distance', 'tertiary_angle_0',\n",
    "       'tertiary_angle_1', 'tertiary_angle_2', 'tertiary_angle_3',\n",
    "       'tertiary_angle_4', 'tertiary_angle_5', 'tertiary_angle_6',\n",
    "       'tertiary_angle_7', 'tertiary_distance_0', 'tertiary_distance_1',\n",
    "       'tertiary_distance_2', 'tertiary_distance_3', 'tertiary_distance_4',\n",
    "       'tertiary_distance_5', 'tertiary_distance_6', 'tertiary_distance_7',\n",
    "    ]\n",
    "\n",
    "    # Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "    input_data=StandardScaler().fit_transform(pd.concat([df_train_.loc[:,input_features],df_test_.loc[:,input_features]]))\n",
    "    \n",
    "    target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n",
    "    target_data_1=df_train_.loc[:,[\"charge_0\",\"charge_1\"]]\n",
    "    target_data_2=df_train_.loc[:,[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]]\n",
    "    target_data_3=df_train_.loc[:,[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]]\n",
    "    \n",
    "    #following parameters should be adjusted to control the loss function\n",
    "    #if all parameters are zero, attractors do not work. (-> simple neural network)\n",
    "    m1=2\n",
    "    m2=4\n",
    "    m3=1\n",
    "    target_data_1=m1*(StandardScaler().fit_transform(target_data_1))\n",
    "    target_data_2=m2*(StandardScaler().fit_transform(target_data_2))\n",
    "    target_data_3=m3*(StandardScaler().fit_transform(target_data_3))\n",
    "    \n",
    "    # Simple split to provide us a validation set to do our CV checks with\n",
    "    train_index, cv_index = train_test_split(np.arange(len(df_train_)),random_state=111, test_size=0.05)\n",
    "    \n",
    "    # Split all our input and targets by train and cv indexes\n",
    "    train_input=input_data[train_index]\n",
    "    cv_input=input_data[cv_index]\n",
    "    train_target=target_data[train_index]\n",
    "    cv_target=target_data[cv_index]\n",
    "    train_target_1=target_data_1[train_index]\n",
    "    cv_target_1=target_data_1[cv_index]\n",
    "    train_target_2=target_data_2[train_index]\n",
    "    cv_target_2=target_data_2[cv_index]\n",
    "    train_target_3=target_data_3[train_index]\n",
    "    cv_target_3=target_data_3[cv_index]\n",
    "    test_input=input_data[len(df_train_):,:]\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model=create_nn_model(train_input.shape[1])\n",
    "    \n",
    "    # If retrain==False, then we load a previous saved model as a starting point.\n",
    "    #if not retrain:\n",
    "    nn_model = load_model(model_name_rd)\n",
    "        \n",
    "    nn_model.compile(loss='mae', optimizer=Adam(lr=0.00001))#, metrics=[auc])\n",
    "    #nn_model.compile(loss='mean_squared_error', optimizer=Adam())#, metrics=[auc])\n",
    "    \n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping(monitor='val_out_loss', min_delta=0.0005, patience=30,verbose=1, mode='auto', restore_best_weights=True)\n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor='val_out_loss', factor=0.3333,patience=15, min_lr=1e-6, mode='auto', verbose=1)\n",
    "    # Save the best value of the model for future use\n",
    "    sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_out_loss', save_best_only=True, period=1)\n",
    "\n",
    "    history = nn_model.fit(train_input,[train_target,train_target_1,train_target_2,train_target_3], \n",
    "                           validation_data=(cv_input,[cv_target,cv_target_1,cv_target_2,cv_target_3]), \n",
    "                           callbacks=[es, rlr, sv_mod],\n",
    "                           epochs=epoch_n,\n",
    "                           batch_size=batch_size,\n",
    "                           verbose=verbose)\n",
    "    \n",
    "    cv_predict=nn_model.predict(cv_input)\n",
    "    plot_history(history, mol_type)\n",
    "    \n",
    "    accuracy=np.mean(np.abs(cv_target-cv_predict[0][:,0]))\n",
    "    print( accuracy,np.log(accuracy) )\n",
    "    \n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total+=np.log(accuracy)\n",
    "    \n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict=nn_model.predict(test_input)\n",
    "    \n",
    "    # for each molecule type we'll grab the predicted values\n",
    "    test_prediction[df_test[\"type\"]==mol_type]=test_predict[0][:,0]\n",
    "    K.clear_session()\n",
    "\n",
    "cv_score_total/=len(mol_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test), len(test_prediction[df_test[\"type\"]==mol_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1JHC : cv score is  -0.60937184\n",
    "# 2JHH : cv score is  -2.0246258\n",
    "# 1JHN : cv score is  -1.3377854\n",
    "# 2JHN : cv score is  -2.0205796\n",
    "# 2JHC : cv score is  -1.4633688\n",
    "# 3JHH : cv score is  -1.8357394\n",
    "# 3JHC : cv score is  -1.4595399\n",
    "# 3JHN : cv score is  -2.2611656\n",
    "# total cv score is -1.6265220269560814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(predictions):\n",
    "    submit = pd.read_csv('../input/sample_submission.csv')\n",
    "    print(len(submit), len(predictions))   \n",
    "    submit[\"scalar_coupling_constant\"] = predictions\n",
    "    submit.to_csv(\"../submissions/keras-neural-net-1-ft.csv\", index=False)\n",
    "submit(test_prediction)\n",
    "\n",
    "print ('Total training time: ', datetime.now() - start_time)\n",
    "\n",
    "i=0\n",
    "for mol_type in mol_types: \n",
    "    print(mol_type,\": cv score is \",cv_score[i])\n",
    "    i+=1\n",
    "print(\"total cv score is\",cv_score_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
